{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Few Shot learning: Classify using few examples!\"\n",
    "author: Deepam Minda\n",
    "date: \"Sep 12, 2023\"\n",
    "categories: [classification, few-shot learning, nlp]\n",
    "image: \"cover.jpeg\"\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-depth: 3\n",
    "    html-math-method: katex\n",
    "    # css: ../../styles.css\n",
    "    code-fold: true\n",
    "  ipynb: default\n",
    "execute:\n",
    "  warning: false\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "If you aren‚Äôt already familiar with it, few-shot learning (FSL) is an exciting concept in machine learning that focuses on training models to make accurate predictions or classifications when provided with only a very limited amount of labeled data for each category or class. Unlike traditional machine learning approaches that often require substantial labeled datasets for training, few-shot learning aims to address scenarios where acquiring such extensive labeled data is impractical or costly.\n",
    "\n",
    "In few-shot learning, the model is trained to generalize effectively from a small number of examples (or shots) per category, allowing it to make predictions for new, unseen data points with confidence. This capability is particularly valuable in situations where data is scarce, and manual annotation is labor-intensive or expensive. Refer to [1] for an in-depth survey in this field.\n",
    "\n",
    "In this blog, I‚Äôm going to show you how to implement a basic few-shot classification technique for text.\n",
    "\n",
    "## Terminology\n",
    "Before we begin, let us familiarize ourselves with the correct terminology.\n",
    "\n",
    "What characterizes FSL is having only a few examples at hand, for unseen classes, during inference. So basically we are showing the model only a few examples of a class which it may or may not have encountered during its pre-training before we make predictions using that model.\n",
    "\n",
    "**Support Set, ùíÆ:** The few annotated examples that we have, make up the support set, with which we may or may not update the model weights to make it generalize to the new classes.\n",
    "\n",
    "**Query Set, ùí¨:** The query set consists of our test set, i.e. the samples we want to classify using the base model and a support set.\n",
    "\n",
    "**N-way K-shot learning scheme:** This is a common phrase used in the FSL literature, which essentially describes the few-shot problem statement that a model will be dealing with. ‚ÄúN‚Äù is the number of classes we have at test time and ‚ÄúK‚Äù is the number of samples per class we have in our support set ‚ÄúùíÆ‚Äù\n",
    "\n",
    "**1-shot classification:** When K=1, i.e. we have only one labeled sample available per class.\n",
    "\n",
    "**0-shot classification:** K=0, i.e. we do not have any labeled samples available during inference.\n",
    "\n",
    "Let us have a look at an example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-overflow: wrap\n",
    "\n",
    "# sample set is 3-way, 3-shot.\n",
    "classes = ['camera', 'battery', 'display']\n",
    "\n",
    "sample_set = {\n",
    "    'camera': [\n",
    "        'absolutely love this quality of my photos!!',\n",
    "        'it even gives great quality in dim lighting. fabulous!!',\n",
    "        'the camera should be much better for such a high price'\n",
    "    ],\n",
    "    'battery': [\n",
    "        \"The battery life on this device is exceptional! It easily lasts me the entire day with heavy usage.\",\n",
    "        \"I'm a bit disappointed with the battery performance. It drains quite quickly, especially when using power-hungry apps.\",\n",
    "        \"The battery is decent, not too bad, not too good. It gets me through the day, but I was hoping for better longevity.\"\n",
    "    ],\n",
    "    'display': [\n",
    "        \"The display on this device is stunning! Colors are vivid, and the resolution is top-notch.\",\n",
    "        \"I'm not too impressed with the display quality. It seems a bit washed out, and the brightness could be better.\",\n",
    "        \"The display is okay, but nothing extraordinary. It gets the job done for everyday tasks.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "query_set = [\"i hate the batteries\", \"does it give good quality photos in the night?\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a 3-way (there are 3 classes), 3-shot (3 examples for each class) setting.\n",
    "\n",
    "## High level design\n",
    "Let us have a quick look at the architecture of the system.\n",
    "\n",
    "![A simple few shot classification system](1_few_shot_system.png){#few-shot}\n",
    "\n",
    "This is the flow of our solution:\n",
    "\n",
    "The first step is to get an embedding module. That can be created using regular supervised learning (Resnets trained on Imagenet) or self-supervised learning (BERT and co).\n",
    "Then, we use the embedding module to get feature representations for our classes in the support set. A simple way to do this is to turn each class‚Äôs examples into embeddings and take the mean of those vectors. This then becomes our ‚Äúprototype‚Äù vectors to compare against.\n",
    "Now for each query, we can take the embeddings of the query text and use cosine similarity to find the predicted class. This closely resembles\n",
    "This system basically allows us to leverage transfer learning to use large backbones as our embedding module. And there is also the advantage of not performing any gradient updates. This helps us maintain a much more dynamic and flexible system.\n",
    "\n",
    "The idea of comparing query samples with the support set samples is inspired by metric learning. Refer to [3, 4] for better understanding.\n",
    "\n",
    "Let's implement this using the transformers library. You can find the implementation in this colab notebook.\n",
    "\n",
    "## Implementation\n",
    "Let‚Äôs start with the good old BERT base model.\n",
    "\n",
    "### 1. Import libraries and download model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| output: false\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "from pprint import pprint\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "# load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Tokenize and encode a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded input:\n",
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]),\n",
      " 'input_ids': tensor([[ 101, 2002, 1005, 1055, 2107, 1037, 2307, 3124,  999,  999,  102,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0]]),\n",
      " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "text = \"He's such a great guy!!\"\n",
    "encoded_input = tokenizer(\n",
    "  text, \n",
    "  return_tensors='pt', \n",
    "  padding='max_length',     # True will pad to max-len in batch\n",
    "  max_length=32\n",
    ")\n",
    "print(f\"encoded input:\")\n",
    "pprint(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where,\n",
    "\n",
    "- input_ids: token id of each token\n",
    "- token_type_id: When we pass two sentences for downstream fine-tuning in BERT, this is used to identify which token belongs to which sentence.\n",
    "- attention_mask: which tokens to ignore. As you‚Äôll see, the padding tokens have been masked.\n",
    "\n",
    "### 3. Generate embeddings using model\n",
    "\n",
    "The output has 2 parts, `cls_token_embeddings` and `last_hidden_states` of the tokens. We can either use the cls_embeddings to represent the sentence or pool the vectors in last_hidden_states. The pooling can be max/min/mean.\n",
    "\n",
    "The dimension of the output will be equal to the embedding dimension of the model, i.e. 784 in our case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of embeddings: (1, 768)\n"
     ]
    }
   ],
   "source": [
    "#| code-overflow: wrap\n",
    "\n",
    "def get_embeddings(model, tokenizer, text, pooling='mean'):\n",
    "  \n",
    "  encoded_input = tokenizer(\n",
    "    text, \n",
    "    return_tensors='pt', \n",
    "    padding='max_length', \n",
    "    max_length=16, \n",
    "    truncation=True\n",
    "  )\n",
    "  encoded_input = encoded_input.to(device)\n",
    "\n",
    "  model.to(device)\n",
    "\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    output = model(**encoded_input)\n",
    "    last_hidden_state, pooler_output = output[0], output[1]\n",
    "    \n",
    "    if pooling=='cls':\n",
    "      embedding = pooler_output\n",
    "    else:\n",
    "      # ignore the pad tokens embeddings by multiplying with attention mask\n",
    "      last_hidden_state = (last_hidden_state * encoded_input['attention_mask'].unsqueeze(2))\n",
    "      embedding = last_hidden_state.mean(dim=-2)\n",
    "  return np.array(embedding.cpu())\n",
    "\n",
    "\n",
    "embeddings = get_embeddings(model, tokenizer, 'hey there! how are you?')\n",
    "print(f\"shape of embeddings: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Prepare the prototypes:\n",
    "\n",
    "To prepare the class prototypes we‚Äôll take the mean of the sentences for each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prototypes(model, tokenizer, sample_set: Dict):\n",
    "  prototype_vectors = dict()\n",
    "  sentence_embeddings = dict()\n",
    "  for category, sentences in sample_set.items():\n",
    "    sentence_embeds = get_embeddings(model, tokenizer, sentences)\n",
    "    sentence_embeddings[category] = sentence_embeds\n",
    "    prototype_vectors[category] = np.mean(sentence_embeddings[category], axis=0)\n",
    "  return prototype_vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Classify\n",
    "\n",
    "To classify a query text, we can run cosine similarity against the prototype vectors and return the argmax as the most probable class!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(model, tokenizer, text, prototype_vectors=None, sample_set=None):\n",
    "  if prototype_vectors==None:\n",
    "      assert sample_set!=None, \"prototype vectors are not passed, either pass a sample set prototype vectors\"\n",
    "      prototype_vectors = make_prototypes(sample_set)\n",
    "\n",
    "  query_embeddings = get_embeddings(model, tokenizer, text)\n",
    "  \n",
    "  prototype_matrix = np.stack(list(prototype_vectors.values()))\n",
    "  scores = sentence_transformers.util.cos_sim(query_embeddings, prototype_matrix)\n",
    "  return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above-defined functions and the sample set from before, we have:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tensor([[0.6121, 0.7127, 0.6388]])\n",
      "the predicted class is battery\n"
     ]
    }
   ],
   "source": [
    "prototype_vectors = make_prototypes(model, tokenizer, sample_set)\n",
    "query_text = \"i hate the batteries\"\n",
    "output = classify(model, tokenizer, query_text, prototype_vectors=prototype_vectors)\n",
    "\n",
    "print(f\"output: {output}\")\n",
    "print(f\"the predicted class is {classes[output.argmax().item()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit strange! Although the expected class is predicted, scores for other classes are also high. Let's try a harder query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tensor([[0.7984, 0.7043, 0.7647]])\n",
      "the predicted class is camera\n"
     ]
    }
   ],
   "source": [
    "query = ['does it give good quality photos in the night?']\n",
    "output = classify(model, tokenizer, query, prototype_vectors=prototype_vectors)\n",
    "\n",
    "print(f\"output: {output}\")\n",
    "print(f\"the predicted class is {classes[output.argmax().item()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Although the highest similarity is for ‚Äòcamera‚Äô, the similarity should be much higher.\n",
    "\n",
    "The results do not get better even if we try cls-pooling. This only means that the embeddings produced by the model do not give us an accurate representation of the sentence.\n",
    "\n",
    "We would then do good to remember that BERT pre-train was trained by MaskedLM, NextSentencePrediction, hence the original purpose of BERT is not to create a meaningful embedding of the sentence but for some specific downstream task. In fact, as the authors of the sentence-transformer paper [2] point out, out-of-the-box Bert embeddings perform even worse than GLoVE representations!\n",
    "\n",
    "> Jacob Devlin‚Äôs comment: I‚Äôm not sure what these vectors are, since BERT does not generate meaningful sentence vectors. It seems that this is is doing average pooling over the word tokens to get a sentence vector, but we never suggested that this will generate meaningful sentence representations. And even if they are decent representations when fed into a DNN trained for a downstream task, it doesn‚Äôt mean that they will be meaningful in terms of cosine distance. (Since cosine distance is a linear space where all dimensions are weighted equally).\n",
    "\n",
    "There are a few ways to improve the bert-base for sentence-level tasks and both involve finetuning the model with some data.\n",
    "\n",
    "- adding a linear layer on top and fine-tuning it.\n",
    "- making embeddings better by contrastive learning.\n",
    "\n",
    "## Using sentence transformers\n",
    "\n",
    "Ultimately, what we need is a better embedding module. Luckily we have such models. As it turns out, contrastive learning is an excellent approach for tuning our models such that different sentences produce semantically different embeddings.\n",
    "\n",
    "We will explore contrastive learning and its inner workings some other day, but for now, let's pick up open-source models that have been finetuned using contrastive learning. There is an entire library (aka sentence-transformers) and paper[2] dedicated to this.\n",
    "\n",
    "We‚Äôll use the `sentence-transformers/stsb-bert-base` model for our purposes.\n",
    "\n",
    "### 1. Import packages and download model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# load a sentence transformer model\n",
    "sts_model = SentenceTransformer('sentence-transformers/stsb-bert-base')\n",
    "model2 = sts_model[0].auto_model.to(device)\n",
    "tokenizer2 = sts_model[0].tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Use the above-defined functions to prepare prototype vectors and classify them in a few-shot setting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tensor([[0.0910, 0.4780, 0.1606]])\n",
      "the predicted class is battery\n"
     ]
    }
   ],
   "source": [
    "prototype_vectors = make_prototypes(model2, tokenizer2, sample_set)\n",
    "query_text = \"i hate the batteries\"\n",
    "output = classify(model2, tokenizer2, query_text, prototype_vectors)\n",
    "\n",
    "print(f\"output: {output}\")\n",
    "print(f\"the predicted class is {classes[output.argmax().item()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tensor([[0.4467, 0.1012, 0.2998]])\n",
      "the predicted class is camera\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = ['does it give good quality photos in the night?']\n",
    "output = classify(model2, tokenizer2, query, prototype_vectors=prototype_vectors)\n",
    "\n",
    "print(f\"output: {output}\")\n",
    "print(f\"the predicted class is {classes[output.argmax().item()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the scores seem much more reasonable this time around. There is a much better correlation with the ground truth labels. Using better base models trained in multiple tasks further improves the performance of these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This brings us to the end of this blog. In summary, we explored the realm of few-shot learning, a machine-learning approach tailored for accurate predictions with limited labeled data. Initially, we employed BERT, but its design didn‚Äôt align with our objectives. Instead, we leveraged a model fine-tuned for sentence-level tasks, `sentence-transformers/stsb-bert-base`, which significantly improved our results.\n",
    "\n",
    "**These are a few things to note:**\n",
    "\n",
    "Although we directly used pre-trained models here, an interesting undertaking would be to perform the contrastive fine-tuning ourselves. Also, instead of using cosine similarity, we can train lightweight classifiers on top of our embedding module for better performance.\n",
    "\n",
    "That‚Äôll be all from my side. Until next time, Happy Reading!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## References\n",
    "\n",
    "[1] [Survey paper on few-shot learning](https://arxiv.org/pdf/1904.05046.pdf)\n",
    "\n",
    "[2] [Sentence-Bert paper](https://arxiv.org/pdf/1908.10084.pdf)\n",
    "\n",
    "[3] [Prototypical Networks](https://proceedings.neurips.cc/paper_files/paper/2017/file/cb8da6767461f2812ae4290eac7cbc42-Paper.pdf)\n",
    "\n",
    "[4] [Excellent much more techincal blog by Lilian Weng](https://lilianweng.github.io/posts/2018-11-30-meta-learning/#metric-based)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
