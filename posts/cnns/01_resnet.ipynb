{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Training a Resnet in Pytorch\"\n",
    "author: Deepam Minda\n",
    "date: \"August 10, 2024\"\n",
    "categories: [cnns, deep-learning]\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-depth: 3\n",
    "    html-math-method: katex\n",
    "    code-fold: false\n",
    "  ipynb: default\n",
    "execute:\n",
    "  warning: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "In my last post [Intro to CNNs](./00_cnn_basics.ipynb), we discussed the basics of CNNs and how to train a simple CNN from scratch. In this post, we will train a ResNet in Pytorch.\n",
    "\n",
    "Resnets came around right after VGGs, where the author explored the idea of training very deep networks for better performance, since deeper models could in thoery learn more complex features. But after a certain point, more depth didnt help with wither validation accuracy nor training performance. This was confusing, since we would assume a larger model can atleast replicate the performance of a smaller network and then some!\n",
    "\n",
    "Mathematically, if we have a shallower network $H(x)$ that performs well, a deeper network $F(x)$ should be able to learn an identity mapping in its additional layers, effectively becoming:\n",
    "\n",
    "$F(x) = H(x) + (F(x) - H(x))$\n",
    "\n",
    "where $(F(x) - H(x))$ is the residual mapping. Instead of hoping that $F(x)$ will learn the entire desired mapping, we can explicitly let it learn the residual mapping $(F(x) - H(x))$. This is the key idea behind ResNets: learning residual functions with reference to the layer inputs, rather than learning unreferenced functions.\n",
    "\n",
    "This approach allows for the creation of very deep networks without the problem of vanishing gradients, as the gradient can flow directly through the skip connections (also known as shortcut connections) during backpropagation. We can imagine this as a graident highway that goes right through the network and then comes back to the input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "![This is a resnet-architecture](resnet-architecture.png)\n",
    "\n",
    "# VGGs\n",
    "\n",
    "Since so much of the comparison is done with VGGs it is important to understand them too. Ignoring batch dimension for now, passing through CNNs can be visualized as volumes of data moving through through the network, volume being $(H, W, C)$ where $H, W$ are the height and width of the image and $C$ is the number of channels.\n",
    "\n",
    "\n",
    "\n",
    "![vgg-19-architecture](vgg.png){#fig-vgg}\n",
    "\n",
    "In VGGs \n",
    "\n",
    "- there are 5 Conv blocks followed by some FC layers for classification. The Conv blocks can be thought of as a feature extractor.\n",
    "- each Conv block is a sequence of come `CONV->RELU` layers followed by a max_pool layer. \n",
    "- each conv layer has a kernel size of 3x3, stride=1 and padding=1.\n",
    "- The output of the last conv block is flattened and passed to the FC layers.\n",
    "- the FC layers have fixed sizes = [4096, 4096, 1000] with ReLU and Dropout after each layer. \n",
    "- Output is a 1000 sized vector., which represents Imagenet classes.\n",
    "\n",
    "\n",
    "## ConvReLU Layer\n",
    "\n",
    "Each block contains some conv_layers followed by a max_pool layer. If the input to a block is $(H, W, C)$, the output of the block is $(H/2, W/2, C*2)$. All in all each block still preserves the total volume. \n",
    "\n",
    "Let us look inside a conv block, for eg the conv2 block in @fig-vgg. We can see that the particular block has a max_pool (red) layer and 2 single-conv layers(blue). Each single-conv layer within that block is a: \n",
    "\n",
    "`conv_layer = [CNN->ReLU]` \n",
    "\n",
    "In torch,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "class ConvReLU(nn.Module):\n",
    "    \"\"\"\n",
    "    A single conv layer followed by batchnorm and activation.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, activation=nn.ReLU(), stride=1, padding=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.activation is not None:\n",
    "            x = self.activation(self.conv(x))\n",
    "        else:\n",
    "            x = self.conv(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create the conv1 layer and visualize the input and output volumes. The input is a rgb image of size `224x224x3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = torch.rand(224, 224, 3)   \n",
    " \n",
    "# input images must have shape (C,H,W) this is usually managed by torch's to_tensor transform\n",
    "input_img = torch.permute(input_img, (2, 0, 1))      \n",
    "\n",
    "# conv layer params -> C_in=3, C_out=64, K=3, S=1, P=1\n",
    "conv_layer_1 = ConvReLU(3, 64, kernel_size=3, stride=1, padding=1) \n",
    "conv_layer_2 = ConvReLU(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "# max pool layer params -> K=2, S=2\n",
    "max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "print(\"input volume = {}\\n\".format(input_img.shape))\n",
    "print(\"output volume after conv_layer_1 = {}\".format(conv_layer_1(input_img).shape))\n",
    "print(\"output volume after conv_layer_2 = {}\".format(conv_layer_2(conv_layer_1(input_img)).shape))\n",
    "# print(\"output volume after max_pool = {}\".format(max_pool(conv_layer_2(conv_layer_1(input_img))).shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block of ConvReLU layers\n",
    "\n",
    "Now to make a block of VGG, we combine multiple such layers together. If you see the @fig-vgg, each block's has a red layer in the beginning. The red layer is a max_pool layer with stride=2 and kernel_size=2. This results in the $H,W$ being halved. Then the subsequent conv_layers preserve the $H,W$ and double the channels. Lets look at a block below:\n",
    "\n",
    "`Block = [conv_layer_1->[conv_layer_i]*(n-1)]`.\n",
    "\n",
    "The feature extractor will look like this:  \n",
    "\n",
    "[`block_1 -> Max_pool -> block_2 -> Max_pool -> block_3 -> ...`]\n",
    "\n",
    "Lets create a block class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channels, num_layers, out_channels, activation=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.out_channels = input_channels*2 if out_channels is None else out_channels\n",
    "        \n",
    "        # first layer must half the H_in,W_in and double the C_in with S=2, P=1\n",
    "        conv_1 = ConvReLU(input_channels, self.out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        layers = [conv_1]\n",
    "        \n",
    "        # rest of the layers preserve the H_in, W_in and C_in with S=1 and P=1\n",
    "        for _ in range(num_layers-1):\n",
    "            conv_ = ConvReLU(self.out_channels, self.out_channels, kernel_size=3, stride=1, padding=1)\n",
    "            layers.append(conv_)\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "conv1_block = Block(input_channels=3, num_layers=2, out_channels=64)\n",
    "\n",
    "x = conv1_block(input_img)\n",
    "\n",
    "print(\"output volume after conv1_block = {}\".format(x.shape))\n",
    "\n",
    "# conv_block_2 has half the h,w (due to max pool right before it) and double the input channels. \n",
    "conv2_block = Block(input_channels=64, num_layers=2, out_channels=128)\n",
    "x = conv2_block(max_pool(x))\n",
    "\n",
    "print(\"output volume after conv2_block = {}\".format(x.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG model\n",
    "We can go on this way for the rest of the blocks, but lets bring it together in a VGG class itself.\n",
    "\n",
    "Lets write down the vgg configuration as a list of items. The model can be imagined as a feature extractor with a classifier head on top. \n",
    "\n",
    "The classifier part has fixed configuration, but the feature extractor can be configured by passing a list of numbers and strings to the constructor. Each item is either a number or a string and represents a conv layer or a max_pool layer. The number is the number of conv layers in the block, and the string is 'M' which indicates a max pool layer. \n",
    "\n",
    "The classifier part consists of 2 hidden-FC layers (4096 neurons each) with activations and dropout regularization added. The final layer is the classifier head with 1000 outputs. (Imagenet dataset has 1k classes)\n",
    "\n",
    "Lastly, to enable the model to initialize with variable input sizes, we have used a dynamic input size of the 1st FC layer., and a dynamic hidden_fc_size for the last FC layer. This will allow us to experiment with smaller datasets locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_vgg_config = {\n",
    "    'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "    'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M']\n",
    "}\n",
    "import logging \n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, config, input_shape=(224,224,3), dropout=0.5, num_classes=1000, hidden_fc_size=4096):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = self.input_shape\n",
    "        self.hidden_fc_size = hidden_fc_size\n",
    "        prev_channels = self.input_shape[2]\n",
    "        \n",
    "        self.blocks = []\n",
    "        for i in config:\n",
    "            if i == 'M':\n",
    "                self.blocks.append(nn.MaxPool2d(kernel_size=2, stride=2)) \n",
    "                self.output_shape = (self.output_shape[0]//2, self.output_shape[1]//2, self.output_shape[2])\n",
    "            else:\n",
    "                self.blocks.append(ConvReLU(prev_channels, i, kernel_size=3, stride=1, padding=1))\n",
    "                self.output_shape = (self.output_shape[0], self.output_shape[1], i)\n",
    "                prev_channels = i\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(*self.blocks)\n",
    "        \n",
    "        self.ConvMLP = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.output_shape[0]*self.output_shape[1]*self.output_shape[2], self.hidden_fc_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.hidden_fc_size, self.hidden_fc_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "        self.head = nn.Linear(self.hidden_fc_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # if a single item turn into a batch\n",
    "        if x.ndim == 3:\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.view(x.size(0), -1)  # this flattens the tensor to (batch_size, -1)\n",
    "        x = self.ConvMLP(x)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets visualize the model, output shapes of various layers and its size, etc using torchinfo.summary().\n",
    "To install the package run `pip install torchinfo` on your terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "vgg19 = VGG(_vgg_config['vgg19'])\n",
    "summary(vgg19, input_size=(3,224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pretty huge model (140million parameters) isn't it?! The original model was trained on the entire Imagenet dataset (150gb). But we'll use the TinyImagenet dataset(<1gb) for this example. So we'll use a much smaller model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_CLASSES=200 # Tiny ImageNet has 200 classes\n",
    "IMG_SIZE = 64\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "model = VGG(\n",
    "    _vgg_config['vgg11'], \n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3), \n",
    "    num_classes=NUM_CLASSES,\n",
    "    hidden_fc_size=1026\n",
    ").to(device)\n",
    "        \n",
    "summary(model, input_size=(3,IMG_SIZE,IMG_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This brings our model size down to a much more manageble 12million parameters from the original ~140 million parameters. \n",
    "\n",
    "For training we'll use the Adam optimizer and learing rate of 0.001. We'll also use the CrossEntropyLoss as the loss function as this is a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "\n",
    "if not os.path.exists(\"data/\"):\n",
    "    os.mkdir(\"data/\")\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 10\n",
    "DATASET_PATH = \"./data/tiny-imagenet-200\"\n",
    "\n",
    "# Download and extract Tiny ImageNet\n",
    "def download_and_extract_tiny_imagenet():\n",
    "    url = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n",
    "    filename = \"tiny-imagenet-200.zip\"\n",
    "    extract_path = DATASET_PATH\n",
    "    \n",
    "    if not os.path.exists(extract_path):\n",
    "        print(\"Downloading Tiny ImageNet...\")\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        \n",
    "        print(\"Extracting Tiny ImageNet...\")\n",
    "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "            zip_ref.extractall(\".\")\n",
    "        \n",
    "        os.remove(filename)\n",
    "        print(\"Tiny ImageNet downloaded and extracted.\")\n",
    "    else:\n",
    "        print(\"Tiny ImageNet already exists.\")\n",
    "    os.system(\"mv tiny-imagenet-200 data/tiny-imagenet-200\")\n",
    "    return extract_path\n",
    "\n",
    "# Custom Dataset for Tiny ImageNet\n",
    "class TinyImageNetDataset(Dataset):\n",
    "    def __init__(self, root, split='train', transform=None, labels=None):\n",
    "        self.root = root\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "\n",
    "        if split == 'train':\n",
    "            for class_dir in os.listdir(os.path.join(root, 'train')):\n",
    "                class_path = os.path.join(root, 'train', class_dir, 'images')\n",
    "                for img_name in os.listdir(class_path):\n",
    "                    self.images.append(os.path.join(class_path, img_name))\n",
    "                    self.labels.append(int(class_dir[1:]))\n",
    "        elif split == 'val':\n",
    "            val_annotations = os.path.join(root, 'val', 'val_annotations.txt')\n",
    "            with open(val_annotations, 'r') as f:\n",
    "                for line in f:\n",
    "                    img_name, class_id, _, _, _, _ = line.strip().split('\\t')\n",
    "                    print()\n",
    "                    self.images.append(os.path.join(root, 'val', 'images', img_name))\n",
    "                    self.labels.append(int(class_id[1:]))\n",
    "        self.labelset = list(set(self.labels)) if labels is None else labels\n",
    "        self.labels_to_class = {label:idx for idx,label in enumerate(self.labelset)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels_to_class[self.labels[idx]]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Download and extract the dataset\n",
    "if os.path.exists(\"./data/tiny-imagenet-200\"):\n",
    "    dataset_path = DATASET_PATH\n",
    "else:\n",
    "    dataset_path = download_and_extract_tiny_imagenet()\n",
    "\n",
    "# Data transforms\n",
    "train_transform = transforms.Compose([\n",
    "    # transforms.RandomResizedCrop(64),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    # transforms.Resize(64),\n",
    "    # transforms.CenterCrop(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = TinyImageNetDataset(dataset_path, split='train', transform=train_transform)\n",
    "val_dataset = TinyImageNetDataset(dataset_path, split='val', transform=val_transform, labels=train_dataset.labelset)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=1)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=1)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# TensorBoard writer\n",
    "writer = SummaryWriter('runs/vgg_tiny_imagenet')\n",
    "\n",
    "def train_and_validate(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "\n",
    "    # Training\n",
    "    for batch_idx, (data, target) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} - Training\")):\n",
    "        print(batch_idx)\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        train_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        print(f\"train loss {torch.mean(train_loss)}\")\n",
    "        writer.add_scalar('Loss/train_step', loss.item())\n",
    "        # writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "    train_loss /= len(train_loader)\n",
    "    train_accuracy = 100. * train_correct / len(train_loader.dataset)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} - Validation\"):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            writer.add_scalar('Loss/val_step', loss.item())\n",
    "            val_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            print(f\"val loss {torch.mean(val_loss)}\")\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100. * val_correct / len(val_loader.dataset)\n",
    "\n",
    "    # Log to TensorBoard\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/train', train_accuracy, epoch)\n",
    "    writer.add_scalar('Accuracy/val', val_accuracy, epoch)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}:')\n",
    "    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
    "    print(f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_and_validate(epoch)\n",
    "\n",
    "writer.close()\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Running vgg11 on colab gives us a score of ~28% accuracy on the validation set in 20 epochs, while training loss and val loss both were kind of still decreasing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_params_and_grads(model, n=3):\n",
    "    grad_values = [(name, param.grad.abs().cpu().detach().numpy())\n",
    "                   for name, param in model.named_parameters()\n",
    "                   if param.grad is not None]\n",
    "    \n",
    "    def plot_grads(grad_values, title_prefix):\n",
    "        num_plots = min(n, len(grad_values))\n",
    "        fig, axes = plt.subplots(1, num_plots, figsize=(5 * num_plots, 5))\n",
    "        if num_plots == 1:\n",
    "            axes = [axes]  # Ensure axes is iterable if only one plot\n",
    "\n",
    "        for idx in range(num_plots):\n",
    "            name, grad = grad_values[idx]\n",
    "            axes[idx].hist(grad.flatten(), bins=50)\n",
    "            axes[idx].set_title(f'{title_prefix} Layer {name}')\n",
    "            axes[idx].set_xlabel('Gradient Magnitude')\n",
    "            axes[idx].set_ylabel('Frequency')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Plot first n parameters\n",
    "    plot_grads(grad_values[:n], title_prefix='First')\n",
    "\n",
    "    # Plot last n parameters\n",
    "    plot_grads(grad_values[-n:], title_prefix='Last')\n",
    "\n",
    "\n",
    "plot_params_and_grads(model, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scaling to vgg_16: \n",
    "\n",
    "        suprisingly model isnt learning at all. loss not going down. upon inspection of gradients we find that while training vgg_199 there are practically no gradients.\n",
    "\n",
    "        Also note that grads start even when loss isnt decreasing at the first few steps.\n",
    "\n",
    "    Look at the the below figures:\n",
    "\n",
    "    ### VGG-11 \n",
    "    !['VGG-11 Final Layer Gradients'](./vgg_11_final_layer_grads.png){#fig-vgg_11_final_layer_grads height=50%, width=100%}\n",
    "\n",
    "\n",
    "    ![\"VGG-11 First Layers Gradient Plot\"](./vgg_11_first_layers_grad_plot.png){#fig-vgg_11_first_layers_grad_plot height=50%, width=100%}\n",
    "\n",
    "    ### VGG-16\n",
    "    ![\"VGG-16 Final Layers Gradients\"](./vgg_16_final_layers_grad.png){#fig-vgg_16_final_layers_grad height=50%, width=100%}\n",
    "\n",
    "\n",
    "    ![\"VGG-16 First Layers Gradient Plot\"(Vanishing Gradients)](./vgg_16_first_layers_grad_plot_vanishing-grads.png){#fig-vgg_16_first_layers_grad_plot_vanishing-grads height=50%, width=100%}\n",
    "\n",
    "\n",
    "- Does removing bottlenecked maxpool before fc layers help?: No, it doesnt. :(\n",
    "- what about vgg13? being clser to the 11 variant this doesnt experience vanishing gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is esssentialy the central limitations that still remain after going very deep. Depth is not enough, we need better ways to allow gradients to be propagated through these deep networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### computational considerations\n",
    "\n",
    "A usual Conv2d layer has a computation complexity of $O(H_{in}*W_{in}*k^2*C_{in}*C_{out})$. \n",
    "Derivation: \n",
    "    \n",
    "- number of output cells = $H_{out}*W_{out}*C_{out}$\n",
    "- computation required to compute each output cell = $k^2*C_{in}$\n",
    "- number of ops  \n",
    "    -> number of output cells * computation required to compute each output cell  \n",
    "    -> $H_{out}*W_{out}*k^2*C_{in}*C_{out}$\n",
    "\n",
    "\n",
    "\n",
    "# Inception\n",
    "\n",
    "VGGs large networks deal with issues of overfitting and increased computation. After it focus shifted to making the networks more compute efficient. Aiming to solve these issues by introducing extra sparsity in the network, the Inception model was proposed.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveInceptionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(NaiveInceptionBlock, self).__init__()\n",
    "        self.conv1x1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv3x3 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5x5 = nn.Conv2d(in_channels, out_channels, kernel_size=5, stride=1, padding=2) # padding to make sure h,w remains same\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        # self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.conv1x1(x)\n",
    "        out2 = self.conv3x3(x)\n",
    "        out3 = self.conv5x5(x)\n",
    "        # out4 = self.maxpool(x)\n",
    "        return torch.cat([out1, out2, out3], dim=1)\n",
    "        # return [out1, out2, out3, out4]\n",
    "\n",
    "\n",
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels_1, out_channels_3, out_channels_5, out_channels_dim_red_3, out_channels_dim_red_5, pool_proj_dim):\n",
    "        super(InceptionBlock, self).__init__()\n",
    "        self.conv1x1 = ConvReLU(in_channels, out_channels_1, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "        self.dim_reduction_3x3 = ConvReLU(in_channels, out_channels_dim_red_3, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv3x3 = ConvReLU(out_channels_dim_red_3, out_channels_3, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.dim_reduction_5x5 = ConvReLU(in_channels, out_channels_dim_red_5, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv5x5 = ConvReLU(out_channels_dim_red_5, out_channels_5, kernel_size=5, stride=1, padding=2)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.pool_proj = ConvReLU(in_channels, pool_proj_dim, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.conv1x1(x)\n",
    "        \n",
    "        out2 = self.dim_reduction_3x3(x)\n",
    "        out2 = self.conv3x3(out2)\n",
    "        \n",
    "        out3 = self.dim_reduction_5x5(x)\n",
    "        out3 = self.conv5x5(out3)\n",
    "        \n",
    "        out4 = self.maxpool(x)\n",
    "        out4 = self.pool_proj(out4)\n",
    "        # return [out1, out2, out3, out4]\n",
    "        return torch.cat([out1, out2, out3, out4], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 192, 64, 64])\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "NaiveInceptionBlock                      [64, 192, 64]             --\n",
      "├─Conv2d: 1-1                            [64, 64, 64]              16,448\n",
      "├─Conv2d: 1-2                            [64, 64, 64]              147,520\n",
      "├─Conv2d: 1-3                            [64, 64, 64]              409,664\n",
      "==========================================================================================\n",
      "Total params: 573,632\n",
      "Trainable params: 573,632\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 2.35\n",
      "==========================================================================================\n",
      "Input size (MB): 4.19\n",
      "Forward/backward pass size (MB): 6.29\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 12.78\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "inp = torch.rand(1, 256, 64, 64) # some intermediate layer output\n",
    "\n",
    "block = NaiveInceptionBlock(256, 64)\n",
    "out = block(inp)\n",
    "print(out.shape)\n",
    "print(summary(NaiveInceptionBlock(256, 64), input_size=(256, 64, 64)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 192, 64, 64])\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "ConvReLU                                 [192, 64, 64]             --\n",
      "├─Conv2d: 1-1                            [192, 64, 64]             442,560\n",
      "├─ReLU: 1-2                              [192, 64, 64]             --\n",
      "==========================================================================================\n",
      "Total params: 442,560\n",
      "Trainable params: 442,560\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 5.44\n",
      "==========================================================================================\n",
      "Input size (MB): 4.19\n",
      "Forward/backward pass size (MB): 6.29\n",
      "Params size (MB): 1.77\n",
      "Estimated Total Size (MB): 12.26\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "conv_block = ConvReLU(256, 192, 3)\n",
    "out = conv_block(inp)\n",
    "print(out.shape)\n",
    "print(summary(conv_block, input_size=(256, 64, 64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 192, 64, 64])\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "InceptionBlock                           [48, 896, 224]            --\n",
      "├─ConvReLU: 1-1                          [48, 224, 224]            --\n",
      "│    └─Conv2d: 2-1                       [48, 224, 224]            12,336\n",
      "├─ConvReLU: 1-10                         --                        (recursive)\n",
      "│    └─ReLU: 2-2                         [48, 224, 224]            --\n",
      "├─ConvReLU: 1-3                          [64, 224, 224]            --\n",
      "│    └─Conv2d: 2-3                       [64, 224, 224]            16,448\n",
      "├─ConvReLU: 1-10                         --                        (recursive)\n",
      "│    └─ReLU: 2-4                         [64, 224, 224]            --\n",
      "├─ConvReLU: 1-5                          [48, 224, 224]            --\n",
      "│    └─Conv2d: 2-5                       [48, 224, 224]            27,696\n",
      "├─ConvReLU: 1-10                         --                        (recursive)\n",
      "│    └─ReLU: 2-6                         [48, 224, 224]            --\n",
      "├─ConvReLU: 1-7                          [32, 224, 224]            --\n",
      "│    └─Conv2d: 2-7                       [32, 224, 224]            8,224\n",
      "├─ConvReLU: 1-10                         --                        (recursive)\n",
      "│    └─ReLU: 2-8                         [32, 224, 224]            --\n",
      "├─ConvReLU: 1-9                          [48, 224, 224]            --\n",
      "│    └─Conv2d: 2-9                       [48, 224, 224]            38,448\n",
      "├─ConvReLU: 1-10                         --                        (recursive)\n",
      "│    └─ReLU: 2-10                        [48, 224, 224]            --\n",
      "├─MaxPool2d: 1-11                        [256, 224, 224]           --\n",
      "├─ConvReLU: 1-12                         [48, 224, 224]            --\n",
      "│    └─Conv2d: 2-11                      [48, 224, 224]            12,336\n",
      "│    └─ReLU: 2-12                        [48, 224, 224]            --\n",
      "==========================================================================================\n",
      "Total params: 115,488\n",
      "Trainable params: 115,488\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 1.27\n",
      "==========================================================================================\n",
      "Input size (MB): 51.38\n",
      "Forward/backward pass size (MB): 115.61\n",
      "Params size (MB): 0.46\n",
      "Estimated Total Size (MB): 167.45\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "out_channels = 192//4\n",
    "block = InceptionBlock(256, out_channels, out_channels, out_channels, 64, 32, out_channels)\n",
    "out = block(inp)\n",
    "print(out.shape)\n",
    "print(summary(block, input_size=(256, 224, 224)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
