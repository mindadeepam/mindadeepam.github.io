<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.53">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2024-07-30">

<title>Intro to Convolution Neural Networks – deepamminda</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">deepamminda</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">about</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html"> 
<span class="menu-text">blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../garden.html"> 
<span class="menu-text">digital garden</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/mindadeepam"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/mindadeepam"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Intro to Convolution Neural Networks</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">cnns</div>
                <div class="quarto-category">deep-learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 30, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#convolutions" id="toc-convolutions" class="nav-link active" data-scroll-target="#convolutions">Convolutions</a></li>
  <li><a href="#neural-networks" id="toc-neural-networks" class="nav-link" data-scroll-target="#neural-networks">Neural networks</a></li>
  <li><a href="#convolutional-layer" id="toc-convolutional-layer" class="nav-link" data-scroll-target="#convolutional-layer">Convolutional layer</a></li>
  <li><a href="#other-layers" id="toc-other-layers" class="nav-link" data-scroll-target="#other-layers">Other layers</a>
  <ul class="collapse">
  <li><a href="#pooling-layer" id="toc-pooling-layer" class="nav-link" data-scroll-target="#pooling-layer">Pooling layer</a></li>
  <li><a href="#activation-layer" id="toc-activation-layer" class="nav-link" data-scroll-target="#activation-layer">Activation layer</a></li>
  <li><a href="#other-jargon" id="toc-other-jargon" class="nav-link" data-scroll-target="#other-jargon">Other jargon</a></li>
  </ul></li>
  <li><a href="#a-simple-cnn-architecture" id="toc-a-simple-cnn-architecture" class="nav-link" data-scroll-target="#a-simple-cnn-architecture">A Simple CNN architecture</a>
  <ul class="collapse">
  <li><a href="#cnn-characteristics" id="toc-cnn-characteristics" class="nav-link" data-scroll-target="#cnn-characteristics">CNN characteristics</a></li>
  </ul></li>
  <li><a href="#lets-train-a-model" id="toc-lets-train-a-model" class="nav-link" data-scroll-target="#lets-train-a-model">Lets train a model</a></li>
  <li><a href="#concluding" id="toc-concluding" class="nav-link" data-scroll-target="#concluding">Concluding</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading">Further Reading</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="00_cnn_basics.out.ipynb" download="00_cnn_basics.out.ipynb"><i class="bi bi-journal-code"></i>Jupyter</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>Hello everyone, in this notebook we’re going to understand the bare fundamentals of convolutional neural networks. We’ll start by getting to know the convolutional operation itself and then proceed to make a very simple CNN model.</p>
<p>The pre-requisites for this post are:</p>
<ul>
<li>Basic understanding of python, numpy and torch.</li>
<li>Basic understanding of neural networks and feed forward networks.</li>
</ul>
<!-- Welcome to the land of vision in deep learning. Gone are the days you need to look at 10 types of thresholding and 20 types of data pre-processing and apply a logistic classifier on the outputs of a cnn feature extractor. These days you can just load a pretrained model and watch a decaying training and validation loss and feel good about yourself.

Turns out, this comfort doesnt last long. You eventually get around to understand what these models actually are because you need to! 
And when it comes to vision, you surely cannot leave out CNNs. Sure its all transformers nowadays but CNNs or convolutional neural networks were essentially the first deep learning models to make a significant impact in the field of computer vision and most would agree started the whole deep learning wave in the modern era. AlexNet, VGG, resnets would be considered the pioneer models in this field.

Now you might ask: All this sounds interesting Deepam, but what the hell is a convolution? Let us understand convolutions and build a simple CNN model from scratch. 

The pre-requisites for this post are:

- Basic understanding of python, numpy and torch.
- Basic understanding of neural networks and feed forward networks.
 -->
<section id="convolutions" class="level2">
<h2 class="anchored" data-anchor-id="convolutions">Convolutions</h2>
<p>Convolution is a mathematical operation that combines two signals to produce a third signal, representing how one signal modifies the other. In general terms, convolution is used to apply a filter (or kernel) to a signal or data.</p>
<p>For discrete signals, the convolution of two signals <span class="math inline">\(f[n]\)</span> and <span class="math inline">\(g[n]\)</span> is defined as:</p>
<p><span class="math display">\[(f * g)[n] = \sum_{m=-\infty}^{\infty} f[m] \cdot g[n - m]\]</span></p>
<p>Where: - <span class="math inline">\(f[n]\)</span> is the input signal, - <span class="math inline">\(g[n]\)</span> is the filter (or kernel), - <span class="math inline">\((f * g)[n]\)</span> is the resulting convolved signal.</p>
<p>This operation involves flipping the filter <span class="math inline">\(g[n]\)</span>, shifting it across the input signal <span class="math inline">\(f[n]\)</span>, multiplying the overlapping values, and summing the results to produce the output signal.</p>
<section id="steps-in-convolution" class="level4">
<h4 class="anchored" data-anchor-id="steps-in-convolution">Steps in Convolution</h4>
<p>Convolution involves the following steps:</p>
<ol type="1">
<li><p><strong>Prepare the Kernel</strong>: Take the transpose(flip) of the kernel <span class="math inline">\((g)\)</span> along the resolution dimensions (<span class="math inline">\(H,W\)</span>).<a href="https://dsp.stackexchange.com/questions/5992/flipping-the-impulse-response-in-convolution/6355#6355">see why</a></p></li>
<li><p><strong>Align and Slide</strong>: Position the flipped kernel at the start of the input signal <span class="math inline">\((f)\)</span>. Then, systematically slide it across the entire length of <span class="math inline">\((f)\)</span>.</p></li>
<li><p><strong>Multiply</strong>: At each position, perform element-wise multiplication between the overlapping portions of the flipped kernel and the input signal.</p></li>
<li><p><strong>Sum</strong>: Add up all the products from step 3 to get a single value. This value represents the convolution result at the current position.</p></li>
<li><p><strong>Record and Repeat</strong>: Store the sum as an element in the output signal, then move the kernel to the next position and repeat steps 3-5 until the entire input signal has been covered.</p></li>
</ol>
<p>The resulting output signal represents how the kernel has “filtered” or modified the input signal, highlighting certain features or patterns based on the kernel’s characteristics.</p>
<p>This process can be extended to 2D (for images) or higher dimensions, where the kernel slides over the input in all dimensions.</p>
<p>For a 1d array this might look like this:</p>
<div id="cell-5" class="cell" data-execution_count="73">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np </span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> np.array([<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="op">-</span><span class="dv">1</span>, <span class="dv">10</span>,<span class="dv">1</span>])</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> np.array([<span class="dv">1</span>,<span class="dv">0</span>,<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>])</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>inverted_g <span class="op">=</span> g[::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"f: </span><span class="sc">{</span>f<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"g: </span><span class="sc">{</span>g<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"inverted_g, g`: </span><span class="sc">{</span>inverted_g<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>window_size <span class="op">=</span> g.shape[<span class="dv">0</span>]</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> []</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Steps in convolution, slide g` over f in a loop:"</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"'* is dot product' </span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"result = f*g = [_ _ _ _]</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, i <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">range</span>(<span class="bu">len</span>(f)<span class="op">-</span>window_size<span class="op">+</span><span class="dv">1</span>)):</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    slice_of_f <span class="op">=</span> f[i:i<span class="op">+</span>window_size]</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"step </span><span class="sc">{</span>idx<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">. </span><span class="ch">\n</span><span class="sc">{</span>slice_of_f<span class="sc">}</span><span class="ss"> * </span><span class="sc">{</span>inverted_g<span class="sc">}</span><span class="ss"> = </span><span class="sc">{</span>slice_of_f<span class="sc">.</span>dot(inverted_g)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(f"step {idx+1}.2 sum({slice_of_f * inverted_g}) = {np.sum(slice_of_f * inverted_g)}")</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    result.append(np.<span class="bu">sum</span>(slice_of_f <span class="op">*</span> inverted_g))</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"result[</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">]: </span><span class="sc">{</span>result[i]<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"result: </span><span class="sc">{</span>result<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>f: [ 1  2  3 -1 10  1]
g: [ 1  0 -1  1]
inverted_g, g`: [ 1 -1  0  1]

Steps in convolution, slide g` over f in a loop:
'* is dot product' 

result = f*g = [_ _ _ _]

step 1. 
[ 1  2  3 -1] * [ 1 -1  0  1] = -2
result[1]: -2

step 2. 
[ 2  3 -1 10] * [ 1 -1  0  1] = 9
result[2]: 9

step 3. 
[ 3 -1 10  1] * [ 1 -1  0  1] = 5
result[3]: 5

result: [-2, 9, 5]</code></pre>
</div>
</div>
<p>Consider <code>len(f)=M</code> &amp; <code>len(g)=N</code>. We also have a few types of convolutions, based on how much padding is added to the input signal:</p>
<ol type="1">
<li><strong>Valid Convolution</strong>:
<ul>
<li>The output is smaller than the input. No padding is applied.</li>
<li>Mode ‘valid’ returns output of length <code>max(M, N) - min(M, N) + 1</code>.</li>
</ul></li>
<li><strong>Same Convolution</strong>:
<ul>
<li>Padding is applied to keep the output size the same as the input.</li>
<li>Mode ‘same’ returns output of length <code>max(M, N)</code>.</li>
</ul></li>
</ol>
<div id="cell-8" class="cell" data-execution_count="88">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"np.convolve(f,g, mode='same') -&gt; </span><span class="sc">{</span>np<span class="sc">.</span>convolve(f,g, mode<span class="op">=</span><span class="st">'same'</span>)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>np.convolve(f,g, mode='same') -&gt; [  2   2  -2   9   5 -11]</code></pre>
</div>
</div>
<ol start="3" type="1">
<li><strong>Full Convolution</strong>:
<ul>
<li>Maximum padding is applied.</li>
<li>This returns the convolution at each point of overlap, with an output shape of (N+M-1,).</li>
</ul></li>
</ol>
<div id="cell-10" class="cell" data-execution_count="89">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"full f*g </span><span class="ch">\n</span><span class="ss">-&gt; </span><span class="sc">{</span>np<span class="sc">.</span>pad(f,<span class="bu">len</span>(g)<span class="op">-</span><span class="dv">1</span>)<span class="sc">}</span><span class="ss"> * </span><span class="sc">{</span>g[::<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"-&gt; </span><span class="sc">{</span>np<span class="sc">.</span>convolve(f,g, mode<span class="op">=</span><span class="st">'full'</span>)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>full f*g 
-&gt; [ 0  0  0  1  2  3 -1 10  1  0  0  0] * [ 1 -1  0  1]
-&gt; [  1   2   2  -2   9   5 -11   9   1]</code></pre>
</div>
</div>
<p>That wasnt so hard was it? Lets look at an example for 2d arrays.</p>
<p>Lets take 2 matrices <span class="math inline">\(f\)</span> &amp; <span class="math inline">\(g\)</span> of shapes (4,4) and (3,3). Since <span class="math inline">\(g\)</span> is a square kernel with side=3, we say it has size 3. First we need to flip the <span class="math inline">\(g\)</span> both horizontally and vertically. Then, for each <span class="math inline">\(g\)</span> sized block in <span class="math inline">\(f\)</span>, we do element-wise multiplication then summation.</p>
<p>Lets see how this works.</p>
<div class="quarto-figure quarto-figure-center" style="display: block; margin: auto;">
<figure class="figure">
<p><img src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*hOI0jW3CcS_yuxcmJIYjKw.gif" class="img-fluid figure-img" width="300"></p>
<figcaption>2d conv</figcaption>
</figure>
</div>
<div id="cell-13" class="cell" data-execution_count="90">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> np.random.rand(<span class="dv">4</span>,<span class="dv">4</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> np.array([[<span class="dv">1</span>,<span class="dv">0</span>,<span class="op">-</span><span class="dv">1</span>],[<span class="dv">2</span>,<span class="dv">0</span>,<span class="op">-</span><span class="dv">2</span>],[<span class="dv">3</span>,<span class="dv">0</span>,<span class="op">-</span><span class="dv">3</span>]])</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>inverted_g <span class="op">=</span> g[::<span class="op">-</span><span class="dv">1</span>, ::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"(f): </span><span class="ch">\n</span><span class="sc">{</span>f<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"(g): </span><span class="ch">\n</span><span class="sc">{</span>g<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"(g`): </span><span class="ch">\n</span><span class="sc">{</span>inverted_g<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>(f): 
[[0.56832851 0.1169398  0.62616176 0.94949466]
 [0.06633587 0.83629314 0.79893411 0.07711767]
 [0.11524299 0.48151466 0.3024186  0.34043628]
 [0.76204227 0.80463651 0.4008499  0.4356766 ]]

(g): 
[[ 1  0 -1]
 [ 2  0 -2]
 [ 3  0 -3]]

(g`): 
[[-3  0  3]
 [-2  0  2]
 [-1  0  1]]
</code></pre>
</div>
</div>
<div id="cell-14" class="cell" data-execution_count="91">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>f_rows, f_columns <span class="op">=</span> f.shape </span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>g_rows, g_columns <span class="op">=</span> g.shape</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> np.zeros((f_rows<span class="op">-</span>g_rows<span class="op">+</span><span class="dv">1</span>, f_columns<span class="op">-</span>g_columns<span class="op">+</span><span class="dv">1</span>))</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Convolution steps:"</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(f_rows<span class="op">-</span>g_rows<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(f_columns<span class="op">-</span>g_columns<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>        item <span class="op">=</span> f[i:i<span class="op">+</span>g_rows, j:j<span class="op">+</span>g_columns] <span class="op">*</span> inverted_g</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i<span class="op">&lt;</span><span class="dv">3</span> <span class="kw">and</span> j<span class="op">==</span><span class="dv">0</span>:</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"step </span><span class="sc">{</span>i<span class="op">+</span>j<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ch">\n</span><span class="ss">-&gt; f[</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>i<span class="op">+</span>g_rows<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>j<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>j<span class="op">+</span>g_columns<span class="sc">}</span><span class="ss">] * g`: </span><span class="ch">\n</span><span class="sc">{</span>item<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"result[</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">,</span><span class="sc">{</span>j<span class="sc">}</span><span class="ss">] -&gt; </span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">sum</span>(item)<span class="sc">:.4f}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>        result[i,j] <span class="op">=</span> np.<span class="bu">sum</span>(item)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"... and so on.</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"final result: </span><span class="ch">\n</span><span class="sc">{</span>result<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Convolution steps:
step 1
-&gt; f[0:3, 0:3] * g`: 
[[-1.70498553  0.          1.87848528]
 [-0.13267174  0.          1.59786822]
 [-0.11524299  0.          0.3024186 ]]
result[0,0] -&gt; 1.8259

step 2
-&gt; f[1:4, 0:3] * g`: 
[[-0.1990076   0.          2.39680234]
 [-0.23048599  0.          0.60483719]
 [-0.76204227  0.          0.4008499 ]]
result[1,0] -&gt; 2.2110

... and so on.

final result: 
[[ 1.82587185  0.83823527]
 [ 2.21095358 -2.92864308]]
</code></pre>
</div>
</div>
<p><strong>Padding</strong>: Padding is now applied on both the height and width dimensions.</p>
<p><strong>Stride</strong>: You can see that the kernel shifts by one pixel everytime. This is the stride of this conv operation. It is generally applied symmetrically across <span class="math inline">\(H\)</span> &amp; <span class="math inline">\(W\)</span>. You can see how doubling the stride would reduce output size by half. In general, larger stride → Smaller output (since we’re skipping pixels)</p>
<div class="quarto-figure quarto-figure-center" style="display: block; margin: auto;">
<figure class="figure">
<p><img src="https://lilianweng.github.io/posts/2017-12-15-object-recognition-part-2/numerical_no_padding_no_strides.gif" class="img-fluid figure-img" width="400"></p>
<figcaption>1</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center" style="display: block; margin: auto;">
<figure class="figure">
<p><img src="https://lilianweng.github.io/posts/2017-12-15-object-recognition-part-2/numerical_padding_strides.gif" class="img-fluid figure-img" width="400"></p>
<figcaption>2</figcaption>
</figure>
</div>
<p><em>Two examples of 2D convolution operation: (1) no padding and 1x1 strides. (2) padding = 1 and 2x2 strides. (Image source: deeplearning.net)</em></p>
</section>
</section>
<section id="neural-networks" class="level2">
<h2 class="anchored" data-anchor-id="neural-networks">Neural networks</h2>
<p>Now that we clearly understand what a convolution is, lets get back to neural networks. Suppose you wanted to classify images before CNNs were developed. How would you do it? In most cases, you would use some pre-determined filters to extract features from the images and then use a fully connected network to classify the images. Those features could be:</p>
<ol type="1">
<li><p>Edge filters: Used to detect edges in images. Common edge detection filters include:</p>
<ul>
<li>Sobel filter: Emphasizes horizontal or vertical edges</li>
<li>Prewitt filter: Similar to Sobel, but with different coefficients</li>
<li>Laplacian filter: Detects edges in all directions</li>
</ul></li>
<li><p>Texture filters: Used to capture texture information:</p>
<ul>
<li>Gabor filters: Detect specific frequencies and orientations</li>
<li>Laws’ texture energy measures: A set of filters for texture analysis</li>
</ul></li>
<li><p>Color histograms: Represent the distribution of colors in an image</p></li>
<li><p>SIFT (Scale-Invariant Feature Transform): Detects and describes local features in images</p></li>
<li><p>HOG (Histogram of Oriented Gradients): Counts occurrences of gradient orientations in localized portions of an image</p></li>
<li><p>Haar-like features: Used in face detection, these features look at rectangular regions and sum up pixel intensities</p></li>
</ol>
<p>After applying these filters, you would typically:</p>
<ol type="1">
<li>Extract the resulting features</li>
<li>Possibly apply dimensionality reduction techniques like PCA</li>
<li>Feed the processed features into a classifier such as SVM, Random Forest, or a simple neural network</li>
</ol>
<p>This approach, while effective for certain tasks, had limitations: - Handcrafted features might not capture all relevant information - The process was often computationally expensive - Feature engineering required domain expertise</p>
<p>CNNs addressed these issues by learning the filters automatically during training, leading to more effective and adaptable vision models. Let us look at all the important components of a CNN in the next section.</p>
</section>
<section id="convolutional-layer" class="level2">
<h2 class="anchored" data-anchor-id="convolutional-layer">Convolutional layer</h2>
<p>We now understand why handcrafted fatures are hard to come up with. Lets back up for a second and remember our old friends feed forward (FF) networks. What is the problem with using feed forward networks to process images? Specifically why dont we just give the FF network the raw pixels and let it learn?</p>
<p>As it turns out, there are quite a few things:</p>
<p>Suppose we have a (224,224,3) sized input-image. Thats a total of 224*224*3 = 150,528 data-points. suppose i want to have a output vector of length 1000 (classifier with 1k classes), this gives me a total of roughly 150k * 1k = 150 million parameters, where we have 1k parameters for each input pixel! That is a LOTT of parameters.</p>
<p>So how can we use the inherent nature of images and image-data to construct better models? We also learned convolutions in the previous section. Maybe that gives you some hint?</p>
<p>Pixel data is inherently very locally dependent. This is very different from tabular data, for example, a housing price dataset where each feature is largely independent of each other. By design feedforward networks do not allow for this kind of information to be shared between different data points in the network. This is beacause for each linear layer, a output shaped weight matrix is learned for each point in the input. This seems very inefficient.</p>
<p>What if instead of treating each individual pixel as a different input, we treat a patch of pixels as a single input? This means we get one output item for a patch of pixels, and our weights can <em>“slide”</em> across an image to produce our output. And if that sounds like something we were doing earlier, you are right!</p>
<p>Enter convolution. The weights will basically be filters we learn. And since the same filter is applied to all the patches in a conv operation, there are a lot less pararmeters to learn.</p>
<p>We can learn a lot of filters that will interact with our image and get us some useful features which can then be fed to a FF network.</p>
<p>A filter always has the same number of dimensions as the input. Images are 3 dimensional tensors <span class="math inline">\((H_{in}, W_{in}, C_{in})\)</span>. The kernel will then be a tensor of size <span class="math inline">\(k,k, C_{in}\)</span>. A single step in their convolution that produces 1 output is the dot product between the kernel and a (<span class="math inline">\(k,k\)</span>) window of the input image. Hence the output is a 2d matrix.</p>
<p>But we learn a lot of these filters, say <span class="math inline">\(C_{out}\)</span>. So we get <span class="math inline">\(C_{out}\)</span> such 2d, matrices. So if we consider</p>
<ul>
<li>input block shape -&gt; <span class="math inline">\((H_{in}, W_{in}, C_{in})\)</span></li>
<li>filter shape -&gt; <span class="math inline">\(k, k, C_{in}\)</span></li>
<li>number of filters -&gt; <span class="math inline">\(C_{out}\)</span></li>
<li>The outputs will be a 3d tensor of shape <span class="math inline">\((H_{out}, W_{out}, C_{out})\)</span>, where <span class="math inline">\((H_{out}, W_{out}\)</span> are determined by input resolution and the padding &amp; stride of the convoution operation.</li>
</ul>
<p><img src="https://qph.cf2.quoracdn.net/main-qimg-b662a8fc3be57f76c708c171fcf29960.png" class="img-fluid" style="display: block; margin: auto;" width="300"></p>
<!-- But images are 3 dimensionsal you say! Sure they are -->
<!-- This is exactly what a convolutional layer does. It consists of a set of learnable filters that slide over the input image and compute the output. If the input image is of size $(H_{in}, W_{in}, C_{in})$, and we have $C_{out}$ filters of size $(k,k,C_{in})$, the output of the convolutional layer is of size $(H_{out}, W_{out}, C_{out})$. 
Each filter slides over the entire input volume and produces a 2d matrix of size $(H_{out}, W_{out})$. And this is done for all the filters in the layer. So the output of the convolutional layer is a 3d tensor of size $(H_{out}, W_{out}, C_{out})$. -->
<!-- Note that the filters will always have depth equal to the input volume's depth. -->
<p>The number of parameters learned here would be <span class="math inline">\((k*k*C_{in}*C_{out})\)</span>. So for a conv layer with 64 filters of size (3x3) for an input image of size (224x224x3), we would have 1792 parameters, which is orders of magnitutde smaller already. Much more efficient isn’t it?</p>
<!-- 
1. Feed forward networks do not share information between different data points in the network. For example if X = [0,1,2,3] is a vector of input to the network, all the interactions between weights happen independently for each data point in one sample of data.
This works fine for tabular features of some data, because they actually are not dependent on their spatial postion wrt each other. The model will learn the same if you shuffle all columns of a dataset. <br><br>But images pixels are a different type of data. They are inherently very locally dependent. If you look at a single pixel, it is very likely to be highly correlated with its neighboring pixels. Hence our network should process atleast patches of images at a time. 

2. Earlier when classical image processing methods were being used, many a times hand-made "filters" were used to extract features from images. In CNNs, we learn these filters! and we learn a lot of them. So think of each layer in this CNN as having a lot of these filters which are learned during network. We'll soon visualize what concepts they learn.

3. Another concept that helps is parameter sharing. We could in thoery have different set of filters for different patches in the image, but this would shoot up the parameter count of our model. So in a single layer, we use a single set of filters for all patches in the image. -->
<!-- 
```
# import torch.nn as nn

# def get_total_params(model):
#     return sum(p.numel() for p in model.parameters())

# model = nn.Linear(150528, 1000)
# print(f"Total params in a linear layer of shape (150528, 1000): {get_total_params(model)}")

# model = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)
# print(f"Total params in a conv layer with kernel size (3,3) and 64 filters: {get_total_params(model)}")
``` -->
<p>Let us visualize the shapes of the input and output of a convolutional layer. (ignoring batch dimension for simplicity)</p>
<div class="quarto-figure quarto-figure-center" style="display: block; margin: auto;">
<figure class="figure">
<p><img src="https://miro.medium.com/v2/resize:fit:1400/1*ubmJTEy3edn5QYm5hNPmVg@2x.gif" class="img-fluid figure-img" width="400"></p>
<figcaption>convolution in CNNs</figcaption>
</figure>
</div>
<p>Let’s break down the convolution operation. Key Parameters are:</p>
<p><strong>Inputs</strong> are of shape <span class="math inline">\((H_{in}, W_{in}, C_{in})\)</span></p>
<p><strong>Conv layer parameters</strong>:</p>
<ul>
<li>Kernel size: <span class="math inline">\(k\)</span></li>
<li>Number of output channels: <span class="math inline">\(C_{out}\)</span></li>
<li>Stride (<span class="math inline">\(S\)</span>): Doubling the stride will reduce output size by half.</li>
<li>Padding (<span class="math inline">\(P\)</span>): Extra zeros added around the input image edges. Image can be padded with other values too. padding is done on both sides of the height and width dimensions.</li>
<li>Dilation (<span class="math inline">\(D\)</span>): a parameter that controls the stride of elements in the window. default=1.</li>
</ul>
<p><strong>Outputs</strong> are of shape <span class="math inline">\((H_{out}, W_{out}, C_{out})\)</span>, where</p>
<ul>
<li><span class="math inline">\(H_{out} = (H_{in} - k + 2P) / S + 1\)</span></li>
<li><span class="math inline">\(W_{out} = (W_{in} - k + 2P) / S + 1\)</span></li>
</ul>
<p>For a visual understanding of how all these parameters interact, check out this <a href="https://ezyang.github.io/convolution-visualizer/">convolution visualizer</a>.</p>
<!-- > Readers would likely also be interested in knowing how the convolution operation is implemented as a matrix multiplication in gpus. We have here the concepts of img2col and col2img where -->
<!-- > Readers would likely also be interested in knowing how the convolution operation is implemented as a matrix multiplication in GPUs. We have here the concepts of img2col and col2img where

The `img2col` operation transforms the input image into a 2D matrix where each row corresponds to a sliding window of the image. This allows the convolution operation to be expressed as a matrix multiplication, which is highly optimized on GPUs. 

The transformation can be represented as:
$$
\text{img2col}(I) = \begin{bmatrix}
I_{1} & I_{2} & \ldots & I_{n}
\end{bmatrix}
$$
where $I_{k}$ represents the flattened sliding window of the image.

Conversely, `col2img` takes the output of the matrix multiplication and reconstructs it back into the original image format. This can be represented as:
$$
\text{col2img}(C) = I
$$
where $C$ is the output matrix from the convolution operation.

This approach leverages the efficiency of matrix operations, enabling faster computations for convolutional layers in deep learning models.

By using these techniques, frameworks like TensorFlow and PyTorch can perform convolutions much more efficiently, making it feasible to train large models on high-resolution images. -->
<section id="img2col-and-col2img" class="level4">
<h4 class="anchored" data-anchor-id="img2col-and-col2img">img2col and col2img</h4>
<p>Readers would likely also be interested in knowing how the convolution operation is implemented as a matrix multiplication in GPUs. We have here the concepts of <strong>img2col</strong> and <strong>col2img</strong>.</p>
<ol type="1">
<li><strong>img2col Transformation</strong></li>
</ol>
<!-- Consider an input image (or feature map) $\mathbf{I}$ of size $H \times W$ with $C$ channels, and a convolutional kernel $\mathbf{K}$ of size $k_H \times k_W$ with $C$ input channels and $M$ output channels. -->
<p><strong>img2col Formula</strong>: The img2col transformation converts the input image into a matrix <span class="math inline">\(\mathbf{X}\)</span> where each column corresponds to a flattened patch of the image that the kernel would convolve over:</p>
<p><span class="math display">\[\mathbf{X} \in \mathbb{R}^{(k_H \cdot k_W \cdot C) \times (H' \cdot W')}\]</span></p>
<p>Here: - <span class="math inline">\(H' = \frac{H - k_H}{s} + 1\)</span> and <span class="math inline">\(W' = \frac{W - k_W}{s} + 1\)</span> are the height and width of the output feature map, where <span class="math inline">\(s\)</span> is the stride. - Each column of <span class="math inline">\(\mathbf{X}\)</span> represents the flattened <span class="math inline">\(k_H \times k_W \times C\)</span> patch from the input image.</p>
<ol start="2" type="1">
<li><strong>Kernel Reshaping</strong></li>
</ol>
<p>The convolutional kernel <span class="math inline">\(\mathbf{K}\)</span> is reshaped into a matrix <span class="math inline">\(\mathbf{W}\)</span> to facilitate matrix multiplication:</p>
<p><span class="math display">\[\mathbf{W} \in \mathbb{R}^{M \times (k_H \cdot k_W \cdot C)}\]</span></p>
<p>Here: - <span class="math inline">\(M\)</span> is the number of output channels. - Each row of <span class="math inline">\(\mathbf{W}\)</span> corresponds to one flattened convolutional filter.</p>
<ol start="3" type="1">
<li><strong>Matrix Multiplication</strong></li>
</ol>
<p>The convolution operation can now be performed as a matrix multiplication:</p>
<p><span class="math display">\[\mathbf{Y} = \mathbf{W} \cdot \mathbf{X}\]</span></p>
<p>Where: - <span class="math inline">\(\mathbf{Y} \in \mathbb{R}^{M \times (H' \cdot W')}\)</span> is the output matrix.</p>
<ol start="4" type="1">
<li><strong>col2img Transformation</strong></li>
</ol>
<p>After the matrix multiplication, the output matrix <span class="math inline">\(\mathbf{Y}\)</span> is reshaped back into the spatial dimensions of the output feature map using the col2img operation:</p>
<p><span class="math display">\[\mathbf{O} \in \mathbb{R}^{H' \times W' \times M}\]</span></p>
<p>This reshaping restores the 2D spatial structure of the output feature map.</p>
<p>These transformations enable the convolution operation to leverage efficient matrix multiplication on GPUs, significantly speeding up the process.</p>
</section>
</section>
<section id="other-layers" class="level2">
<h2 class="anchored" data-anchor-id="other-layers">Other layers</h2>
<p>Conolutional layers are usually followed by activation functions like ReLU and then a pooling layer.</p>
<section id="pooling-layer" class="level3">
<h3 class="anchored" data-anchor-id="pooling-layer">Pooling layer</h3>
<p>Pooling layer? Yes, pooling layer. Pooling layer is a means to perform downsampling of an image. Given a volume of input, instead of using filters, we can use non-parameterized operations like min/max/avg to downsample the data and get a smaller volume. It reduces the H and W dimensions of the input volume, keeping the depth the same. But why would we do something like that? Two main reasons:</p>
<ol type="1">
<li>Reduce the number of parameters: By reducing the number of parameters, the model can learn more general features.</li>
<li>Reduce the amount of overfitting: Since pooling leads to some information loss, it also acts as a good regularizer.</li>
</ol>
<p>The entire operation is the same as the convolutional layer except instead of convolving <span class="math inline">\(f\)</span> with a filter <span class="math inline">\(g\)</span> of size <span class="math inline">\((k,k, C_{in})\)</span>, we take the max/mean of <span class="math inline">\(f\)</span> for each patch of size <span class="math inline">\((k,k)\)</span>. This is done separately for each input channel.</p>
<p>Let’s look at an example of max pooling, which is the most common type of pooling:</p>
<div id="cell-24" class="cell" data-execution_count="93">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Input feature map</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span> <span class="op">=</span> np.array([</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>[<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>],</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>[<span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">4</span>, <span class="dv">2</span>],</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>[<span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">9</span>, <span class="dv">4</span>],</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">5</span>]</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Max pooling with 2x2 filter and stride 2</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> max_pool(<span class="bu">input</span>, filter_size, stride):</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    height, width <span class="op">=</span> <span class="bu">input</span>.shape</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    output_height <span class="op">=</span> (height <span class="op">-</span> filter_size) <span class="op">//</span> stride <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    output_width <span class="op">=</span> (width <span class="op">-</span> filter_size) <span class="op">//</span> stride <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> np.zeros((output_height, output_width))</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(output_height):</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(output_width):</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>            start_i <span class="op">=</span> i <span class="op">*</span> stride</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>            start_j <span class="op">=</span> j <span class="op">*</span> stride</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>            window <span class="op">=</span> <span class="bu">input</span>[start_i:start_i<span class="op">+</span>filter_size, start_j:start_j<span class="op">+</span>filter_size]</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>            output[i, j] <span class="op">=</span> np.<span class="bu">max</span>(window)</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> max_pool(<span class="bu">input</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Input:"</span>)</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">input</span>)</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">After 2x2 Max Pooling:"</span>)</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"""Result: </span></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a><span class="ss">[[ max(f[0:2, 0:2]) max(f[0:2, 2:4])]</span></span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a><span class="ss"> [ max(f[2:4, 0:2]) max(f[2:4, 2:4])]]</span></span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a><span class="ss">"""</span>)</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(result)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Input:
[[1 3 2 1]
 [5 6 4 2]
 [7 8 9 4]
 [1 2 3 5]]

After 2x2 Max Pooling:
Result: 
[[ max(f[0:2, 0:2]) max(f[0:2, 2:4])]
 [ max(f[2:4, 0:2]) max(f[2:4, 2:4])]]

[[6. 4.]
 [8. 9.]]</code></pre>
</div>
</div>
<p>As you can see, max pooling takes the maximum value in each 2x2 region, effectively reducing the spatial dimensions of the feature map while retaining the most prominent features. Generally a stride of 2 is used, which means that pooling results in half the resolution as the input (h,w).</p>
</section>
<section id="activation-layer" class="level3">
<h3 class="anchored" data-anchor-id="activation-layer">Activation layer</h3>
<p>Activations used are sigmoid, ReLu, and tanh. Since VGGs and deeper networks use ReLu, it is common to see ReLu used in CNNs. This is because ReLu partially solves the vanishing gradient problem, which is a common problem in deep networks. Read more about this <a href="https://stats.stackexchange.com/a/240491">here</a>.</p>
<p>But what the ReLU layer does is very simple, it applies max(0, x) to each element in the input. Effectively, it zeros out negative values, and leaves positive values unchanged.</p>
</section>
<section id="other-jargon" class="level3">
<h3 class="anchored" data-anchor-id="other-jargon">Other jargon</h3>
<p>Before we dive into training a model, let’s review some important jargon related to CNNs:</p>
<ul>
<li><strong>activation maps</strong>: the output when a <code>\[Conv-&gt;Relu\]</code> filter interacts with entire image.</li>
<li><strong>feature maps</strong>: the output of a convolutional layer. (before the activation)</li>
<li><strong>depth</strong>: refers to the number of channels.</li>
<li><strong>receptive field</strong>: the area of the input that a given filter is able to see at a time is called the receptive field. If a filter is of size (<span class="math inline">\(k\)</span>) its receptive field is of size (<span class="math inline">\(k*k\)</span>)</li>
</ul>
<!-- ## CNNs from scratch!
 
Let us implement a CNN from scratch. 

Starting with the Convolutional layer.

```python
import torch 

class ConvLayer:
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.weights = torch.randn(out_channels, in_channels, kernel_size, kernel_size)
        self.biases = torch.zeros(out_channels)

    def forward(self, x):
        if x.ndim != 4:
            raise ValueError("Input tensor must have 4 dimensions: (batch_size, in_channels, height, width)")
        if x.shape[1] != self.in_channels:
            raise ValueError(f"Expected input shape to have {self.in_channels} channels, but got {x.shape[1]}")
'''
# broadcast weights across batch, and then perform conv operation
# conv operation
    # apply padding
    start with a patch  of k sized kernel and do element wise operation in 2 nested loops (across H and W)
    
    B*C*H*W * 
'''
``` -->
<p>Now we have all the basic components we need for a tiny cnn model.</p>
</section>
</section>
<section id="a-simple-cnn-architecture" class="level2">
<h2 class="anchored" data-anchor-id="a-simple-cnn-architecture">A Simple CNN architecture</h2>
<p>Lets assume a simple task of image classification. The most common form of a CNN architecture stacks a few CONV-RELU layers, follows them with POOL layers, and repeats this pattern until the image volume has reduced to a small size. As the number of the layers increase, the resolution becomes smaller and smaller and depth of the activation maps increases.</p>
<p>At some point, it is common to transition to fully-connected(FC) layers. The final activation maps of the conv layers is pooled and flattened to a 2d matrix, ie <span class="math inline">\((B,C,H,W)-&gt;(B,C*H*W)\)</span>. The result then goes to the FC layers. The last fully-connected layer gives the output, such as the class scores. In other words, the most common ConvNet architectures follow the pattern:</p>
<p><code>INPUT -&gt; [[CONV -&gt; RELU]*N -&gt; POOL?]*M -&gt;flatten-&gt; [FC -&gt; RELU]*K -&gt; FC</code></p>
<p>Note that since architectures like <a href="https://arxiv.org/pdf/1512.03385">Resnet</a> and <a href="https://arxiv.org/pdf/1409.4842">Inception</a> emerged, this is not the case, and the CNNs feature more intricate and different connectivity structures.</p>
<section id="cnn-characteristics" class="level3">
<h3 class="anchored" data-anchor-id="cnn-characteristics">CNN characteristics</h3>
<p>While we are here, let us also take note of some characteristics of CNNs:</p>
<ul>
<li><strong>Sparse connectivity</strong>: CNNs focus on local patterns in data, particularly useful for spatial data like images. A single patch in feature map is connected to only a small patch of image (in MLPs there is dense/full connection).</li>
<li><strong>Parameter sharing</strong>: the same kernel/filter slides across the image. ie different neurons in each activation map is calculated using the same filter. In MLPs each neuron in the output space is calculated using different weight values. this makes it efficient for computation.</li>
<li><strong>Spatial hierarchy</strong>: CNNs build a hierarchy of increasingly abstract features. Lower layers detect simple features (e.g., edges), while deeper layers combine these to detect more complex patterns.</li>
<li><strong>Translation invariance</strong>: CNNs can recognize patterns regardless of their position in the input. This is because we are using filters that slide over patches of data, so information is processed in the same way for different patches of data This is crucial for tasks like object recognition in images.</li>
</ul>
<p>Have a look at <a href="https://arxiv.org/pdf/1311.2901">this</a> wonderful paper that dives deep into visualizing and understanding Cnns.</p>
<div id="fig-cnn-activation-maps" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-activation-maps-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="visualizing-activation-maps-cnns.png" style="width:90.0%" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-activation-maps-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: visualizing activation maps in cnns. From the paper ‘Visualizing and Understanding Convolutional Networks’
</figcaption>
</figure>
</div>
<p>Now lets build a CNN model step by step.</p>
<section id="convrelu-layer" class="level4">
<h4 class="anchored" data-anchor-id="convrelu-layer">ConvReLU Layer</h4>
<p>The first block will be <code>CONV-&gt;RELU</code>. In torch,</p>
<div id="cell-33" class="cell" data-execution_count="100">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch </span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ConvReLU(nn.Module):</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co">    A single conv layer followed by batchnorm and activation.</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels, out_channels, kernel_size, activation<span class="op">=</span>nn.ReLU(), stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv <span class="op">=</span> nn.Conv2d(in_channels, out_channels, kernel_size<span class="op">=</span>kernel_size, stride<span class="op">=</span>stride, padding<span class="op">=</span>padding)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activation <span class="op">=</span> activation</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.activation <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> <span class="va">self</span>.activation(<span class="va">self</span>.conv(x))</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> <span class="va">self</span>.conv(x)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Lets create the conv1 layer and visualize the input and output volumes. The input is a rgb image of size <code>224x224x3</code>. We’ll adjust the padding and stride such that the resolution remains the same.</p>
<div id="cell-35" class="cell" data-execution_count="107">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>input_img <span class="op">=</span> torch.rand(<span class="dv">224</span>, <span class="dv">224</span>, <span class="dv">3</span>)   </span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co"># input images must have shape (C,H,W) this is usually managed by torch's to_tensor transform</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>input_img <span class="op">=</span> torch.permute(input_img, (<span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">1</span>))      </span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>conv_layer_1 <span class="op">=</span> ConvReLU(<span class="dv">3</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>) </span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>conv_layer_2 <span class="op">=</span> ConvReLU(<span class="dv">64</span>, <span class="dv">128</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"input volume = </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(input_img.shape))</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"output volume after conv_layer_1 (num_channels 3-&gt;64) = </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(conv_layer_1(input_img).shape))</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"output volume after conv_layer_2 (num_channels 64-&gt;128)= </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(conv_layer_2(conv_layer_1(input_img)).shape))</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="co"># print("output volume after max_pool = {}".format(max_pool(conv_layer_2(conv_layer_1(input_img))).shape))</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>input volume = torch.Size([3, 224, 224])
output volume after conv_layer_1 = torch.Size([64, 224, 224])
output volume after conv_layer_2 = torch.Size([128, 224, 224])</code></pre>
</div>
</div>
</section>
<section id="block-of-convrelu-layers" class="level4">
<h4 class="anchored" data-anchor-id="block-of-convrelu-layers">Block of ConvReLU layers</h4>
<!-- Now to make a block of VGG, we combine multiple such layers together. If you see the @fig-vgg, each block's has a red layer in the beginning. The red layer is a max_pool layer with stride=2 and kernel_size=2. This results in the $H,W$ being halved. Then the subsequent conv_layers preserve the $H,W$ and double the channels. Lets look at a block below: -->
<p>Now lets make a bigger block one that does this operation:</p>
<p><code>Block(n) = [conv_layer_1-&gt;[conv_layer_i]*(n-1)]</code>.</p>
<p>The feature extractor inn our CNN is going to look like this:</p>
<p>[<code>block -&gt; Max_pool -&gt; block -&gt; Max_pool -&gt; block -&gt; ...</code>]</p>
<p>Each block takes as input <span class="math inline">\(C_{in}\)</span> and <span class="math inline">\(C_{out}\)</span> and the number of layers(&gt;1). The first layer will always change <span class="math inline">\(C_{in}\)</span> to <span class="math inline">\(C_{out}\)</span> and the rest of the layers keep all the dimensions intact. Lets create a block class:</p>
<div id="cell-37" class="cell" data-execution_count="141">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Block(nn.Module):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_channels, out_channels, num_layers, activation<span class="op">=</span>nn.ReLU()):</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.out_channels <span class="op">=</span> input_channels<span class="op">*</span><span class="dv">2</span> <span class="cf">if</span> out_channels <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> out_channels</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>        conv_0 <span class="op">=</span> ConvReLU(input_channels, <span class="va">self</span>.out_channels, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>        layers <span class="op">=</span> [conv_0, <span class="op">*</span>[ConvReLU(<span class="va">self</span>.out_channels, <span class="va">self</span>.out_channels, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers<span class="op">-</span><span class="dv">1</span>)]]</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.Sequential(<span class="op">*</span>layers)</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.layers(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Lets analyse a sequence of <code>[BLOCK-&gt;POOL-&gt;BLOCK]</code>.</p>
<div id="cell-39" class="cell" data-execution_count="179">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>max_pool <span class="op">=</span> nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>, stride<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>conv1_block <span class="op">=</span> Block(input_channels<span class="op">=</span><span class="dv">3</span>, out_channels<span class="op">=</span><span class="dv">64</span>, num_layers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"input_img volume = </span><span class="sc">{</span>input_img<span class="sc">.</span>shape<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> conv1_block(input_img)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"output volume after conv1_block (Channels: 3-&gt;64) = </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(x.shape))</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="co"># conv_block_2 has half the h,w (due to max pool right before it) and double the input channels. </span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>conv2_block <span class="op">=</span> Block(input_channels<span class="op">=</span><span class="dv">64</span>, out_channels<span class="op">=</span><span class="dv">128</span>, num_layers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> max_pool(x)</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">output volume after maxpool (halves H,W; preserves C) = </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(x.shape))</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> conv2_block(x)</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">output volume after conv2_block (Channels: 64-&gt;128) = </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(x.shape))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>input_img volume = torch.Size([3, 224, 224])

output volume after conv1_block (Channels: 3-&gt;64) = torch.Size([64, 224, 224])

output volume after maxpool (halves H,W; preserves C) = torch.Size([64, 112, 112])

output volume after conv2_block (Channels: 64-&gt;128) = torch.Size([128, 112, 112])</code></pre>
</div>
</div>
<p>Finally the model.</p>
<p>We will later use a small dataset of (28,28) grayscale images to train this model. Do not pay much heed to the parameters. They have been arbritarily chosen keeping in mind the input size and total model size.</p>
<div id="cell-41" class="cell" data-execution_count="234">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleCNN(nn.Module):</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(SimpleCNN, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pool <span class="op">=</span> nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>, stride<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.final_pool = nn.MaxPool2d(kernel_size=4, stride=4)</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.feature_extractor <span class="op">=</span> nn.Sequential(</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>            Block(<span class="dv">1</span>, <span class="dv">32</span>, <span class="dv">2</span>),</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.pool,</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>            Block(<span class="dv">32</span>, <span class="dv">64</span>, <span class="dv">2</span>),</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.pool,</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>            Block(<span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">2</span>),</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.pool</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classifier <span class="op">=</span> nn.Sequential(</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">128</span> <span class="op">*</span> <span class="dv">3</span> <span class="op">*</span> <span class="dv">3</span>, <span class="dv">2048</span>),</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">2048</span>, <span class="dv">10</span>),</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.feature_extractor(x)</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">128</span> <span class="op">*</span> <span class="dv">3</span> <span class="op">*</span> <span class="dv">3</span>)     <span class="co"># flattening the tensor to feed it to FC layer</span></span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.classifier(x)</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Lets look at the model summary.</p>
<div id="cell-43" class="cell" data-execution_count="236">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchinfo <span class="im">import</span> summary</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SimpleCNN()</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>input_img <span class="op">=</span> torch.rand(<span class="dv">1</span>,<span class="dv">28</span>,<span class="dv">28</span>)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>summary(model, input_data<span class="op">=</span>input_img)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="236">
<pre><code>==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
SimpleCNN                                [1, 10]                   --
├─Sequential: 1-1                        [128, 3, 3]               --
│    └─Block: 2-1                        [32, 28, 28]              --
│    │    └─Sequential: 3-1              [32, 28, 28]              9,568
│    └─Block: 2-9                        --                        (recursive)
│    │    └─Sequential: 3-8              --                        (recursive)
│    └─Block: 2-3                        --                        (recursive)
│    │    └─Sequential: 3-3              --                        (recursive)
│    └─Block: 2-9                        --                        (recursive)
│    │    └─Sequential: 3-8              --                        (recursive)
│    └─MaxPool2d: 2-5                    [32, 14, 14]              --
│    └─Block: 2-6                        [64, 14, 14]              --
│    │    └─Sequential: 3-5              [64, 14, 14]              55,424
│    └─Block: 2-9                        --                        (recursive)
│    │    └─Sequential: 3-8              --                        (recursive)
│    └─Block: 2-8                        --                        (recursive)
│    │    └─Sequential: 3-7              --                        (recursive)
│    └─Block: 2-9                        --                        (recursive)
│    │    └─Sequential: 3-8              --                        (recursive)
│    └─MaxPool2d: 2-10                   [64, 7, 7]                --
│    └─Block: 2-11                       [128, 7, 7]               --
│    │    └─Sequential: 3-9              [128, 7, 7]               221,440
│    └─MaxPool2d: 2-12                   [128, 3, 3]               --
├─Sequential: 1-2                        [1, 10]                   --
│    └─Linear: 2-13                      [1, 2048]                 2,361,344
│    └─ReLU: 2-14                        [1, 2048]                 --
│    └─Linear: 2-15                      [1, 10]                   20,490
==========================================================================================
Total params: 2,714,442
Trainable params: 2,714,442
Non-trainable params: 0
Total mult-adds (M): 259.02
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.72
Params size (MB): 10.67
Estimated Total Size (MB): 11.40
==========================================================================================</code></pre>
</div>
</div>
<p>Note that the model has <span class="math inline">\(\approx\)</span> 2.7 million parameters and a total size of around 11 MB.</p>
<p>Model precision refers to the number of bits used to represent each weight or activation. Since our model weights are of dtype float32, the precision is 32 bits. Thus, the total size of a model can be calculated using the formula: total size <span class="math inline">\(\approx\)</span> (number of parameters * precision) / 8.</p>
<p>Also, note that the FC layers have most of the parameters (over 80%). In general conv layers are more memory heavy (input size is large there) while the FC layers have large number of parameters.</p>
</section>
</section>
</section>
<section id="lets-train-a-model" class="level2">
<h2 class="anchored" data-anchor-id="lets-train-a-model">Lets train a model</h2>
<p>Let us quickly now train a model to classify images from the fashion mnist dataset. The dataset contains very small grayscale (ie single channel) images of size (28*28).</p>
<blockquote class="blockquote">
<p>the below code is all generated by claude-sonnet-3.5, bcuz its kinda boring to train a toy model on a toy dataset, that too for classification. dont worry though, ill soon be back with a more interesting vision problem to get our hands dirty.</p>
</blockquote>
<p>Training itself is pretty straightforward. We split the data into train-test, and then train the model for a few epochs. We use the Adam optimizer and CrossEntropyLoss as the loss function. The outputs of the model are bare logits, which the loss functions accepts with targets. A learning rate of 0.001 is used for the optimizer. In the end we test on the test set, and print the accuracy.</p>
<div id="cell-46" class="cell" data-execution_count="240">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.transforms <span class="im">as</span> transforms</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Set device</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"mps"</span> <span class="cf">if</span> torch.backends.mps.is_built() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"device: </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Load and preprocess the Fashion MNIST dataset (just plain old standardization)</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> transforms.Compose([transforms.ToTensor(), transforms.Normalize((<span class="fl">0.5</span>,), (<span class="fl">0.5</span>,))])</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> torchvision.datasets.FashionMNIST(root<span class="op">=</span><span class="st">'./data'</span>, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>transform)</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>test_dataset <span class="op">=</span> torchvision.datasets.FashionMNIST(root<span class="op">=</span><span class="st">'./data'</span>, train<span class="op">=</span><span class="va">False</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>transform)</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span><span class="dv">64</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>test_loader <span class="op">=</span> DataLoader(test_dataset, batch_size<span class="op">=</span><span class="dv">64</span>, shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SimpleCNN()</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>model.to(device)</span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop</span></span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>all_losses <span class="op">=</span> []</span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a>    epoch_losses <span class="op">=</span> []</span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch_idx, (data, targets) <span class="kw">in</span> <span class="bu">enumerate</span>(train_loader):</span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a>        data, targets <span class="op">=</span> data.to(device), targets.to(device)</span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> model(data)</span>
<span id="cb22-38"><a href="#cb22-38" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(outputs, targets)</span>
<span id="cb22-39"><a href="#cb22-39" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb22-40"><a href="#cb22-40" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb22-41"><a href="#cb22-41" aria-hidden="true" tabindex="-1"></a>        epoch_losses.append(loss.item())</span>
<span id="cb22-42"><a href="#cb22-42" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-43"><a href="#cb22-43" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch [</span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>num_epochs<span class="sc">}</span><span class="ss">], Loss: </span><span class="sc">{</span>np<span class="sc">.</span>mean(epoch_losses)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb22-44"><a href="#cb22-44" aria-hidden="true" tabindex="-1"></a>    all_losses.extend(epoch_losses)</span>
<span id="cb22-45"><a href="#cb22-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-46"><a href="#cb22-46" aria-hidden="true" tabindex="-1"></a><span class="co"># plot train loss curve</span></span>
<span id="cb22-47"><a href="#cb22-47" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb22-48"><a href="#cb22-48" aria-hidden="true" tabindex="-1"></a>plt.plot(all_losses)</span>
<span id="cb22-49"><a href="#cb22-49" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epoch'</span>)</span>
<span id="cb22-50"><a href="#cb22-50" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Loss'</span>)</span>
<span id="cb22-51"><a href="#cb22-51" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Training Loss Curve'</span>)</span>
<span id="cb22-52"><a href="#cb22-52" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb22-53"><a href="#cb22-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-54"><a href="#cb22-54" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluation</span></span>
<span id="cb22-55"><a href="#cb22-55" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb22-56"><a href="#cb22-56" aria-hidden="true" tabindex="-1"></a>correct <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb22-57"><a href="#cb22-57" aria-hidden="true" tabindex="-1"></a>total <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb22-58"><a href="#cb22-58" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb22-59"><a href="#cb22-59" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> data, targets <span class="kw">in</span> test_loader:</span>
<span id="cb22-60"><a href="#cb22-60" aria-hidden="true" tabindex="-1"></a>        data, targets <span class="op">=</span> data.to(device), targets.to(device)</span>
<span id="cb22-61"><a href="#cb22-61" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> model(data)</span>
<span id="cb22-62"><a href="#cb22-62" aria-hidden="true" tabindex="-1"></a>        _, predicted <span class="op">=</span> torch.<span class="bu">max</span>(outputs.data, <span class="dv">1</span>)</span>
<span id="cb22-63"><a href="#cb22-63" aria-hidden="true" tabindex="-1"></a>        total <span class="op">+=</span> targets.size(<span class="dv">0</span>)</span>
<span id="cb22-64"><a href="#cb22-64" aria-hidden="true" tabindex="-1"></a>        correct <span class="op">+=</span> (predicted <span class="op">==</span> targets).<span class="bu">sum</span>().item()</span>
<span id="cb22-65"><a href="#cb22-65" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> <span class="dv">100</span> <span class="op">*</span> correct <span class="op">/</span> total</span>
<span id="cb22-66"><a href="#cb22-66" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Test Accuracy:</span><span class="sc">{</span>accuracy<span class="sc">:.2f}</span><span class="ss"> %'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>device: mps
Epoch [1/5], Loss: 0.4570
Epoch [2/5], Loss: 0.2674
Epoch [3/5], Loss: 0.2233
Epoch [4/5], Loss: 0.1939
Epoch [5/5], Loss: 0.1718</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="00_cnn_basics_files/figure-html/cell-14-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Test Accuracy:91.85 %</code></pre>
</div>
</div>
<p>Over 91% accuracy. Neat!! Now let’s visualize some of the predictions.</p>
<div id="cell-48" class="cell" data-execution_count="238">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>class_names <span class="op">=</span> [<span class="st">'T-shirt/top'</span>, <span class="st">'Trouser'</span>, <span class="st">'Pullover'</span>, <span class="st">'Dress'</span>, <span class="st">'Coat'</span>, </span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>               <span class="st">'Sandal'</span>, <span class="st">'Shirt'</span>, <span class="st">'Sneaker'</span>, <span class="st">'Bag'</span>, <span class="st">'Ankle boot'</span>]   <span class="co"># verify using train_dataset.classes</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">5</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">6</span>))</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">'Sample Predictions'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, idx <span class="kw">in</span> <span class="bu">enumerate</span>(random.sample(<span class="bu">range</span>(<span class="bu">len</span>(test_dataset)), <span class="dv">10</span>)):</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>        image, label <span class="op">=</span> test_dataset[idx]</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> model(image.unsqueeze(<span class="dv">0</span>).to(device))</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>        predicted <span class="op">=</span> output.argmax(<span class="dv">1</span>).item()</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>        ax <span class="op">=</span> axes[i <span class="op">//</span> <span class="dv">5</span>, i <span class="op">%</span> <span class="dv">5</span>]</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>        ax.imshow(image.squeeze(), cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>        ax.axis(<span class="st">'off'</span>)</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>        ax.set_title(<span class="ss">f'Pred: </span><span class="sc">{</span>class_names[predicted]<span class="sc">}</span><span class="ch">\n</span><span class="ss">True: </span><span class="sc">{</span>class_names[label]<span class="sc">}</span><span class="ss">'</span>, </span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>                     color<span class="op">=</span><span class="st">'green'</span> <span class="cf">if</span> predicted <span class="op">==</span> label <span class="cf">else</span> <span class="st">'red'</span>)</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="00_cnn_basics_files/figure-html/cell-15-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Now let us visualize the confisuion matrix on the test set to analyse errors.</p>
<div id="cell-50" class="cell" data-execution_count="239">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># see the confusion matrix</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Collect predictions and true labels</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>y_true, y_pred <span class="op">=</span> [], []</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> data, targets <span class="kw">in</span> test_loader:</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>        data, targets <span class="op">=</span> data.to(device), targets.to(device)</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> model(data)</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>        _, predicted <span class="op">=</span> torch.<span class="bu">max</span>(outputs.data, <span class="dv">1</span>)</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>        y_true.extend(targets.cpu().numpy())</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>        y_pred.extend(predicted.cpu().numpy())</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Create and plot confusion matrix</span></span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>cm <span class="op">=</span> confusion_matrix(y_true, y_pred)</span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>plt.imshow(cm, interpolation<span class="op">=</span><span class="st">'nearest'</span>, cmap<span class="op">=</span><span class="st">"Pastel1"</span>)</span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Confusion Matrix'</span>)</span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a>tick_marks <span class="op">=</span> <span class="bu">range</span>(<span class="bu">len</span>(class_names))</span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a>plt.xticks(tick_marks, class_names, rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a>plt.yticks(tick_marks, class_names)</span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Predicted'</span>)</span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'True'</span>)</span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-28"><a href="#cb26-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Add text annotations</span></span>
<span id="cb26-29"><a href="#cb26-29" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(cm.shape[<span class="dv">0</span>]):</span>
<span id="cb26-30"><a href="#cb26-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(cm.shape[<span class="dv">1</span>]):</span>
<span id="cb26-31"><a href="#cb26-31" aria-hidden="true" tabindex="-1"></a>        plt.text(j, i, <span class="bu">str</span>(cm[i, j]), ha<span class="op">=</span><span class="st">"center"</span>, va<span class="op">=</span><span class="st">"center"</span>)</span>
<span id="cb26-32"><a href="#cb26-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-33"><a href="#cb26-33" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb26-34"><a href="#cb26-34" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="00_cnn_basics_files/figure-html/cell-16-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We can see that the model is particularly confused in the “Tshirt” &amp; “Shirt” class (less than 70% accuracy) and most confident in “Trouser”, “Sneaker”, “Bag” and “Ankle boot” (greater than 95% accuracy)</p>
</section>
<section id="concluding" class="level2">
<h2 class="anchored" data-anchor-id="concluding">Concluding</h2>
<p>I hope you clearly understand the core CNN fundamental architecture. This is just the base that’ll act as a foundation for more interesting architectures as we go on. Until then, see you in the next one!</p>
<p>Byeee :)</p>
</section>
<section id="further-reading" class="level2">
<h2 class="anchored" data-anchor-id="further-reading">Further Reading</h2>
<ul>
<li>Sebastian Raschka Intorduction to CNNs course <a href="https://sebastianraschka.com/blog/2021/dl-course.html#l13-introduction-to-convolutional-neural-networks">here</a></li>
<li>CS231N Cnn notes <a href="https://cs231n.github.io/convolutional-networks/#case">here</a>. Great intuition and more detail about the shapes, local connectivity, spatial arrangement, and loads of other stuff.</li>
<li>A beginner friendly article on Medium (its a great blog-series for ML) <a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721">here</a></li>
<li>Visualizing and Understanding Convolutional Networks. 2014 paper <a href="https://arxiv.org/pdf/1311.2901">here</a></li>
<li>Find out some of the modifications made on this basic architecture <a href="https://towardsdatascience.com/10-papers-you-should-read-to-understand-image-classification-in-the-deep-learning-era-4b9d792f45a7">here</a></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/mindadeepam\.github\.io");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>