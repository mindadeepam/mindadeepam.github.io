[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJul 20, 2024\n\n\nLanguage Modelling with RNNs\n\n\nDeepam Minda\n\n\n\n\nSep 12, 2023\n\n\nFew Shot learning: Classify using few examples!\n\n\nDeepam Minda\n\n\n\n\nJul 30, 2023\n\n\nSo you know convolutions huh?\n\n\nDeepam Minda\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deepam Minda",
    "section": "",
    "text": "Hi Y‚Äôall. My name is Deepam Minda and this is my website. I‚Äôll be writing a lot more blogs, TILs, etc about various things that interest me. I‚Äôm interested in topics like AI, llms, and tech in general. I hope you learn something new here. Feel free to share your thoughts or hit me up.\nCheers!"
  },
  {
    "objectID": "posts/few-shot-learning/prototype_networks.html",
    "href": "posts/few-shot-learning/prototype_networks.html",
    "title": "Few Shot learning: Classify using few examples!",
    "section": "",
    "text": "If you aren‚Äôt already familiar with it, few-shot learning (FSL) is an exciting concept in machine learning that focuses on training models to make accurate predictions or classifications when provided with only a very limited amount of labeled data for each category or class. Unlike traditional machine learning approaches that often require substantial labeled datasets for training, few-shot learning aims to address scenarios where acquiring such extensive labeled data is impractical or costly.\nIn few-shot learning, the model is trained to generalize effectively from a small number of examples (or shots) per category, allowing it to make predictions for new, unseen data points with confidence. This capability is particularly valuable in situations where data is scarce, and manual annotation is labor-intensive or expensive. Refer to [1] for an in-depth survey in this field.\nIn this blog, I‚Äôm going to show you how to implement a basic few-shot classification technique for text."
  },
  {
    "objectID": "posts/few-shot-learning/prototype_networks.html#introduction",
    "href": "posts/few-shot-learning/prototype_networks.html#introduction",
    "title": "Few Shot learning: Classify using few examples!",
    "section": "",
    "text": "If you aren‚Äôt already familiar with it, few-shot learning (FSL) is an exciting concept in machine learning that focuses on training models to make accurate predictions or classifications when provided with only a very limited amount of labeled data for each category or class. Unlike traditional machine learning approaches that often require substantial labeled datasets for training, few-shot learning aims to address scenarios where acquiring such extensive labeled data is impractical or costly.\nIn few-shot learning, the model is trained to generalize effectively from a small number of examples (or shots) per category, allowing it to make predictions for new, unseen data points with confidence. This capability is particularly valuable in situations where data is scarce, and manual annotation is labor-intensive or expensive. Refer to [1] for an in-depth survey in this field.\nIn this blog, I‚Äôm going to show you how to implement a basic few-shot classification technique for text."
  },
  {
    "objectID": "posts/few-shot-learning/prototype_networks.html#terminology",
    "href": "posts/few-shot-learning/prototype_networks.html#terminology",
    "title": "Few Shot learning: Classify using few examples!",
    "section": "Terminology",
    "text": "Terminology\nBefore we begin, let us familiarize ourselves with the correct terminology.\nWhat characterizes FSL is having only a few examples at hand, for unseen classes, during inference. So basically we are showing the model only a few examples of a class which it may or may not have encountered during its pre-training before we make predictions using that model.\nSupport Set, ùíÆ: The few annotated examples that we have, make up the support set, with which we may or may not update the model weights to make it generalize to the new classes.\nQuery Set, ùí¨: The query set consists of our test set, i.e.¬†the samples we want to classify using the base model and a support set.\nN-way K-shot learning scheme: This is a common phrase used in the FSL literature, which essentially describes the few-shot problem statement that a model will be dealing with. ‚ÄúN‚Äù is the number of classes we have at test time and ‚ÄúK‚Äù is the number of samples per class we have in our support set ‚ÄúùíÆ‚Äù\n1-shot classification: When K=1, i.e.¬†we have only one labeled sample available per class.\n0-shot classification: K=0, i.e.¬†we do not have any labeled samples available during inference.\nLet us have a look at an example.\n\n\nCode\n# sample set is 3-way, 3-shot.\nclasses = ['camera', 'battery', 'display']\n\nsample_set = {\n    'camera': [\n        'absolutely love this quality of my photos!!',\n        'it even gives great quality in dim lighting. fabulous!!',\n        'the camera should be much better for such a high price'\n    ],\n    'battery': [\n        \"The battery life on this device is exceptional! It easily lasts me the entire day with heavy usage.\",\n        \"I'm a bit disappointed with the battery performance. It drains quite quickly, especially when using power-hungry apps.\",\n        \"The battery is decent, not too bad, not too good. It gets me through the day, but I was hoping for better longevity.\"\n    ],\n    'display': [\n        \"The display on this device is stunning! Colors are vivid, and the resolution is top-notch.\",\n        \"I'm not too impressed with the display quality. It seems a bit washed out, and the brightness could be better.\",\n        \"The display is okay, but nothing extraordinary. It gets the job done for everyday tasks.\"\n    ]\n}\n\nquery_set = [\"i hate the batteries\", \"does it give good quality photos in the night?\"]\n\n\nHere we have a 3-way (there are 3 classes), 3-shot (3 examples for each class) setting."
  },
  {
    "objectID": "posts/few-shot-learning/prototype_networks.html#high-level-design",
    "href": "posts/few-shot-learning/prototype_networks.html#high-level-design",
    "title": "Few Shot learning: Classify using few examples!",
    "section": "High level design",
    "text": "High level design\nLet us have a quick look at the architecture of the system.\n\n\n\nA simple few shot classification system\n\n\nThis is the flow of our solution:\nThe first step is to get an embedding module. That can be created using regular supervised learning (Resnets trained on Imagenet) or self-supervised learning (BERT and co). Then, we use the embedding module to get feature representations for our classes in the support set. A simple way to do this is to turn each class‚Äôs examples into embeddings and take the mean of those vectors. This then becomes our ‚Äúprototype‚Äù vectors to compare against. Now for each query, we can take the embeddings of the query text and use cosine similarity to find the predicted class. This closely resembles This system basically allows us to leverage transfer learning to use large backbones as our embedding module. And there is also the advantage of not performing any gradient updates. This helps us maintain a much more dynamic and flexible system.\nThe idea of comparing query samples with the support set samples is inspired by metric learning. Refer to [3, 4] for better understanding.\nLet‚Äôs implement this using the transformers library. You can find the implementation in this colab notebook."
  },
  {
    "objectID": "posts/few-shot-learning/prototype_networks.html#implementation",
    "href": "posts/few-shot-learning/prototype_networks.html#implementation",
    "title": "Few Shot learning: Classify using few examples!",
    "section": "Implementation",
    "text": "Implementation\nLet‚Äôs start with the good old BERT base model.\n\n1. Import libraries and download model\n\n\nCode\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport numpy as np\nfrom typing import Dict\nfrom pprint import pprint\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\nif torch.backends.mps.is_available():\n    device = torch.device('mps')\nelse:\n    device = torch.device('cpu')\n    \n# load the model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\nmodel = AutoModel.from_pretrained(\"bert-base-uncased\")\n\n\n\n\n2. Tokenize and encode a sentence\n\n\nCode\ntext = \"He's such a great guy!!\"\nencoded_input = tokenizer(\n  text, \n  return_tensors='pt', \n  padding='max_length',     # True will pad to max-len in batch\n  max_length=32\n)\nprint(f\"encoded input:\")\npprint(encoded_input)\n\n\nencoded input:\n{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]]),\n 'input_ids': tensor([[ 101, 2002, 1005, 1055, 2107, 1037, 2307, 3124,  999,  999,  102,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0]]),\n 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]])}\n\n\nwhere,\n\ninput_ids: token id of each token\ntoken_type_id: When we pass two sentences for downstream fine-tuning in BERT, this is used to identify which token belongs to which sentence.\nattention_mask: which tokens to ignore. As you‚Äôll see, the padding tokens have been masked.\n\n\n\n3. Generate embeddings using model\nThe output has 2 parts, cls_token_embeddings and last_hidden_states of the tokens. We can either use the cls_embeddings to represent the sentence or pool the vectors in last_hidden_states. The pooling can be max/min/mean.\nThe dimension of the output will be equal to the embedding dimension of the model, i.e.¬†784 in our case.\n\n\nCode\ndef get_embeddings(model, tokenizer, text, pooling='mean'):\n  \n  encoded_input = tokenizer(\n    text, \n    return_tensors='pt', \n    padding='max_length', \n    max_length=16, \n    truncation=True\n  )\n  encoded_input = encoded_input.to(device)\n\n  model.to(device)\n\n  model.eval()\n  with torch.no_grad():\n    output = model(**encoded_input)\n    last_hidden_state, pooler_output = output[0], output[1]\n    \n    if pooling=='cls':\n      embedding = pooler_output\n    else:\n      # ignore the pad tokens embeddings by multiplying with attention mask\n      last_hidden_state = (last_hidden_state * encoded_input['attention_mask'].unsqueeze(2))\n      embedding = last_hidden_state.mean(dim=-2)\n  return np.array(embedding.cpu())\n\n\nembeddings = get_embeddings(model, tokenizer, 'hey there! how are you?')\nprint(f\"shape of embeddings: {embeddings.shape}\")\n\n\nshape of embeddings: (1, 768)\n\n\n\n\n4. Prepare the prototypes:\nTo prepare the class prototypes we‚Äôll take the mean of the sentences for each class.\n\n\nCode\ndef make_prototypes(model, tokenizer, sample_set: Dict):\n  prototype_vectors = dict()\n  sentence_embeddings = dict()\n  for category, sentences in sample_set.items():\n    sentence_embeds = get_embeddings(model, tokenizer, sentences)\n    sentence_embeddings[category] = sentence_embeds\n    prototype_vectors[category] = np.mean(sentence_embeddings[category], axis=0)\n  return prototype_vectors\n\n\n\n\n5. Classify\nTo classify a query text, we can run cosine similarity against the prototype vectors and return the argmax as the most probable class!\n\n\nCode\ndef classify(model, tokenizer, text, prototype_vectors=None, sample_set=None):\n  if prototype_vectors==None:\n      assert sample_set!=None, \"prototype vectors are not passed, either pass a sample set prototype vectors\"\n      prototype_vectors = make_prototypes(sample_set)\n\n  query_embeddings = get_embeddings(model, tokenizer, text)\n  \n  prototype_matrix = np.stack(list(prototype_vectors.values()))\n  scores = sentence_transformers.util.cos_sim(query_embeddings, prototype_matrix)\n  return scores\n\n\nUsing the above-defined functions and the sample set from before, we have:\n\n\nCode\nprototype_vectors = make_prototypes(model, tokenizer, sample_set)\nquery_text = \"i hate the batteries\"\noutput = classify(model, tokenizer, query_text, prototype_vectors=prototype_vectors)\n\nprint(f\"output: {output}\")\nprint(f\"the predicted class is {classes[output.argmax().item()]}\")\n\n\noutput: tensor([[0.6121, 0.7127, 0.6388]])\nthe predicted class is battery\n\n\nA bit strange! Although the expected class is predicted, scores for other classes are also high. Let‚Äôs try a harder query.\n\n\nCode\nquery = ['does it give good quality photos in the night?']\noutput = classify(model, tokenizer, query, prototype_vectors=prototype_vectors)\n\nprint(f\"output: {output}\")\nprint(f\"the predicted class is {classes[output.argmax().item()]}\")\n\n\noutput: tensor([[0.7984, 0.7043, 0.7647]])\nthe predicted class is camera\n\n\nAlthough the highest similarity is for ‚Äòcamera‚Äô, the similarity should be much higher.\nThe results do not get better even if we try cls-pooling. This only means that the embeddings produced by the model do not give us an accurate representation of the sentence.\nWe would then do good to remember that BERT pre-train was trained by MaskedLM, NextSentencePrediction, hence the original purpose of BERT is not to create a meaningful embedding of the sentence but for some specific downstream task. In fact, as the authors of the sentence-transformer paper [2] point out, out-of-the-box Bert embeddings perform even worse than GLoVE representations!\n\nJacob Devlin‚Äôs comment: I‚Äôm not sure what these vectors are, since BERT does not generate meaningful sentence vectors. It seems that this is is doing average pooling over the word tokens to get a sentence vector, but we never suggested that this will generate meaningful sentence representations. And even if they are decent representations when fed into a DNN trained for a downstream task, it doesn‚Äôt mean that they will be meaningful in terms of cosine distance. (Since cosine distance is a linear space where all dimensions are weighted equally).\n\nThere are a few ways to improve the bert-base for sentence-level tasks and both involve finetuning the model with some data.\n\nadding a linear layer on top and fine-tuning it.\nmaking embeddings better by contrastive learning."
  },
  {
    "objectID": "posts/few-shot-learning/prototype_networks.html#using-sentence-transformers",
    "href": "posts/few-shot-learning/prototype_networks.html#using-sentence-transformers",
    "title": "Few Shot learning: Classify using few examples!",
    "section": "Using sentence transformers",
    "text": "Using sentence transformers\nUltimately, what we need is a better embedding module. Luckily we have such models. As it turns out, contrastive learning is an excellent approach for tuning our models such that different sentences produce semantically different embeddings.\nWe will explore contrastive learning and its inner workings some other day, but for now, let‚Äôs pick up open-source models that have been finetuned using contrastive learning. There is an entire library (aka sentence-transformers) and paper[2] dedicated to this.\nWe‚Äôll use the sentence-transformers/stsb-bert-base model for our purposes.\n\n1. Import packages and download model\n\n\nCode\nfrom sentence_transformers import SentenceTransformer\n\n# load a sentence transformer model\nsts_model = SentenceTransformer('sentence-transformers/stsb-bert-base')\nmodel2 = sts_model[0].auto_model.to(device)\ntokenizer2 = sts_model[0].tokenizer\n\n\n\n\n2. Use the above-defined functions to prepare prototype vectors and classify them in a few-shot setting\n\n\nCode\nprototype_vectors = make_prototypes(model2, tokenizer2, sample_set)\nquery_text = \"i hate the batteries\"\noutput = classify(model2, tokenizer2, query_text, prototype_vectors)\n\nprint(f\"output: {output}\")\nprint(f\"the predicted class is {classes[output.argmax().item()]}\")\n\n\noutput: tensor([[0.0910, 0.4780, 0.1606]])\nthe predicted class is battery\n\n\n\n\nCode\n\nquery = ['does it give good quality photos in the night?']\noutput = classify(model2, tokenizer2, query, prototype_vectors=prototype_vectors)\n\nprint(f\"output: {output}\")\nprint(f\"the predicted class is {classes[output.argmax().item()]}\")\n\n\noutput: tensor([[0.4467, 0.1012, 0.2998]])\nthe predicted class is camera\n\n\nAs we can see, the scores seem much more reasonable this time around. There is a much better correlation with the ground truth labels. Using better base models trained in multiple tasks further improves the performance of these models."
  },
  {
    "objectID": "posts/few-shot-learning/prototype_networks.html#conclusion",
    "href": "posts/few-shot-learning/prototype_networks.html#conclusion",
    "title": "Few Shot learning: Classify using few examples!",
    "section": "Conclusion",
    "text": "Conclusion\nThis brings us to the end of this blog. In summary, we explored the realm of few-shot learning, a machine-learning approach tailored for accurate predictions with limited labeled data. Initially, we employed BERT, but its design didn‚Äôt align with our objectives. Instead, we leveraged a model fine-tuned for sentence-level tasks, sentence-transformers/stsb-bert-base, which significantly improved our results.\nThese are a few things to note:\nAlthough we directly used pre-trained models here, an interesting undertaking would be to perform the contrastive fine-tuning ourselves. Also, instead of using cosine similarity, we can train lightweight classifiers on top of our embedding module for better performance.\nThat‚Äôll be all from my side. Until next time, Happy Reading!"
  },
  {
    "objectID": "posts/few-shot-learning/prototype_networks.html#references",
    "href": "posts/few-shot-learning/prototype_networks.html#references",
    "title": "Few Shot learning: Classify using few examples!",
    "section": "References",
    "text": "References\n[1] Survey paper on few-shot learning\n[2] Sentence-Bert paper\n[3] Prototypical Networks\n[4] Excellent much more techincal blog by Lilian Weng"
  },
  {
    "objectID": "posts/cnns/00_cnn_basics.html",
    "href": "posts/cnns/00_cnn_basics.html",
    "title": "So you know convolutions huh?",
    "section": "",
    "text": "Welcome to the land of vision in deep learning. Gone are the days you need to look at 10 types of thresholding and 20 types of data pre-processing and apply a logistic classifier on the outputs of a cnn feature extractor. These days you can just load a pretrained model and watch a decaying training and validation loss and feel good about yourself.\nTurns out, this doesnt last long though. You eventually get around to understand what these models actually are because you need to! And when it comes to vision, you surely cannot leave out CNNs. Sure its all transformers nowadays but CNNs or convolutional neural networks were essentially the first deep learning models to make a significant impact in the field of computer vision and most would agree started the whole deep learning wave in the modern era. AlexNet, VGG, resnets would be considered the pioneer models in this field.\nNow you might ask: All this sounds interesting Deepam, but what the hell is a convolution?"
  },
  {
    "objectID": "posts/cnns/00_cnn_basics.html#convolutions",
    "href": "posts/cnns/00_cnn_basics.html#convolutions",
    "title": "So you know convolutions huh?",
    "section": "Convolutions",
    "text": "Convolutions\nConvolutions is just an operation between 2 signals (vectors/matrices/continuous-signals) that returns a 3rd signal. It represents how one signal modifies the other signal. In general terms, convolution is used to apply a filter to a signal or data."
  },
  {
    "objectID": "posts/cnns/00_cnn_basics.html#steps-in-convolution",
    "href": "posts/cnns/00_cnn_basics.html#steps-in-convolution",
    "title": "So you know convolutions huh?",
    "section": "Steps in Convolution",
    "text": "Steps in Convolution\n\nFlip the Kernel: The kernel (or filter) (g) is flipped both horizontally and vertically. why?-&gt; link\nSlide the Kernel: The flipped kernel is then slid (convolved) over the input function (f) .\nElement-wise Multiplication: At each position, element-wise multiplications are performed between the kernel and the overlapping portion of the input function.\nSummation: The results of these multiplications are summed to get a single value, which is placed in the output function.\n\nFor a 1d array this might look like this:\n\n\nCode\nimport numpy as np \n\nf = np.array([1,2,3,-1, 10, -4])\ng = np.array([1,0,-1])\n\nprint(f\"f: {f}\")\nprint(f\"g: {g}\")\nwindow_size = g.shape[0]\nresult = []\nprint(\"\\nslice_of_f * g[::-1]: \")\nfor idx, i in enumerate(range(len(f)-window_size+1)):\n    slice_of_f = f[i:i+window_size]\n    print(f\"{idx}. sum({slice_of_f} * {g[::-1]}) = sum({slice_of_f * g[::-1]}) = {np.sum(slice_of_f * g[::-1])}\")\n    result.append(np.sum(slice_of_f * g[::-1]))\n\nprint(f\"\\nresult python: {result}\")\nprint(f\"result using np.convolve: {np.convolve(f,g, mode='valid')}\") # valid is the no padding\n\n\nf: [ 1  2  3 -1 10 -4]\ng: [ 1  0 -1]\n\nslice_of_f * g[::-1]: \n0. sum([1 2 3] * [-1  0  1]) = sum([-1  0  3]) = 2\n1. sum([ 2  3 -1] * [-1  0  1]) = sum([-2  0 -1]) = -3\n2. sum([ 3 -1 10] * [-1  0  1]) = sum([-3  0 10]) = 7\n3. sum([-1 10 -4] * [-1  0  1]) = sum([ 1  0 -4]) = -3\n\nresult python: [2, -3, 7, -3]\nresult using np.convolve: [ 2 -3  7 -3]\n\n\nThat wasnt so hard was it? The concept seems simple enough. slide one signal over another and each time, do element wise multiplication and sum to get a value.\nLets look at an example for 2d arrays. Here lets take 2 arrays of shapes (6,6) and (3,3). First we need to flip the 2nd array both horizontally and vertically. Then, as the (3,3) array slides over the other we should get a (4,4) matrix. Lets see how this works.\n\n\nCode\nfrom scipy.signal import convolve2d\n\nf = np.random.rand(6,6)\ng = np.array([[1,0,-1],[1,0,-1],[1,0,-1]])\n\nprint(f\"f shape: {f.shape}\")\nprint(f\"g shape: {g.shape}\")\n# print(f\"result: {np.convolve(f,g, mode='valid')}\")\n\n\nf shape: (6, 6)\ng shape: (3, 3)\n\n\n\n\nCode\nf_rows, f_columns = f.shape \ng_rows, g_columns = g.shape\n\nresult = np.zeros((f_rows-g_rows+1, f_columns-g_columns+1))\nfor i in range(f_rows-g_rows+1):\n    for j in range(f_columns-g_columns+1):\n        item = f[i:i+g_rows, j:j+g_columns] * g[::-1, ::-1]\n        if i&lt;3 and j==0:\n            print(f\"result[{i},{j}] \\n-&gt;sum(f[{i}:{i+g_rows}, {j}:{j+g_columns}] * g[::-1, ::-1]): \\n{item}\")\n            print(f\"-&gt; {np.sum(item)}\\n\")\n        result[i,j] = np.sum(item)\nprint(\"... and so on.\\n\")\nprint(f\"final result: \\n{result}\\n\")\n# print(f\"result shape: {result.shape}\")\n\nresult = convolve2d(f,g, mode='valid')\nprint(f\"Which is the same as the result of scipy.convolve2d: \\n{result}\")\n\n\nresult[0,0] \n-&gt;sum(f[0:3, 0:3] * g[::-1, ::-1]): \n[[-0.50043879  0.          0.96533241]\n [-0.57575706  0.          0.96608904]\n [-0.26910721  0.          0.02162452]]\n-&gt; 0.6077429029172687\n\nresult[1,0] \n-&gt;sum(f[1:4, 0:3] * g[::-1, ::-1]): \n[[-0.57575706  0.          0.96608904]\n [-0.26910721  0.          0.02162452]\n [-0.16668093  0.          0.3390268 ]]\n-&gt; 0.3151951622196356\n\nresult[2,0] \n-&gt;sum(f[2:5, 0:3] * g[::-1, ::-1]): \n[[-0.26910721  0.          0.02162452]\n [-0.16668093  0.          0.3390268 ]\n [-0.61184461  0.          0.5508951 ]]\n-&gt; -0.1360863354721753\n\n... and so on.\n\nfinal result: \n[[ 0.6077429  -0.14661592  0.29036408  0.19567047]\n [ 0.31519516 -0.07866619  0.52920251 -0.44948681]\n [-0.13608634 -0.90171287  0.94176527 -0.91012219]\n [ 0.69872346 -0.43798077 -0.38699654 -0.62373429]]\n\nWhich is the same as the result of scipy.convolve2d: \n[[ 0.6077429  -0.14661592  0.29036408  0.19567047]\n [ 0.31519516 -0.07866619  0.52920251 -0.44948681]\n [-0.13608634 -0.90171287  0.94176527 -0.91012219]\n [ 0.69872346 -0.43798077 -0.38699654 -0.62373429]]"
  },
  {
    "objectID": "posts/cnns/00_cnn_basics.html#convolutional-neural-networks-cnnsconvnets",
    "href": "posts/cnns/00_cnn_basics.html#convolutional-neural-networks-cnnsconvnets",
    "title": "So you know convolutions huh?",
    "section": "Convolutional neural networks (CNNs/Convnets)",
    "text": "Convolutional neural networks (CNNs/Convnets)\nWith that out of the way lets back up for a second and remember our old friends feed forward networks. What is the problem with using feed forward networks to process images? As it turns out, quite a few things:\n\nFeed forward networks do not share information between different data points in the network. For example if X = [0,1,2,3] is a vector of input to the network, all the interactions between weights happen independently for each data point in one sample of data. This works fine for tabular features of some data, because they actually are not dependent on their spatial postion wrt each other. The model will learn the same if you shuffle all columns of a dataset. But images pixels are a different type of data. They are inherently very locally dependent. If you look at a single pixel, it is very likely to be highly correlated with its neighboring pixels. Hence our network should process atleast patches of images at a time.\nEarlier when classical image processing methods were being used, many a times hand-made ‚Äúfilters‚Äù were used to extract features from images. In CNNs, we learn these filters! and we learn a lot of them. So think of each layer in this CNN as having a lot of these filters which are learned during network.\nAnother concept that helps is parameter sharing. We could in thoery have different set of filters for different patches in the image, but this would shoot up the parameter count of our model. So in a single layer, we use a single set of filters for all patches in the image.\n\nLets get into the tensor shapes now."
  },
  {
    "objectID": "posts/cnns/00_cnn_basics.html#shapes",
    "href": "posts/cnns/00_cnn_basics.html#shapes",
    "title": "So you know convolutions huh?",
    "section": "Shapes",
    "text": "Shapes\nImages are mostly represented as 3d tensors, of shape: (H_{in}, W_{in}, C_{in}) where H_{in} is the height of the image, W_{in} is the width of the image and C_{in} is the number of channels in the image. It helps to visualize volumes (3d) of numbers interacting with each other and resulting in volumes of data.\n\ninput to Conv layer -&gt; (H_{in}, W_{in}, C_{in})\nfilters/weights of Conv layer -&gt; C_{out} * (k,k,C_{in}) (ignoring biases for simplicity)\noutput of Conv layer -&gt; (H_{out}, W_{out}, C_{out})\n\nC_{out} number of filters are learned. Each one is a tensor of shapes (k,k,C_{in}) where k is the size of the filter (assuming square for simplicity too) and C_{in} is the number of input channels. C_{out} is the number of filters learned.\nSo when C_{out} filters of shape (k,k,C_{in}) are applied to an image of shape (H_{in},W_{in},C_{in}), the output is a 3d tensor of shape (H_{out}, W_{out}, C_{out}) where H_{out} is the height of the output and W_{out} is the width of the output.\nThat is basically the essence of a convolutional layer. There are a few other parameters to consider which together decide the values for H_{out} and W_{out}.\n\nS: The stride. When we slide the filter over the input, we move it S steps at a time. If stride is 2, we move the filter 2 pixels at a time. The output shape will be inversely proportional to the stride size.\nP: The padding. if we want to keep the height and width of the image, we can pad the input with zeros. see example here:\n\nf: [1,  2,  3, -1, 10, -4]\ng: [1, 0, -1]\nconvolve(f,g) = [2,-3,7,-3]\nBut after padding f with 2 zeros on either side:\nf: [0, 0, 1,  2,  3, -1, 10, -4, 0, 0]\nconvolve(f,g) = [1, 2, 2, -3, 7, -3, -10, 4]\nI would at this point ask you to take me on my word when i say that the output dimensions turn out to be:\n\nH_{out}=(H_{in}‚àík+2P)/S+1\nW_{out}=(W_{in}‚àík+2P)/S+1\n\nAltough this is not true everytime, it is useful as a rule of thumb. the only times this wont be true are if we have different S, k or P along the height and width dimensions.\nTo visualize the interaction of filters and input images, you can go here."
  },
  {
    "objectID": "posts/cnns/00_cnn_basics.html#other-layers",
    "href": "posts/cnns/00_cnn_basics.html#other-layers",
    "title": "So you know convolutions huh?",
    "section": "Other layers",
    "text": "Other layers\nCNN layers are usually follwed by activation functions like ReLU and then a pooling layer.\nPooling layer? yeah, pooling layer. So given a volume of input say instead of using filters we just use operations non parameterized like min and max just to downsample the data and get a smaller volume. That is what pooling does. But why would we do something like that? 2 reasons:\n\nReduce the number of parameters: By reducing the number of parameters, the model can learn more general features.\nReduce the amount of overfitting: By reducing the amount of overfitting, the model can learn more robust features."
  },
  {
    "objectID": "posts/cnns/00_cnn_basics.html#some-jargon",
    "href": "posts/cnns/00_cnn_basics.html#some-jargon",
    "title": "So you know convolutions huh?",
    "section": "Some jargon",
    "text": "Some jargon\n\nfilters: also called kernels. they are learnable parameters in a convolutional layer. Each filter is a small matrix (usually 3x3 or 5x5) that slides over the input image to detect specific features. The values in these filters are updated during training to learn important features for the task at hand.\nactivation maps: the output when a [Conv-&gt;Relu] filter interacts with entire image.\nfeature maps: the output of a convolutional layer. (before the activation)\ndepth: refers to the number of channels.\nreceptive field: the area of the input that a given filter is able to see at a time is called the receptive field."
  },
  {
    "objectID": "posts/cnns/00_cnn_basics.html#a-simple-cnn-architecture",
    "href": "posts/cnns/00_cnn_basics.html#a-simple-cnn-architecture",
    "title": "So you know convolutions huh?",
    "section": "A Simple CNN architecture",
    "text": "A Simple CNN architecture\nLets assume a simple task of image classification. The most common form of a CNN architecture stacks a few CONV-RELU layers, follows them with POOL layers, and repeats this pattern until the image volume has reduced to a small size. At some point, it is common to transition to fully-connected layers. The last fully-connected layer holds the output, such as the class scores. In other words, the most common ConvNet architectures follow the pattern:\nINPUT -&gt; [[CONV -&gt; RELU]*N -&gt; POOL?]*M -&gt; [FC -&gt; RELU]*K -&gt; FC\nhere FC: feed-forward layer.\nNote that since architectures like Resnet and Inception emerged, this is not the case, and the CNNs feature more intricate and different connectivity structures."
  },
  {
    "objectID": "posts/cnns/00_cnn_basics.html#cnn-characteristics",
    "href": "posts/cnns/00_cnn_basics.html#cnn-characteristics",
    "title": "So you know convolutions huh?",
    "section": "CNN characteristics",
    "text": "CNN characteristics\nWhile we are here, let us also take note of some characteristics of CNNs:\n\nSparse connectivity: CNNs focus on local patterns in data, particularly useful for spatial data like images. A single patch in feature map is connected to only a small patch of image (in MLPs there is dense/full connection).\nParameter sharing: the same kernel/filter slides across the image. ie different neurons in each activation map is calculated using the same filter. In MLPs each neuron in the output space is calculated using different weight values. this makes it efficient for computation.\nSpatial hierarchy: CNNs build a hierarchy of increasingly abstract features. Lower layers detect simple features (e.g., edges), while deeper layers combine these to detect more complex patterns.\nTranslation invariance: CNNs can recognize patterns regardless of their position in the input. This is because we are using filters that slide over patches of data, so information is processed in the same way for different patches of data This is crucial for tasks like object recognition in images.\n\n\n\n\n\n\n\nFigure¬†1: visualizing activation maps in cnns. From the paper ‚ÄòVisualizing and Understanding Convolutional Networks‚Äô\n\n\n\nHave a look at this wonderful paper that dives deep into visualizing and understanding Cnns."
  },
  {
    "objectID": "posts/cnns/00_cnn_basics.html#lets-train-a-model-yaar",
    "href": "posts/cnns/00_cnn_basics.html#lets-train-a-model-yaar",
    "title": "So you know convolutions huh?",
    "section": "Lets train a model yaar",
    "text": "Lets train a model yaar\nI‚Äôd be remiss if I let you finish here thinking that you got CNNs down. Just for old times sake, lets train a model to classify images from the fashion mnist dataset. The dataset contains very small grayscale (ie single channel) images of size (28*28).\n\nthe below code is all generated by claude-sonnet-3.5, bcuz its kinda boring to train a toy model on a toy dataset, that too for classification. dont worry though, ill soon be back with a more interesting vision problem to get our hands dirty.\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nfrom torchsummary import summary\nimport numpy as np\n\n# Define the CNN architecture\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)     # input channels in layer1 is equal to number of input channels in the input image\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv3 = nn.Conv2d(64, 32, kernel_size=3, padding=1)\n        self.fc1 = nn.Linear(32 * 3 * 3, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.pool(torch.relu(self.conv1(x)))\n        x = self.pool(torch.relu(self.conv2(x)))\n        x = self.pool(torch.relu(self.conv3(x)))\n        x = x.view(-1, 32 * 3 * 3)     # flattening the tensor to feed it to FC layer\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_built() else \"cpu\")\nprint(f\"device: {device}\")\n\n# Load and preprocess the Fashion MNIST dataset (just plain old standardization)\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n\ntrain_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\ntest_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n# Initialize the model, loss function, and optimizer\nmodel = SimpleCNN()\n\n# Visualize model summary\nsummary(model, (1, 28, 28), device=\"cpu\")\n\nmodel.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nnum_epochs = 5\nall_losses = []\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_losses = []\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        data, targets = data.to(device), targets.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(data)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        epoch_losses.append(loss.item())\n    \n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {np.mean(epoch_losses):.4f}\")\n    all_losses.extend(epoch_losses)\n\n# plot train loss curve\nplt.figure(figsize=(10, 5))\nplt.plot(all_losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss Curve')\nplt.show()\n\n# Evaluation\nmodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for data, targets in test_loader:\n        data, targets = data.to(device), targets.to(device)\n        outputs = model(data)\n        _, predicted = torch.max(outputs.data, 1)\n        total += targets.size(0)\n        correct += (predicted == targets).sum().item()\n\nprint(f'Test Accuracy: {100 * correct / total:.2f}%')\n\ndevice: mps\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 32, 28, 28]             320\n         MaxPool2d-2           [-1, 32, 14, 14]               0\n            Conv2d-3           [-1, 64, 14, 14]          18,496\n         MaxPool2d-4             [-1, 64, 7, 7]               0\n            Conv2d-5             [-1, 32, 7, 7]          18,464\n         MaxPool2d-6             [-1, 32, 3, 3]               0\n            Linear-7                  [-1, 128]          36,992\n            Linear-8                   [-1, 10]           1,290\n================================================================\nTotal params: 75,562\nTrainable params: 75,562\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.37\nParams size (MB): 0.29\nEstimated Total Size (MB): 0.67\n----------------------------------------------------------------\nEpoch [1/5], Loss: 0.5219\n\n\n\nimport random\n\nclass_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']   # verify using train_dataset.classes\n\nmodel.eval()\nfig, axes = plt.subplots(2, 5, figsize=(15, 6))\nfig.suptitle('Sample Predictions', fontsize=16)\n\nwith torch.no_grad():\n    for i, idx in enumerate(random.sample(range(len(test_dataset)), 10)):\n        image, label = test_dataset[idx]\n        output = model(image.unsqueeze(0).to(device))\n        predicted = output.argmax(1).item()\n        \n        ax = axes[i // 5, i % 5]\n        ax.imshow(image.squeeze(), cmap='gray')\n        ax.axis('off')\n        ax.set_title(f'Pred: {class_names[predicted]}\\nTrue: {class_names[label]}', \n                     color='green' if predicted == label else 'red')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/cnns/00_cnn_basics.html#references",
    "href": "posts/cnns/00_cnn_basics.html#references",
    "title": "So you know convolutions huh?",
    "section": "References",
    "text": "References\n\nsebastian raschka course material here\nCS231N Cnn notes here. Great intuition and more detail about the shapes, local connectivity, spatial arrangement, and loads of other stuff.\narc folder where i maintain things related to CNNs here\nbeginner friendly article on Medium (its a great blog-series for ML) here\nVisualizing and Understanding Convolutional Networks. here"
  },
  {
    "objectID": "posts/rnns/01_rnns_from_scratch.html",
    "href": "posts/rnns/01_rnns_from_scratch.html",
    "title": "Language Modelling with RNNs",
    "section": "",
    "text": "The only reason you would be hearing RNNs right now is probably when xLSTMs were released in May, 2024. Apart from this they have pretty much taken a back seat to watch transformers revolutionalize NLP and the entire field of AI in general.\nBut one would do well to remember how we got here, and RNNs played a massive role in bringing us here. So in this blog post I‚Äôm going to build a small RNN model and we‚Äôll try to train it to generate text."
  },
  {
    "objectID": "posts/rnns/01_rnns_from_scratch.html#understanding-rnns",
    "href": "posts/rnns/01_rnns_from_scratch.html#understanding-rnns",
    "title": "Language Modelling with RNNs",
    "section": "Understanding RNNs",
    "text": "Understanding RNNs\nRNNs have 2 matrices, one (W_{xh}) that maps input tokens to hidden_vector size and another (W_{hh}) that maps from hidden_vector to hidden_vector. You‚Äôll see how these are used in a minute.\nLet us first look at input-output shapes for an RNN layer. We initially had a batch of text-tokens. Lets assume batch size of 4 and max_seq_len of 32. Hence the shape of input is (4,32).\nNow for each token, we encode it to a number and then map it to a vector (which we generally call an embedding). Hence each token is now represented by a vector of fixed-shape, and lets call this embedding_dimension and set it to 10. (This can also be done by classical methods like one-hot encoding, ngram-models)\nThe shape of our input batch is now (batch_size, max_seq_len, emb_dim), ie (4,32,10).\nNow let us peek into the matrix multiplications inside a RNN layer. Firstly, lets us recall that for a linear layer, this is the matrix equation:\nz (N, n_{out}) = \\sigma(x (N, n_{in}) * W_x^T (n_{in}, n_{out}) + b (N))\nwhere ,\n\ninput-features = n_{in}\noutput-features = n_{out}\nbatch-size = N\n\nIn a linear layer, each token/feature is attended to by a different weight in the weight matrix and no information is shared among the sequence tokens. But when processing ‚Äúsequences‚Äù we obviously want the model to remember stuff from previous tokens for the current token, right?\nHence RNNs maintain a hidden_vector for each token, that takes as input the current token and the hidden_vector from the previous token‚Äôs output.\nSo for the t‚Äôth token,\nh_t (N, h)= x_t (N, n_{in}) * W_{xh}^T (n_{in}, h) + h_{t-1} (N, h) * W_{hh}^T (h, h) + biases\nwhere\n\ninput-features = n_{in}\nhidden-size = h\nbatch-size = N\nsequence-length = s\n\nAs you‚Äôll notice since each token depends on previous tokens output, we cannot process this parallelly and have to iteratively calculate the output for each token. Also note we generally refer to the different tokens in a sequence as different timesteps, ie token at timestep t is x_t.\nHence for a complete batch, inputs are:\n\nX of shape (N, s, n_{in})\nh_0 of shape (N, h) (this is optional, if not given most libraries will initiate a h_0 of all zeros or random numbers)\n\nAnd outputs are:\n\nhidden states of all timesteps, ie H of shape (N, s, h)\nlast_hidden_state ie h_n of shape (N, h)\n\nNote: sometimes you will see outputs of rnn fed into a linear layer like so,\noutputs, h_n = self.rnn(x)\ny = self.fc(outputs[:,-1,:])\nHere h_n and outputs[:,-1,:] are the same thing. They both represent the last hidden state for the entire batch. (to make shapes equal use h_n.squeeze())\nLets verify the above by passing inputs to an rnn layer.\n\n\nshow code\nemb_dim = 128\nhidden_size = 128\nbatch_size = 8\nmax_seq_len = 32\n\nprint(f\"batch_size: {batch_size}, hidden_size: {hidden_size}, max_seq_len: {max_seq_len}, emb_dim: {emb_dim}\")\nX,y = get_data(train_data, seq_len=max_seq_len, batch_size=batch_size)\nprint(f\"shape of initial input -&gt; {X.shape}\")\n\nemb_layer = nn.Embedding(num_embeddings=len(vocab), embedding_dim=emb_dim)\nrnn_layer = nn.RNN(input_size=emb_dim, hidden_size=hidden_size, batch_first=True, bidirectional=False, num_layers=1)\n\nX = emb_layer(X)\nprint(f\"post embedding; shape of input to RNN layer -&gt; {X.shape}\")\nh_0 = torch.randn(1, batch_size, hidden_size)\noutputs = rnn_layer(X, h_0)\n\nprint(f\"RNN output shapes -&gt; {outputs[0].shape}, {[outputs[1][i].shape for i in range(len(outputs[1]))]}\")\n\n\nbatch_size: 8, hidden_size: 128, max_seq_len: 32, emb_dim: 128\nshape of initial input -&gt; torch.Size([8, 32])\npost embedding; shape of input to RNN layer -&gt; torch.Size([8, 32, 128])\nRNN output shapes -&gt; torch.Size([8, 32, 128]), [torch.Size([8, 128])]"
  },
  {
    "objectID": "posts/rnns/01_rnns_from_scratch.html#the-model",
    "href": "posts/rnns/01_rnns_from_scratch.html#the-model",
    "title": "Language Modelling with RNNs",
    "section": "The model",
    "text": "The model\n\n# calculate size of model parameters\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nclass Rnn_model(nn.Module):\n\n    def __init__(self, embedding_size, max_seq_len, hidden_size, num_layers=1, vocab_size=None):\n        \"\"\"\n        Initializes the Rnn_model class.\n\n        Args:\n            embedding_size (int): The size of the embedding dimension.\n            max_seq_len (int): The maximum sequence length.\n            hidden_size (int): The size of the hidden state dimension.\n            num_layers (int, optional): The number of recurrent layers. Defaults to 1.\n            vocab_size (int, optional): The size of the vocabulary. Defaults to None.\n\n        \"\"\"\n        super(Rnn_model, self).__init__()\n\n        self.max_seq_len = max_seq_len\n        self.vocab_size = len(vocab) if vocab_size is None else vocab_size\n        self.embedding = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=embedding_size)\n        self.rnn = nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, batch_first=True, num_layers=num_layers)\n        self.fc = nn.Linear(hidden_size, len(vocab))\n        self.softmax = nn.Softmax(dim=-1)\n        self.criterion = nn.CrossEntropyLoss(ignore_index=0)  # ignore pad token\n\n    def forward(self, x, targets=None):\n        \"\"\" \n        a forward pas thorugh the model.\n        x: input torch tensor (B,T,S)\n        targets: input targets (B,T,S)\n\n        Returns\n        (model output logits, loss)\n        \"\"\"\n        x = x[:, -self.max_seq_len:]\n        x = self.embedding(x)\n        H, h_n = self.rnn(x)\n\n        # y = self.fc(H[:,-1,:])\n        y = self.fc(H)\n        \n        if targets is not None:\n            B, T, V = y.shape\n            loss = self.criterion(y.view(B*T, V), targets.view(-1))\n        else: loss=None\n        \n        return y, loss\n    \n    \n    @torch.no_grad\n    def generate(self, input_text, max_len=32, device='cpu'):\n        \"\"\" \n        input_text: a string or list of strings to generate text using the model.\n        max_len: model will generate maximum of max_len tokens.\n        \"\"\"\n        \n        encoded_tokens = torch.tensor(encode_arr([input_text], max_seq_len=32))\n        if encoded_tokens.ndim == 1:\n            encoded_tokens = encoded_tokens.unsqueeze(0)\n\n        self = self.to(device)\n        encoded_tokens = encoded_tokens.to(device)\n        for i in range(max_len):\n            # only keep the most recent seq_len sized numbers.\n            outputs, _ = self(encoded_tokens[:, -self.max_seq_len:])\n\n            # last output token\n            outputs = outputs[:, -1, :]\n\n            # get pribabilities from logits\n            next_token_probs = nn.functional.softmax(outputs, dim=-1)\n\n            # sample indices from it using a multinomial distribution\n            next_tokens = torch.multinomial(next_token_probs, 1)\n\n            # concat prediction to original text\n            encoded_tokens = torch.concat((encoded_tokens, next_tokens), axis=1)\n\n        decoded_texts = decode_arr(encoded_tokens)\n        if len(decoded_texts)==1:\n            return decoded_texts[0] #.replace(\"&lt;pad&gt;\", \"\") \n        else: \n            # return [text.replace(\"&lt;pad&gt;\", \"\") for text in decoded_texts]\n            return decoded_texts"
  },
  {
    "objectID": "posts/rnns/01_rnns_from_scratch.html#training-loop",
    "href": "posts/rnns/01_rnns_from_scratch.html#training-loop",
    "title": "Language Modelling with RNNs",
    "section": "Training loop",
    "text": "Training loop\n\nfrom torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter('./runs/with_gradients/')\n\nNUM_STEPS = 10000\nMAX_SEQ_LEN = 32\nBATCH_SIZE = 32\nEMBEDDING_SIZE = 256\nHIDDEN_SIZE = 256\nNUM_LAYERS = 2\nLR = 0.001 \nLOG_EVERY = 200\nGENERATE_EVERY = 1000\n\ndevice = torch.device(\"mps\" if torch.backends.mps.is_built() else \"cpu\")\n\nmodel = Rnn_model(embedding_size=EMBEDDING_SIZE, max_seq_len=MAX_SEQ_LEN, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS)\nprint(f\"paramter count of the model: {count_parameters(model)}, data_size: {len(train_data)}\\n\")\noptimizer = torch.optim.Adam(model.parameters(), lr=LR)\nlosses = []\nfor i in (range(NUM_STEPS)):\n    optimizer.zero_grad()\n    x, y = get_data(train_data, BATCH_SIZE, MAX_SEQ_LEN)\n    \n    model.to(device)\n    x, y = x.to(device), y.to(device)\n    outputs, loss = model(x, y)\n    loss.backward()\n\n    # clip gradients\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n    # Log gradients before the optimization step\n    for name, param in model.named_parameters():\n        if param.grad is not None:\n            writer.add_histogram(f'gradients/{name}', param.grad, i)\n\n    optimizer.step()\n    if i%LOG_EVERY==0:\n        losses.append(loss)\n        # print(loss.item())\n    \n    writer.add_scalar('Loss/train', loss.item(), i)\n\n    \n    if i%GENERATE_EVERY==0 or i==NUM_STEPS-1:\n        print(f\"after {i} steps: \")\n        prompt = 'but I told him clearly, that if'\n        print(f\"prompt is: {prompt}\")\n        gen_text = model.generate(prompt, max_len=128)\n        gen_text = gen_text.replace(\"&lt;pad&gt;\", \"\")\n        print(f\"\\ntext generated by model: \\n{gen_text}\\n\")\n        print('-'*75)\n        \n\nwriter.close()\n\nlosses = [loss.cpu().detach().numpy().item() for loss in losses] \nprint(losses[::3])\n\nparamter count of the model: 1114232, data_size: 2449724\n\nafter 0 steps: \nprompt is: but I told him clearly, that if\n\ntext generated by model: \nbut I told him clearly, that if√¢DaL‚ÄîWX2m‚Äò;D3√¶Jq√®up¬Ω‚ÄôVDnJi^Rjje(lS√ØVcCrgWmMQSXA9W9`_F√Æ+E5l&1sVq\\6]=^f√™‚Äî&gt;804√†√¢!y√±√™I„Éªy=6&l,{$O√ª.1‚Äîu*$9‚ÄòKM!O√±-cW‚ÄôM√©√±hB~M.j6r{}xl\n\n---------------------------------------------------------------------------\nafter 1000 steps: \nprompt is: but I told him clearly, that if\n\ntext generated by model: \nbut I told him clearly, that if\nTwen tracked of him grivel-napregred whotain one come,\nin hy midge withoush, which shoulquould before so speeded the feet in th\n\n---------------------------------------------------------------------------\nafter 2000 steps: \nprompt is: but I told him clearly, that if\n\ntext generated by model: \nbut I told him clearly, that if still come, but I don‚Äôt know. Mc! So yester I shall five onnestible, ‚Äúthat peired, want and\nasfected to his harded has\nFarching\n\n---------------------------------------------------------------------------\nafter 3000 steps: \nprompt is: but I told him clearly, that if\n\ntext generated by model: \nbut I told him clearly, that ifner, for sheep there indobless.\nAnd Holmes\nwhen we start the once.‚Äù\n\n‚ÄúThanks between rouse vrisings of as one of\nfiried presing,\n\n---------------------------------------------------------------------------\nafter 4000 steps: \nprompt is: but I told him clearly, that if\n\ntext generated by model: \nbut I told him clearly, that if you had a explain and after we like it. He is every are that the main that in a valued her this example in our myselture for th\n\n---------------------------------------------------------------------------\nafter 5000 steps: \nprompt is: but I told him clearly, that if\n\ntext generated by model: \nbut I told him clearly, that if yim you have helpered befored. Therest name It\nfamousore, that he sat three\n      it, and he said that we met him earsh back in\n\n---------------------------------------------------------------------------\nafter 6000 steps: \nprompt is: but I told him clearly, that if\n\ntext generated by model: \nbut I told him clearly, that if I can\nstarted which he came many way heir imperiable takely room. A now,\n      ‚ÄúWhat was only fancy,‚Äù he was a gentleman concei\n\n---------------------------------------------------------------------------\nafter 7000 steps: \nprompt is: but I told him clearly, that if\n\ntext generated by model: \nbut I told him clearly, that if your mistaken, and yet use_ the\nClose of Baskerville, is obvious fashions and left your\nbrusticion of the longly seat from this\n\n---------------------------------------------------------------------------\nafter 8000 steps: \nprompt is: but I told him clearly, that if\n\ntext generated by model: \nbut I told him clearly, that if I‚Äôd fain the\nothers.‚Äù\n\n‚ÄúHow made you, Watson, he was demilent? With past as another? One enemal with fift me out and was art vi\n\n---------------------------------------------------------------------------\nafter 9000 steps: \nprompt is: but I told him clearly, that if\n\ntext generated by model: \nbut I told him clearly, that if I do follow made the owns\nbehind these common understand that is I, and they may not cast\noccurred to suggest we idea?‚Äù\n\n‚ÄúNot r\n\n---------------------------------------------------------------------------\nafter 9999 steps: \nprompt is: but I told him clearly, that if\n\ntext generated by model: \nbut I told him clearly, that if if I was able faintled back in the hand, air which had\ndirmanscoctors way. That‚Äôs close.\n\n‚ÄúDoes_ gone. I recond feare, and that\n\n---------------------------------------------------------------------------\n[4.787812232971191, 1.8653450012207031, 1.7018814086914062, 1.655837059020996, 1.5209026336669922, 1.5495105981826782, 1.5696816444396973, 1.560757040977478, 1.5683640241622925, 1.4509146213531494, 1.4306949377059937, 1.4603841304779053, 1.3696606159210205, 1.5044567584991455, 1.3786689043045044, 1.3815288543701172, 1.3697071075439453]\n\n\n\n\nshow code\n# plot losses\nimport matplotlib.pyplot as plt\nplt.plot(losses)\nplt.title(\"Training Loss\")\nplt.xlabel(\"Steps\")\nplt.ylabel(\"Loss\")\n\n\nText(0, 0.5, 'Loss')\n\n\n\n\n\n\n\n\n\n\nsqitching from SGD to adam instantly gives boost (2.8 vs under 2)\ntripling the data gave no gains on 50k basic rnn model!\n300k characters seems enough to generate coherent text, why not happening here?\n\nlets try a few things, take small corpus (2-3mil chars), &lt;1 million param model and use techiques like grad-clipping, mps on mac local(faster), and see if we can get it to generate coherent text.\nmps reduces training-time by more than half.\ngrad clipping doesnt seem to affect quality on inspection of generated text.\n\nlogging to tensorboard to keep track of experiments and to visualize gradients too. (X-ray vision)\n\nLets look at some outputs from the model.\n\nfor line in model.generate([\"hey there handsome what\", \"but I told him clearly, that if\"], max_len=256, device=\"cpu\"):\n    print(line.replace(\"&lt;pad&gt;\", \"\"), \"\\n\")\n\nhey there handsome whateve. It was the matlice. Now,‚Äù get\n      with the foot up a death five office, for I had\n      left not be assumed his hands and way, and McMurdo done to like\n      that he was no amid good discollance. They are also dashing came? They were alone of as wem \n\nbut I told him clearly, that if he is neighbours, then a place\nof Norwood the end, however, that on the raid of them frece. ‚ÄòYou were they found down certain that\n      good victupek and spoken upon an\nway.‚Äù\n\n      Litch is a feweprecions, Mr. Whomay for the\ncrowns, indibribulties, knet \n\n\n\nThe results dont make much sense, but the model is learning to generate text. By scaling the model and training for longer, we can get better results.\nThank you for reading this and I hope you found it useful. The next time I update this blog post, I‚Äôll be adding the following:\n\nEvaluation on test set (metrics like perplexity).\nImplementing RNN, LSTM and GRU from scratch.\n\nByee :)\n\n100k rnn model - 3million chars: ~1.5 loss, kinda starts to form words\noutputs: 1. hey there handsome what death which are a discroose.‚Äù 2. I recall went small-boy‚Äôs Louded spoke. T\n\n\n100k rnn model - 12million chars: ~1.6 loss, similar\n\nhey there handsome what to on?‚Äù\n\nMidams: I lady of quiked in her put intented to the burgent wonder the risons of the saw four spikely. To belled 2. I recall Katerinoad and sure conforty, unsole only geitute and, and it stigged refundled as a was abserved. Every tran at you.‚Äù\nTHO‚ÄùCi\n\n\n330k lstm model - 12million chars: ~ 1.52 loss,\n\nhey there handsome what?‚Äù are that indid of nebvense of the Samiskamen theory instance Loins, fry committed, or suggested fagreedingly.\n\nJudge Stannaph 2. I recall are that any quality shall, as the man been unknight, in his closkered. The common as though sad so‚Äôll he had she askoccite\n\n\n1million lstm model - 3million chars: ~1.4 loss, 10k steps\nparamter count of the model: 1114232, 3million chars\ntext generated by model after 0 steps of training but I told him clearly, that ifVOS¬£QY^D√™X¬£v√©F LD[s≈ìPyZ√™t¬Ω‚ÄôsXz&lt;nf#t0|%√ßIw&lt;√†‚Äô\"n„Éª√™+r^√™i?\\fZ√¢El@K\nYu%@3V√¢ZJ}_}B` √†l„Éª√†r√Ø:h√†NBbZ0j¬Ω~6#√ª8v√†C/[VnKR√©√ºylyh√™|.√ª√Æ&EJ2I\ntext generated by model after 1000 steps of training but I told him clearly, that if the bet‚Äôs of the yecraiuply that deen-tated for langent pasts with! I was a tedeter.‚Äù I at, encurned. A, my dook a\ntext generated by model after 2000 steps of training but I told him clearly, that if he reabsa deed some seard us a heard his nevriept took‚Äôh myself and come against is need of Eaker, and terrised a clearly enoye\ntext generated by model after 3000 steps of training but I told him clearly, that if Right then, who roronian had monting that I must settating you passion. Fut us contain him thion. Amy taken and to sympent-litt\ntext generated by model after 4000 steps of training but I told him clearly, that if ears whet a your smalls friend so.‚Äù\n‚ÄúAnd you? Scotling us in the ensetate on the stappered. ‚ÄúA stors, but which I work w\ntext generated by model after 5000 steps of training but I told him clearly, that if be whose give anches was for me to his many wears the anyone? The back down of brought than answered, sir; ‚ÄúSudden horror and c\ntext generated by model after 6000 steps of training but I told him clearly, that if he had telegreas.‚Äù\n‚ÄúWhy I knew I have been examed.‚Äù\n‚ÄúCertails, he spetter lodge oppon the next opportmend in I. I skined it,\ntext generated by model after 7000 steps of training but I told him clearly, that if the Dark, sharp_ and that their repaudance with us a fearer with a largue down, ‚Äúbut the live night, Murch-Starter reading upon\ntext generated by model after 8000 steps of training but I told him clearly, that if you will opens but I year?‚Äù said he. ‚ÄúThat is near that shifted as into the canner life came.‚ÄîThis warning friend that I had ca\ntext generated by model after 9000 steps of training but I told him clearly, that if I trauntion, that they are note too vellering keep of their part, and there was help being upon us the whole was snicks when?‚Äù"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hi Y‚Äôall. My name is Deepam Minda and this is my website. I‚Äôll be writing a lot more blogs, TILs, etc about various things that interest me. I‚Äôm interested in topics like AI, llms, and tech in general.\nI am a data scientist/ML engineer at Farmart by profession. I graduated from Delhi Technological University, Delhi, India. Apart from this, I also ocassionally dabble with a bit of guitar, and like everyone sane, music in general.\nI am always inspired and fascinated by excellent blogs maintained by people online where they share all sorts of cool stuff. I hope you learn something new here. Feel free to share your thoughts or hit me up.\nCheers!"
  }
]