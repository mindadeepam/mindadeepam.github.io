[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "blog",
    "section": "",
    "text": "The Floyd-Warshall Algorithm\n\n\n4 min\n\n\n\nAug 22, 2024\n\n\n\n\n\n\n\n\n\n\n\nTraining a Resnet in Pytorch\n\n\n6 min\n\n\n\nAug 10, 2024\n\n\n\n\n\n\n\n\n\n\n\nIntro to Convolution Neural Networks\n\n\n22 min\n\n\n\nJul 30, 2024\n\n\n\n\n\n\n\n\n\n\n\nLanguage Modelling with RNNs\n\n\n10 min\n\n\n\nJul 20, 2024\n\n\n\n\n\n\n\n\n\n\n\nFew Shot learning: Classify using few examples!\n\n\n7 min\n\n\n\nSep 12, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "Hi Y’all. My name is Deepam Minda and this is my website. I’ll be writing a lot more blogs, TILs, etc about various things that interest me, which include but are not limited to AI, tech, music, football.\nI am a data scientist/ML engineer at Farmart by profession. I graduated from Delhi Technological University, Delhi, India. Apart from this, I also ocassionally dabble with a bit of guitar, and like everyone sane, music in general.\nI am always inspired and fascinated by excellent blogs maintained by people online where they share all sorts of cool stuff. I hope you learn something new here. Feel free to share your thoughts or hit me up.\nCheers!"
  },
  {
    "objectID": "posts/rnns/01_rnns_basics.html",
    "href": "posts/rnns/01_rnns_basics.html",
    "title": "Language Modelling with RNNs",
    "section": "",
    "text": "The only reason you would be hearing RNNs right now is probably when xLSTMs were released in May, 2024. Apart from this they have pretty much taken a back seat to watch transformers revolutionalize NLP and the entire field of AI in general.\nBut one would do well to remember how we got here, and RNNs played a massive role in bringing us here. So in this blog post I’m going to build a small RNN model and we’ll try to train it to generate text."
  },
  {
    "objectID": "posts/rnns/01_rnns_basics.html#understanding-rnns",
    "href": "posts/rnns/01_rnns_basics.html#understanding-rnns",
    "title": "Language Modelling with RNNs",
    "section": "Understanding RNNs",
    "text": "Understanding RNNs\nRNNs have 2 matrices, one (W_{xh}) that maps input tokens to hidden_vector size and another (W_{hh}) that maps from hidden_vector to hidden_vector. You’ll see how these are used in a minute.\nLet us first look at input-output shapes for an RNN layer. We initially had a batch of text-tokens. Lets assume batch size of 4 and max_seq_len of 32. Hence the shape of input is (4,32).\nNow for each token, we encode it to a number and then map it to a vector (which we generally call an embedding). Hence each token is now represented by a vector of fixed-shape, and lets call this embedding_dimension and set it to 10. (This can also be done by classical methods like one-hot encoding, ngram-models)\nThe shape of our input batch is now (batch_size, max_seq_len, emb_dim), ie (4,32,10).\nNow let us peek into the matrix multiplications inside a RNN layer. Firstly, lets us recall that for a linear layer, this is the matrix equation:\nz (N, n_{out}) = \\sigma(x (N, n_{in}) * W_x^T (n_{in}, n_{out}) + b (N))\nwhere ,\n\ninput-features = n_{in}\noutput-features = n_{out}\nbatch-size = N\n\nIn a linear layer, each token/feature is attended to by a different weight in the weight matrix and no information is shared among the sequence tokens. But when processing “sequences” we obviously want the model to remember stuff from previous tokens for the current token, right?\nHence RNNs maintain a hidden_vector for each token, that takes as input the current token and the hidden_vector from the previous token’s output.\nSo for the t’th token,\nh_t (N, h)= x_t (N, n_{in}) * W_{xh}^T (n_{in}, h) + h_{t-1} (N, h) * W_{hh}^T (h, h) + biases\nwhere\n\ninput-features = n_{in}\nhidden-size = h\nbatch-size = N\nsequence-length = s\n\nAs you’ll notice since each token depends on previous tokens output, we cannot process this parallelly and have to iteratively calculate the output for each token. Also note we generally refer to the different tokens in a sequence as different timesteps, ie token at timestep t is x_t.\nHence for a complete batch, inputs are:\n\nX of shape (N, s, n_{in})\nh_0 of shape (N, h) (this is optional, if not given most libraries will initiate a h_0 of all zeros or random numbers)\n\nAnd outputs are:\n\nhidden states of all timesteps, ie H of shape (N, s, h)\nlast_hidden_state ie h_n of shape (N, h)\n\nNote: sometimes you will see outputs of rnn fed into a linear layer like so,\noutputs, h_n = self.rnn(x)\ny = self.fc(outputs[:,-1,:])\nHere h_n and outputs[:,-1,:] are the same thing. They both represent the last hidden state for the entire batch. (to make shapes equal use h_n.squeeze())\nLets verify the above by passing inputs to an rnn layer.\n\n\nshow code\nemb_dim = 128\nhidden_size = 128\nbatch_size = 8\nmax_seq_len = 32\n\nprint(f\"batch_size: {batch_size}, hidden_size: {hidden_size}, max_seq_len: {max_seq_len}, emb_dim: {emb_dim}\")\nX,y = get_data(train_data, seq_len=max_seq_len, batch_size=batch_size)\nprint(f\"shape of initial input -&gt; {X.shape}\")\n\nemb_layer = nn.Embedding(num_embeddings=len(vocab), embedding_dim=emb_dim)\nrnn_layer = nn.RNN(input_size=emb_dim, hidden_size=hidden_size, batch_first=True, bidirectional=False, num_layers=1)\n\nX = emb_layer(X)\nprint(f\"post embedding; shape of input to RNN layer -&gt; {X.shape}\")\nh_0 = torch.randn(1, batch_size, hidden_size)\noutputs = rnn_layer(X, h_0)\n\nprint(f\"RNN output shapes -&gt; {outputs[0].shape}, {[outputs[1][i].shape for i in range(len(outputs[1]))]}\")\n\n\nbatch_size: 8, hidden_size: 128, max_seq_len: 32, emb_dim: 128\nshape of initial input -&gt; torch.Size([8, 32])\npost embedding; shape of input to RNN layer -&gt; torch.Size([8, 32, 128])\nRNN output shapes -&gt; torch.Size([8, 32, 128]), [torch.Size([8, 128])]"
  },
  {
    "objectID": "posts/rnns/01_rnns_basics.html#the-model",
    "href": "posts/rnns/01_rnns_basics.html#the-model",
    "title": "Language Modelling with RNNs",
    "section": "The model",
    "text": "The model\n\n# calculate size of model parameters\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nclass Rnn_model(nn.Module):\n\n    def __init__(self, embedding_size, max_seq_len, hidden_size, num_layers=1, vocab_size=None):\n        \"\"\"\n        Initializes the Rnn_model class.\n\n        Args:\n            embedding_size (int): The size of the embedding dimension.\n            max_seq_len (int): The maximum sequence length.\n            hidden_size (int): The size of the hidden state dimension.\n            num_layers (int, optional): The number of recurrent layers. Defaults to 1.\n            vocab_size (int, optional): The size of the vocabulary. Defaults to None.\n\n        \"\"\"\n        super(Rnn_model, self).__init__()\n\n        self.max_seq_len = max_seq_len\n        self.vocab_size = len(vocab) if vocab_size is None else vocab_size\n        self.embedding = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=embedding_size)\n        self.rnn = nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, batch_first=True, num_layers=num_layers)\n        self.fc = nn.Linear(hidden_size, len(vocab))\n        self.softmax = nn.Softmax(dim=-1)\n        self.criterion = nn.CrossEntropyLoss(ignore_index=0)  # ignore pad token\n\n    def forward(self, x, targets=None):\n        \"\"\" \n        a forward pas thorugh the model.\n        x: input torch tensor (B,T,S)\n        targets: input targets (B,T,S)\n\n        Returns\n        (model output logits, loss)\n        \"\"\"\n        x = x[:, -self.max_seq_len:]\n        x = self.embedding(x)\n        H, h_n = self.rnn(x)\n\n        # y = self.fc(H[:,-1,:])\n        y = self.fc(H)\n        \n        if targets is not None:\n            B, T, V = y.shape\n            loss = self.criterion(y.view(B*T, V), targets.view(-1))\n        else: loss=None\n        \n        return y, loss\n    \n    \n    @torch.no_grad\n    def generate(self, input_text, max_len=32, device='cpu'):\n        \"\"\" \n        input_text: a string or list of strings to generate text using the model.\n        max_len: model will generate maximum of max_len tokens.\n        \"\"\"\n        \n        encoded_tokens = torch.tensor(encode_arr([input_text], max_seq_len=32))\n        if encoded_tokens.ndim == 1:\n            encoded_tokens = encoded_tokens.unsqueeze(0)\n\n        self = self.to(device)\n        encoded_tokens = encoded_tokens.to(device)\n        for i in range(max_len):\n            # only keep the most recent seq_len sized numbers.\n            outputs, _ = self(encoded_tokens[:, -self.max_seq_len:])\n\n            # last output token\n            outputs = outputs[:, -1, :]\n\n            # get pribabilities from logits\n            next_token_probs = nn.functional.softmax(outputs, dim=-1)\n\n            # sample indices from it using a multinomial distribution\n            next_tokens = torch.multinomial(next_token_probs, 1)\n\n            # concat prediction to original text\n            encoded_tokens = torch.concat((encoded_tokens, next_tokens), axis=1)\n\n        decoded_texts = decode_arr(encoded_tokens)\n        if len(decoded_texts)==1:\n            return decoded_texts[0] #.replace(\"&lt;pad&gt;\", \"\") \n        else: \n            # return [text.replace(\"&lt;pad&gt;\", \"\") for text in decoded_texts]\n            return decoded_texts"
  },
  {
    "objectID": "posts/rnns/01_rnns_basics.html#training-loop",
    "href": "posts/rnns/01_rnns_basics.html#training-loop",
    "title": "Language Modelling with RNNs",
    "section": "Training loop",
    "text": "Training loop\nNow I’ll define a basic training loop to train this model, and we’ll generate text every 1k steps to see the model learn right before us! Im also using tensorboard to log and view my runs. IMO its the best way to visualize loss curves and debug model behavior.\n\nfrom torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter('./runs/with_gradients/')\n\nNUM_STEPS = 10000\nMAX_SEQ_LEN = 32\nBATCH_SIZE = 32\nEMBEDDING_SIZE = 256\nHIDDEN_SIZE = 256\nNUM_LAYERS = 2\nLR = 0.001 \nLOG_EVERY = 200\nGENERATE_EVERY = 1000\n\ndevice = torch.device(\"mps\" if torch.backends.mps.is_built() else \"cpu\")\n\nmodel = Rnn_model(embedding_size=EMBEDDING_SIZE, max_seq_len=MAX_SEQ_LEN, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS)\nprint(f\"paramter count of the model: {count_parameters(model)}, data_size: {len(train_data)}\\n\")\noptimizer = torch.optim.Adam(model.parameters(), lr=LR)\nlosses = []\nfor i in (range(NUM_STEPS)):\n    optimizer.zero_grad()\n    x, y = get_data(train_data, BATCH_SIZE, MAX_SEQ_LEN)\n    \n    model.to(device)\n    x, y = x.to(device), y.to(device)\n    outputs, loss = model(x, y)\n    loss.backward()\n\n    # clip gradients\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n    # Log gradients before the optimization step\n    for name, param in model.named_parameters():\n        if param.grad is not None:\n            writer.add_histogram(f'gradients/{name}', param.grad, i)\n\n    optimizer.step()\n    if i%LOG_EVERY==0:\n        losses.append(loss)\n        # print(loss.item())\n    \n    writer.add_scalar('Loss/train', loss.item(), i)\n\n    \n    if i%GENERATE_EVERY==0 or i==NUM_STEPS-1:\n        print(f\"after {i} steps: \")\n        prompt = 'but I told him clearly, that if'\n        print(f\"prompt is: {prompt}\")\n        gen_text = model.generate(prompt, max_len=128)\n        gen_text = gen_text.replace(\"&lt;pad&gt;\", \"\")\n        print(f\"\\ntext generated by model: \\n{gen_text}\\n\")\n        print('-'*75)\n        \n\nwriter.close()\n\nlosses = [loss.cpu().detach().numpy().item() for loss in losses] \n# print(losses[::3])\n\nparamter count of the model: 1114232, data_size: 2449724\n\nafter 0 steps: \nprompt is: but I told him clearly, that if\n\ntext generated by model: \nbut I told him clearly, that ifâDaL—WX2m‘;D3æJqèup½’VDnJi^Rjje(lSïVcCrgWmMQSXA9W9`_Fî+E5l&1sVq\\6]=^fê—&gt;804àâ!yñêI・y=6&l,{$Oû.1—u*$9‘KM!Oñ-cW’MéñhB~M.j6r{}xl\n\n---------------------------------------------------------------------------\nafter 1000 steps: \nprompt is: but I told him clearly, that if\n\ntext generated by model: \nbut I told him clearly, that if\nTwen tracked of him grivel-napregred whotain one come,\nin hy midge withoush, which shoulquould before so speeded the feet in th\n\n---------------------------------------------------------------------------\nafter 2000 steps: \nprompt is: but I told him clearly, that if\n\ntext generated by model: \nbut I told him clearly, that if still come, but I don’t know. Mc! So yester I shall five onnestible, “that peired, want and\nasfected to his harded has\nFarching\n\n---------------------------------------------------------------------------\nafter 3000 steps: \nprompt is: but I told him clearly, that if\n\ntext generated by model: \nbut I told him clearly, that ifner, for sheep there indobless.\nAnd Holmes\nwhen we start the once.”\n\n“Thanks between rouse vrisings of as one of\nfiried presing,\n\n---------------------------------------------------------------------------\nafter 4000 steps: \nprompt is: but I told him clearly, that if\n\ntext generated by model: \nbut I told him clearly, that if you had a explain and after we like it. He is every are that the main that in a valued her this example in our myselture for th\n\n---------------------------------------------------------------------------\nafter 5000 steps: \nprompt is: but I told him clearly, that if\n\ntext generated by model: \nbut I told him clearly, that if yim you have helpered befored. Therest name It\nfamousore, that he sat three\n      it, and he said that we met him earsh back in\n\n---------------------------------------------------------------------------\nafter 6000 steps: \nprompt is: but I told him clearly, that if\n\ntext generated by model: \nbut I told him clearly, that if I can\nstarted which he came many way heir imperiable takely room. A now,\n      “What was only fancy,” he was a gentleman concei\n\n---------------------------------------------------------------------------\nafter 7000 steps: \nprompt is: but I told him clearly, that if\n\ntext generated by model: \nbut I told him clearly, that if your mistaken, and yet use_ the\nClose of Baskerville, is obvious fashions and left your\nbrusticion of the longly seat from this\n\n---------------------------------------------------------------------------\nafter 8000 steps: \nprompt is: but I told him clearly, that if\n\ntext generated by model: \nbut I told him clearly, that if I’d fain the\nothers.”\n\n“How made you, Watson, he was demilent? With past as another? One enemal with fift me out and was art vi\n\n---------------------------------------------------------------------------\nafter 9000 steps: \nprompt is: but I told him clearly, that if\n\ntext generated by model: \nbut I told him clearly, that if I do follow made the owns\nbehind these common understand that is I, and they may not cast\noccurred to suggest we idea?”\n\n“Not r\n\n---------------------------------------------------------------------------\nafter 9999 steps: \nprompt is: but I told him clearly, that if\n\ntext generated by model: \nbut I told him clearly, that if if I was able faintled back in the hand, air which had\ndirmanscoctors way. That’s close.\n\n“Does_ gone. I recond feare, and that\n\n---------------------------------------------------------------------------\n[4.787812232971191, 1.8653450012207031, 1.7018814086914062, 1.655837059020996, 1.5209026336669922, 1.5495105981826782, 1.5696816444396973, 1.560757040977478, 1.5683640241622925, 1.4509146213531494, 1.4306949377059937, 1.4603841304779053, 1.3696606159210205, 1.5044567584991455, 1.3786689043045044, 1.3815288543701172, 1.3697071075439453]\n\n\n\n\nshow code\n# plot losses\nimport matplotlib.pyplot as plt\n\nplt.plot(losses)\nplt.title(\"Training Loss\")\nplt.xlabel(\"Steps\")\nplt.ylabel(\"Loss\")\n\n\nText(0, 0.5, 'Loss')\n\n\n\n\n\n\n\n\n\nThe results certainly improve over the iterations, and its clearly visible that the model is learning to generate text. By scaling the model and training for longer, we can get better results.\nThank you for reading this and I hope you found it useful. The next time I update this blog post, I’ll be adding the following:\n\nEvaluation on test set (metrics like perplexity).\nImplementing RNN, LSTM and GRU from scratch.\n\nByee :)"
  },
  {
    "objectID": "posts/cnns/00_cnn_basics.html",
    "href": "posts/cnns/00_cnn_basics.html",
    "title": "Intro to Convolution Neural Networks",
    "section": "",
    "text": "Hello everyone, in this notebook we’re going to understand the bare fundamentals of convolutional neural networks. We’ll start by getting to know the convolutional operation itself and then proceed to make a very simple CNN model.\nThe pre-requisites for this post are:"
  },
  {
    "objectID": "posts/cnns/00_cnn_basics.html#convolutions",
    "href": "posts/cnns/00_cnn_basics.html#convolutions",
    "title": "Intro to Convolution Neural Networks",
    "section": "Convolutions",
    "text": "Convolutions\nConvolution is a mathematical operation that combines two signals to produce a third signal, representing how one signal modifies the other. In general terms, convolution is used to apply a filter (or kernel) to a signal or data.\nFor discrete signals, the convolution of two signals \\(f[n]\\) and \\(g[n]\\) is defined as:\n\\[(f * g)[n] = \\sum_{m=-\\infty}^{\\infty} f[m] \\cdot g[n - m]\\]\nWhere: - \\(f[n]\\) is the input signal, - \\(g[n]\\) is the filter (or kernel), - \\((f * g)[n]\\) is the resulting convolved signal.\nThis operation involves flipping the filter \\(g[n]\\), shifting it across the input signal \\(f[n]\\), multiplying the overlapping values, and summing the results to produce the output signal.\n\nSteps in Convolution\nConvolution involves the following steps:\n\nPrepare the Kernel: Take the transpose(flip) of the kernel \\((g)\\) along the resolution dimensions (\\(H,W\\)).see why\nAlign and Slide: Position the flipped kernel at the start of the input signal \\((f)\\). Then, systematically slide it across the entire length of \\((f)\\).\nMultiply: At each position, perform element-wise multiplication between the overlapping portions of the flipped kernel and the input signal.\nSum: Add up all the products from step 3 to get a single value. This value represents the convolution result at the current position.\nRecord and Repeat: Store the sum as an element in the output signal, then move the kernel to the next position and repeat steps 3-5 until the entire input signal has been covered.\n\nThe resulting output signal represents how the kernel has “filtered” or modified the input signal, highlighting certain features or patterns based on the kernel’s characteristics.\nThis process can be extended to 2D (for images) or higher dimensions, where the kernel slides over the input in all dimensions.\nFor a 1d array this might look like this:\n\n\nCode\n\nimport numpy as np \n\nf = np.array([1,2,3,-1, 10,1])\ng = np.array([1,0,-1,1])\ninverted_g = g[::-1]\n\nprint(f\"f: {f}\")\nprint(f\"g: {g}\")\nprint(f\"inverted_g, g`: {inverted_g}\")\n\nwindow_size = g.shape[0]\nresult = []\n\nprint(\"\\nSteps in convolution, slide g` over f in a loop:\")\nprint(\"'* is dot product' \\n\")\nprint(\"result = f*g = [_ _ _ _]\\n\")\nfor idx, i in enumerate(range(len(f)-window_size+1)):\n    slice_of_f = f[i:i+window_size]\n    print(f\"step {idx+1}. \\n{slice_of_f} * {inverted_g} = {slice_of_f.dot(inverted_g)}\")\n    # print(f\"step {idx+1}.2 sum({slice_of_f * inverted_g}) = {np.sum(slice_of_f * inverted_g)}\")\n    result.append(np.sum(slice_of_f * inverted_g))\n    print(f\"result[{i+1}]: {result[i]}\\n\")\n\nprint(f\"result: {result}\")\n\n\nf: [ 1  2  3 -1 10  1]\ng: [ 1  0 -1  1]\ninverted_g, g`: [ 1 -1  0  1]\n\nSteps in convolution, slide g` over f in a loop:\n'* is dot product' \n\nresult = f*g = [_ _ _ _]\n\nstep 1. \n[ 1  2  3 -1] * [ 1 -1  0  1] = -2\nresult[1]: -2\n\nstep 2. \n[ 2  3 -1 10] * [ 1 -1  0  1] = 9\nresult[2]: 9\n\nstep 3. \n[ 3 -1 10  1] * [ 1 -1  0  1] = 5\nresult[3]: 5\n\nresult: [-2, 9, 5]\n\n\nConsider len(f)=M & len(g)=N. We also have a few types of convolutions, based on how much padding is added to the input signal:\n\nValid Convolution:\n\nThe output is smaller than the input. No padding is applied.\nMode ‘valid’ returns output of length max(M, N) - min(M, N) + 1.\n\nSame Convolution:\n\nPadding is applied to keep the output size the same as the input.\nMode ‘same’ returns output of length max(M, N).\n\n\n\n\nCode\nprint(f\"np.convolve(f,g, mode='same') -&gt; {np.convolve(f,g, mode='same')}\")\n\n\nnp.convolve(f,g, mode='same') -&gt; [  2   2  -2   9   5 -11]\n\n\n\nFull Convolution:\n\nMaximum padding is applied.\nThis returns the convolution at each point of overlap, with an output shape of (N+M-1,).\n\n\n\n\nCode\nprint(f\"full f*g \\n-&gt; {np.pad(f,len(g)-1)} * {g[::-1]}\")\nprint(f\"-&gt; {np.convolve(f,g, mode='full')}\")\n\n\nfull f*g \n-&gt; [ 0  0  0  1  2  3 -1 10  1  0  0  0] * [ 1 -1  0  1]\n-&gt; [  1   2   2  -2   9   5 -11   9   1]\n\n\nThat wasnt so hard was it? Lets look at an example for 2d arrays.\nLets take 2 matrices \\(f\\) & \\(g\\) of shapes (4,4) and (3,3). Since \\(g\\) is a square kernel with side=3, we say it has size 3. First we need to flip the \\(g\\) both horizontally and vertically. Then, for each \\(g\\) sized block in \\(f\\), we do element-wise multiplication then summation.\nLets see how this works.\n\n\n\n2d conv\n\n\n\n\nCode\nf = np.random.rand(4,4)\ng = np.array([[1,0,-1],[2,0,-2],[3,0,-3]])\ninverted_g = g[::-1, ::-1]\nprint(f\"(f): \\n{f}\\n\")\nprint(f\"(g): \\n{g}\\n\")\nprint(f\"(g`): \\n{inverted_g}\\n\")\n\n\n(f): \n[[0.56832851 0.1169398  0.62616176 0.94949466]\n [0.06633587 0.83629314 0.79893411 0.07711767]\n [0.11524299 0.48151466 0.3024186  0.34043628]\n [0.76204227 0.80463651 0.4008499  0.4356766 ]]\n\n(g): \n[[ 1  0 -1]\n [ 2  0 -2]\n [ 3  0 -3]]\n\n(g`): \n[[-3  0  3]\n [-2  0  2]\n [-1  0  1]]\n\n\n\n\n\nCode\nf_rows, f_columns = f.shape \ng_rows, g_columns = g.shape\n\nresult = np.zeros((f_rows-g_rows+1, f_columns-g_columns+1))\nprint(f\"Convolution steps:\")\nfor i in range(f_rows-g_rows+1):\n    for j in range(f_columns-g_columns+1):\n        item = f[i:i+g_rows, j:j+g_columns] * inverted_g\n        if i&lt;3 and j==0:\n            print(f\"step {i+j+1}\\n-&gt; f[{i}:{i+g_rows}, {j}:{j+g_columns}] * g`: \\n{item}\")\n            print(f\"result[{i},{j}] -&gt; {np.sum(item):.4f}\\n\")\n        result[i,j] = np.sum(item)\nprint(\"... and so on.\\n\")\nprint(f\"final result: \\n{result}\\n\")\n\n\nConvolution steps:\nstep 1\n-&gt; f[0:3, 0:3] * g`: \n[[-1.70498553  0.          1.87848528]\n [-0.13267174  0.          1.59786822]\n [-0.11524299  0.          0.3024186 ]]\nresult[0,0] -&gt; 1.8259\n\nstep 2\n-&gt; f[1:4, 0:3] * g`: \n[[-0.1990076   0.          2.39680234]\n [-0.23048599  0.          0.60483719]\n [-0.76204227  0.          0.4008499 ]]\nresult[1,0] -&gt; 2.2110\n\n... and so on.\n\nfinal result: \n[[ 1.82587185  0.83823527]\n [ 2.21095358 -2.92864308]]\n\n\n\nPadding: Padding is now applied on both the height and width dimensions.\nStride: You can see that the kernel shifts by one pixel everytime. This is the stride of this conv operation. It is generally applied symmetrically across \\(H\\) & \\(W\\). You can see how doubling the stride would reduce output size by half. In general, larger stride → Smaller output (since we’re skipping pixels)\n\n\n\n1\n\n\n\n\n\n2\n\n\nTwo examples of 2D convolution operation: (1) no padding and 1x1 strides. (2) padding = 1 and 2x2 strides. (Image source: deeplearning.net)"
  },
  {
    "objectID": "posts/cnns/00_cnn_basics.html#neural-networks",
    "href": "posts/cnns/00_cnn_basics.html#neural-networks",
    "title": "Intro to Convolution Neural Networks",
    "section": "Neural networks",
    "text": "Neural networks\nNow that we clearly understand what a convolution is, lets get back to neural networks. Suppose you wanted to classify images before CNNs were developed. How would you do it? In most cases, you would use some pre-determined filters to extract features from the images and then use a fully connected network to classify the images. Those features could be:\n\nEdge filters: Used to detect edges in images. Common edge detection filters include:\n\nSobel filter: Emphasizes horizontal or vertical edges\nPrewitt filter: Similar to Sobel, but with different coefficients\nLaplacian filter: Detects edges in all directions\n\nTexture filters: Used to capture texture information:\n\nGabor filters: Detect specific frequencies and orientations\nLaws’ texture energy measures: A set of filters for texture analysis\n\nColor histograms: Represent the distribution of colors in an image\nSIFT (Scale-Invariant Feature Transform): Detects and describes local features in images\nHOG (Histogram of Oriented Gradients): Counts occurrences of gradient orientations in localized portions of an image\nHaar-like features: Used in face detection, these features look at rectangular regions and sum up pixel intensities\n\nAfter applying these filters, you would typically:\n\nExtract the resulting features\nPossibly apply dimensionality reduction techniques like PCA\nFeed the processed features into a classifier such as SVM, Random Forest, or a simple neural network\n\nThis approach, while effective for certain tasks, had limitations: - Handcrafted features might not capture all relevant information - The process was often computationally expensive - Feature engineering required domain expertise\nCNNs addressed these issues by learning the filters automatically during training, leading to more effective and adaptable vision models. Let us look at all the important components of a CNN in the next section."
  },
  {
    "objectID": "posts/cnns/00_cnn_basics.html#convolutional-layer",
    "href": "posts/cnns/00_cnn_basics.html#convolutional-layer",
    "title": "Intro to Convolution Neural Networks",
    "section": "Convolutional layer",
    "text": "Convolutional layer\nWe now understand why handcrafted fatures are hard to come up with. Lets back up for a second and remember our old friends feed forward (FF) networks. What is the problem with using feed forward networks to process images? Specifically why dont we just give the FF network the raw pixels and let it learn?\nAs it turns out, there are quite a few things:\nSuppose we have a (224,224,3) sized input-image. Thats a total of 224*224*3 = 150,528 data-points. suppose i want to have a output vector of length 1000 (classifier with 1k classes), this gives me a total of roughly 150k * 1k = 150 million parameters, where we have 1k parameters for each input pixel! That is a LOTT of parameters.\nSo how can we use the inherent nature of images and image-data to construct better models? We also learned convolutions in the previous section. Maybe that gives you some hint?\nPixel data is inherently very locally dependent. This is very different from tabular data, for example, a housing price dataset where each feature is largely independent of each other. By design feedforward networks do not allow for this kind of information to be shared between different data points in the network. This is beacause for each linear layer, a output shaped weight matrix is learned for each point in the input. This seems very inefficient.\nWhat if instead of treating each individual pixel as a different input, we treat a patch of pixels as a single input? This means we get one output item for a patch of pixels, and our weights can “slide” across an image to produce our output. And if that sounds like something we were doing earlier, you are right!\nEnter convolution. The weights will basically be filters we learn. And since the same filter is applied to all the patches in a conv operation, there are a lot less pararmeters to learn.\nWe can learn a lot of filters that will interact with our image and get us some useful features which can then be fed to a FF network.\nA filter always has the same number of dimensions as the input. Images are 3 dimensional tensors \\((H_{in}, W_{in}, C_{in})\\). The kernel will then be a tensor of size \\(k,k, C_{in}\\). A single step in their convolution that produces 1 output is the dot product between the kernel and a (\\(k,k\\)) window of the input image. Hence the output is a 2d matrix.\nBut we learn a lot of these filters, say \\(C_{out}\\). So we get \\(C_{out}\\) such 2d, matrices. So if we consider\n\ninput block shape -&gt; \\((H_{in}, W_{in}, C_{in})\\)\nfilter shape -&gt; \\(k, k, C_{in}\\)\nnumber of filters -&gt; \\(C_{out}\\)\nThe outputs will be a 3d tensor of shape \\((H_{out}, W_{out}, C_{out})\\), where \\((H_{out}, W_{out}\\) are determined by input resolution and the padding & stride of the convoution operation.\n\n\n\n\n\nThe number of parameters learned here would be \\((k*k*C_{in}*C_{out})\\). So for a conv layer with 64 filters of size (3x3) for an input image of size (224x224x3), we would have 1792 parameters, which is orders of magnitutde smaller already. Much more efficient isn’t it?\n\n\nLet us visualize the shapes of the input and output of a convolutional layer. (ignoring batch dimension for simplicity)\n\n\n\nconvolution in CNNs\n\n\nLet’s break down the convolution operation. Key Parameters are:\nInputs are of shape \\((H_{in}, W_{in}, C_{in})\\)\nConv layer parameters:\n\nKernel size: \\(k\\)\nNumber of output channels: \\(C_{out}\\)\nStride (\\(S\\)): Doubling the stride will reduce output size by half.\nPadding (\\(P\\)): Extra zeros added around the input image edges. Image can be padded with other values too. padding is done on both sides of the height and width dimensions.\nDilation (\\(D\\)): a parameter that controls the stride of elements in the window. default=1.\n\nOutputs are of shape \\((H_{out}, W_{out}, C_{out})\\), where\n\n\\(H_{out} = (H_{in} - k + 2P) / S + 1\\)\n\\(W_{out} = (W_{in} - k + 2P) / S + 1\\)\n\nFor a visual understanding of how all these parameters interact, check out this convolution visualizer.\n\n\n\nimg2col and col2img\nReaders would likely also be interested in knowing how the convolution operation is implemented as a matrix multiplication in GPUs. We have here the concepts of img2col and col2img.\n\nimg2col Transformation\n\n\nimg2col Formula: The img2col transformation converts the input image into a matrix \\(\\mathbf{X}\\) where each column corresponds to a flattened patch of the image that the kernel would convolve over:\n\\[\\mathbf{X} \\in \\mathbb{R}^{(k_H \\cdot k_W \\cdot C) \\times (H' \\cdot W')}\\]\nHere: - \\(H' = \\frac{H - k_H}{s} + 1\\) and \\(W' = \\frac{W - k_W}{s} + 1\\) are the height and width of the output feature map, where \\(s\\) is the stride. - Each column of \\(\\mathbf{X}\\) represents the flattened \\(k_H \\times k_W \\times C\\) patch from the input image.\n\nKernel Reshaping\n\nThe convolutional kernel \\(\\mathbf{K}\\) is reshaped into a matrix \\(\\mathbf{W}\\) to facilitate matrix multiplication:\n\\[\\mathbf{W} \\in \\mathbb{R}^{M \\times (k_H \\cdot k_W \\cdot C)}\\]\nHere: - \\(M\\) is the number of output channels. - Each row of \\(\\mathbf{W}\\) corresponds to one flattened convolutional filter.\n\nMatrix Multiplication\n\nThe convolution operation can now be performed as a matrix multiplication:\n\\[\\mathbf{Y} = \\mathbf{W} \\cdot \\mathbf{X}\\]\nWhere: - \\(\\mathbf{Y} \\in \\mathbb{R}^{M \\times (H' \\cdot W')}\\) is the output matrix.\n\ncol2img Transformation\n\nAfter the matrix multiplication, the output matrix \\(\\mathbf{Y}\\) is reshaped back into the spatial dimensions of the output feature map using the col2img operation:\n\\[\\mathbf{O} \\in \\mathbb{R}^{H' \\times W' \\times M}\\]\nThis reshaping restores the 2D spatial structure of the output feature map.\nThese transformations enable the convolution operation to leverage efficient matrix multiplication on GPUs, significantly speeding up the process."
  },
  {
    "objectID": "posts/cnns/00_cnn_basics.html#other-layers",
    "href": "posts/cnns/00_cnn_basics.html#other-layers",
    "title": "Intro to Convolution Neural Networks",
    "section": "Other layers",
    "text": "Other layers\nConolutional layers are usually followed by activation functions like ReLU and then a pooling layer.\n\nPooling layer\nPooling layer? Yes, pooling layer. Pooling layer is a means to perform downsampling of an image. Given a volume of input, instead of using filters, we can use non-parameterized operations like min/max/avg to downsample the data and get a smaller volume. It reduces the H and W dimensions of the input volume, keeping the depth the same. But why would we do something like that? Two main reasons:\n\nReduce the number of parameters: By reducing the number of parameters, the model can learn more general features.\nReduce the amount of overfitting: Since pooling leads to some information loss, it also acts as a good regularizer.\n\nThe entire operation is the same as the convolutional layer except instead of convolving \\(f\\) with a filter \\(g\\) of size \\((k,k, C_{in})\\), we take the max/mean of \\(f\\) for each patch of size \\((k,k)\\). This is done separately for each input channel.\nLet’s look at an example of max pooling, which is the most common type of pooling:\n\n\nCode\n# Input feature map\ninput = np.array([\n[1, 3, 2, 1],\n[5, 6, 4, 2],\n[7, 8, 9, 4],\n[1, 2, 3, 5]\n])\n\n# Max pooling with 2x2 filter and stride 2\ndef max_pool(input, filter_size, stride):\n    height, width = input.shape\n    output_height = (height - filter_size) // stride + 1\n    output_width = (width - filter_size) // stride + 1\n    output = np.zeros((output_height, output_width))\n    \n    for i in range(output_height):\n        for j in range(output_width):\n            start_i = i * stride\n            start_j = j * stride\n            window = input[start_i:start_i+filter_size, start_j:start_j+filter_size]\n            output[i, j] = np.max(window)\n    \n    return output\n\nresult = max_pool(input, 2, 2)\nprint(\"Input:\")\nprint(input)\nprint(\"\\nAfter 2x2 Max Pooling:\")\nprint(f\"\"\"Result: \n[[ max(f[0:2, 0:2]) max(f[0:2, 2:4])]\n [ max(f[2:4, 0:2]) max(f[2:4, 2:4])]]\n\"\"\")\nprint(result)\n\n\nInput:\n[[1 3 2 1]\n [5 6 4 2]\n [7 8 9 4]\n [1 2 3 5]]\n\nAfter 2x2 Max Pooling:\nResult: \n[[ max(f[0:2, 0:2]) max(f[0:2, 2:4])]\n [ max(f[2:4, 0:2]) max(f[2:4, 2:4])]]\n\n[[6. 4.]\n [8. 9.]]\n\n\nAs you can see, max pooling takes the maximum value in each 2x2 region, effectively reducing the spatial dimensions of the feature map while retaining the most prominent features. Generally a stride of 2 is used, which means that pooling results in half the resolution as the input (h,w).\n\n\nActivation layer\nActivations used are sigmoid, ReLu, and tanh. Since VGGs and deeper networks use ReLu, it is common to see ReLu used in CNNs. This is because ReLu partially solves the vanishing gradient problem, which is a common problem in deep networks. Read more about this here.\nBut what the ReLU layer does is very simple, it applies max(0, x) to each element in the input. Effectively, it zeros out negative values, and leaves positive values unchanged.\n\n\nOther jargon\nBefore we dive into training a model, let’s review some important jargon related to CNNs:\n\nactivation maps: the output when a \\[Conv-&gt;Relu\\] filter interacts with entire image.\nfeature maps: the output of a convolutional layer. (before the activation)\ndepth: refers to the number of channels.\nreceptive field: the area of the input that a given filter is able to see at a time is called the receptive field. If a filter is of size (\\(k\\)) its receptive field is of size (\\(k*k\\))\n\n\nNow we have all the basic components we need for a tiny cnn model."
  },
  {
    "objectID": "posts/cnns/00_cnn_basics.html#a-simple-cnn-architecture",
    "href": "posts/cnns/00_cnn_basics.html#a-simple-cnn-architecture",
    "title": "Intro to Convolution Neural Networks",
    "section": "A Simple CNN architecture",
    "text": "A Simple CNN architecture\nLets assume a simple task of image classification. The most common form of a CNN architecture stacks a few CONV-RELU layers, follows them with POOL layers, and repeats this pattern until the image volume has reduced to a small size. As the number of the layers increase, the resolution becomes smaller and smaller and depth of the activation maps increases.\nAt some point, it is common to transition to fully-connected(FC) layers. The final activation maps of the conv layers is pooled and flattened to a 2d matrix, ie \\((B,C,H,W)-&gt;(B,C*H*W)\\). The result then goes to the FC layers. The last fully-connected layer gives the output, such as the class scores. In other words, the most common ConvNet architectures follow the pattern:\nINPUT -&gt; [[CONV -&gt; RELU]*N -&gt; POOL?]*M -&gt;flatten-&gt; [FC -&gt; RELU]*K -&gt; FC\nNote that since architectures like Resnet and Inception emerged, this is not the case, and the CNNs feature more intricate and different connectivity structures.\n\nCNN characteristics\nWhile we are here, let us also take note of some characteristics of CNNs:\n\nSparse connectivity: CNNs focus on local patterns in data, particularly useful for spatial data like images. A single patch in feature map is connected to only a small patch of image (in MLPs there is dense/full connection).\nParameter sharing: the same kernel/filter slides across the image. ie different neurons in each activation map is calculated using the same filter. In MLPs each neuron in the output space is calculated using different weight values. this makes it efficient for computation.\nSpatial hierarchy: CNNs build a hierarchy of increasingly abstract features. Lower layers detect simple features (e.g., edges), while deeper layers combine these to detect more complex patterns.\nTranslation invariance: CNNs can recognize patterns regardless of their position in the input. This is because we are using filters that slide over patches of data, so information is processed in the same way for different patches of data This is crucial for tasks like object recognition in images.\n\nHave a look at this wonderful paper that dives deep into visualizing and understanding Cnns.\n\n\n\n\n\n\nFigure 1: visualizing activation maps in cnns. From the paper ‘Visualizing and Understanding Convolutional Networks’\n\n\n\nNow lets build a CNN model step by step.\n\nConvReLU Layer\nThe first block will be CONV-&gt;RELU. In torch,\n\nimport torch \nfrom torch import nn\n\nclass ConvReLU(nn.Module):\n    \"\"\"\n    A single conv layer followed by batchnorm and activation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, activation=nn.ReLU(), stride=1, padding=1):\n        super().__init__()\n\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n        self.activation = activation\n        \n    def forward(self, x):\n        if self.activation is not None:\n            x = self.activation(self.conv(x))\n        else:\n            x = self.conv(x)\n\n        return x\n\nLets create the conv1 layer and visualize the input and output volumes. The input is a rgb image of size 224x224x3. We’ll adjust the padding and stride such that the resolution remains the same.\n\n\nCode\n\ninput_img = torch.rand(224, 224, 3)   \n \n# input images must have shape (C,H,W) this is usually managed by torch's to_tensor transform\ninput_img = torch.permute(input_img, (2, 0, 1))      \n\nconv_layer_1 = ConvReLU(3, 64, kernel_size=3, stride=1, padding=1) \nconv_layer_2 = ConvReLU(64, 128, kernel_size=3, stride=1, padding=1)\n\nprint(\"input volume = {}\".format(input_img.shape))\n\nprint(\"output volume after conv_layer_1 (num_channels 3-&gt;64) = {}\".format(conv_layer_1(input_img).shape))\nprint(\"output volume after conv_layer_2 (num_channels 64-&gt;128)= {}\".format(conv_layer_2(conv_layer_1(input_img)).shape))\n# print(\"output volume after max_pool = {}\".format(max_pool(conv_layer_2(conv_layer_1(input_img))).shape))\n\n\ninput volume = torch.Size([3, 224, 224])\noutput volume after conv_layer_1 = torch.Size([64, 224, 224])\noutput volume after conv_layer_2 = torch.Size([128, 224, 224])\n\n\n\n\nBlock of ConvReLU layers\n\nNow lets make a bigger block one that does this operation:\nBlock(n) = [conv_layer_1-&gt;[conv_layer_i]*(n-1)].\nThe feature extractor inn our CNN is going to look like this:\n[block -&gt; Max_pool -&gt; block -&gt; Max_pool -&gt; block -&gt; ...]\nEach block takes as input \\(C_{in}\\) and \\(C_{out}\\) and the number of layers(&gt;1). The first layer will always change \\(C_{in}\\) to \\(C_{out}\\) and the rest of the layers keep all the dimensions intact. Lets create a block class:\n\nclass Block(nn.Module):\n\n    def __init__(self, input_channels, out_channels, num_layers, activation=nn.ReLU()):\n        super().__init__()\n        \n        self.out_channels = input_channels*2 if out_channels is None else out_channels\n        \n        conv_0 = ConvReLU(input_channels, self.out_channels, kernel_size=3, stride=1, padding=1)\n        layers = [conv_0, *[ConvReLU(self.out_channels, self.out_channels, kernel_size=3, stride=1, padding=1) for _ in range(num_layers-1)]]\n\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.layers(x)\n\nLets analyse a sequence of [BLOCK-&gt;POOL-&gt;BLOCK].\n\n\nCode\n\nmax_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\nconv1_block = Block(input_channels=3, out_channels=64, num_layers=2)\nprint(f\"input_img volume = {input_img.shape}\\n\")\nx = conv1_block(input_img)\n\nprint(\"output volume after conv1_block (Channels: 3-&gt;64) = {}\".format(x.shape))\n\n# conv_block_2 has half the h,w (due to max pool right before it) and double the input channels. \nconv2_block = Block(input_channels=64, out_channels=128, num_layers=2)\nx = max_pool(x)\nprint(\"\\noutput volume after maxpool (halves H,W; preserves C) = {}\".format(x.shape))\nx = conv2_block(x)\n\nprint(\"\\noutput volume after conv2_block (Channels: 64-&gt;128) = {}\".format(x.shape))\n\n\ninput_img volume = torch.Size([3, 224, 224])\n\noutput volume after conv1_block (Channels: 3-&gt;64) = torch.Size([64, 224, 224])\n\noutput volume after maxpool (halves H,W; preserves C) = torch.Size([64, 112, 112])\n\noutput volume after conv2_block (Channels: 64-&gt;128) = torch.Size([128, 112, 112])\n\n\nFinally the model.\nWe will later use a small dataset of (28,28) grayscale images to train this model. Do not pay much heed to the parameters. They have been arbritarily chosen keeping in mind the input size and total model size.\n\n\nCode\nimport torch\nimport torch.nn as nn\n\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        # self.final_pool = nn.MaxPool2d(kernel_size=4, stride=4)\n\n        self.feature_extractor = nn.Sequential(\n            Block(1, 32, 2),\n            self.pool,\n            Block(32, 64, 2),\n            self.pool,\n            Block(64, 128, 2),\n            self.pool\n        )\n        self.classifier = nn.Sequential(\n            nn.Linear(128 * 3 * 3, 2048),\n            nn.ReLU(),\n            nn.Linear(2048, 10),\n        )\n        \n    def forward(self, x):\n\n        x = self.feature_extractor(x)\n        x = x.view(-1, 128 * 3 * 3)     # flattening the tensor to feed it to FC layer\n        x = self.classifier(x)\n        return x\n\n\nLets look at the model summary.\n\n\nCode\nfrom torchinfo import summary\nmodel = SimpleCNN()\n\ninput_img = torch.rand(1,28,28)\nsummary(model, input_data=input_img)\n\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nSimpleCNN                                [1, 10]                   --\n├─Sequential: 1-1                        [128, 3, 3]               --\n│    └─Block: 2-1                        [32, 28, 28]              --\n│    │    └─Sequential: 3-1              [32, 28, 28]              9,568\n│    └─Block: 2-9                        --                        (recursive)\n│    │    └─Sequential: 3-8              --                        (recursive)\n│    └─Block: 2-3                        --                        (recursive)\n│    │    └─Sequential: 3-3              --                        (recursive)\n│    └─Block: 2-9                        --                        (recursive)\n│    │    └─Sequential: 3-8              --                        (recursive)\n│    └─MaxPool2d: 2-5                    [32, 14, 14]              --\n│    └─Block: 2-6                        [64, 14, 14]              --\n│    │    └─Sequential: 3-5              [64, 14, 14]              55,424\n│    └─Block: 2-9                        --                        (recursive)\n│    │    └─Sequential: 3-8              --                        (recursive)\n│    └─Block: 2-8                        --                        (recursive)\n│    │    └─Sequential: 3-7              --                        (recursive)\n│    └─Block: 2-9                        --                        (recursive)\n│    │    └─Sequential: 3-8              --                        (recursive)\n│    └─MaxPool2d: 2-10                   [64, 7, 7]                --\n│    └─Block: 2-11                       [128, 7, 7]               --\n│    │    └─Sequential: 3-9              [128, 7, 7]               221,440\n│    └─MaxPool2d: 2-12                   [128, 3, 3]               --\n├─Sequential: 1-2                        [1, 10]                   --\n│    └─Linear: 2-13                      [1, 2048]                 2,361,344\n│    └─ReLU: 2-14                        [1, 2048]                 --\n│    └─Linear: 2-15                      [1, 10]                   20,490\n==========================================================================================\nTotal params: 2,714,442\nTrainable params: 2,714,442\nNon-trainable params: 0\nTotal mult-adds (M): 259.02\n==========================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.72\nParams size (MB): 10.67\nEstimated Total Size (MB): 11.40\n==========================================================================================\n\n\nNote that the model has \\(\\approx\\) 2.7 million parameters and a total size of around 11 MB.\nModel precision refers to the number of bits used to represent each weight or activation. Since our model weights are of dtype float32, the precision is 32 bits. Thus, the total size of a model can be calculated using the formula: total size \\(\\approx\\) (number of parameters * precision) / 8.\nAlso, note that the FC layers have most of the parameters (over 80%). In general conv layers are more memory heavy (input size is large there) while the FC layers have large number of parameters."
  },
  {
    "objectID": "posts/cnns/00_cnn_basics.html#lets-train-a-model",
    "href": "posts/cnns/00_cnn_basics.html#lets-train-a-model",
    "title": "Intro to Convolution Neural Networks",
    "section": "Lets train a model",
    "text": "Lets train a model\nLet us quickly now train a model to classify images from the fashion mnist dataset. The dataset contains very small grayscale (ie single channel) images of size (28*28).\n\nthe below code is all generated by claude-sonnet-3.5, bcuz its kinda boring to train a toy model on a toy dataset, that too for classification. dont worry though, ill soon be back with a more interesting vision problem to get our hands dirty.\n\nTraining itself is pretty straightforward. We split the data into train-test, and then train the model for a few epochs. We use the Adam optimizer and CrossEntropyLoss as the loss function. The outputs of the model are bare logits, which the loss functions accepts with targets. A learning rate of 0.001 is used for the optimizer. In the end we test on the test set, and print the accuracy.\n\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_built() else \"cpu\")\nprint(f\"device: {device}\")\n \n\n# Load and preprocess the Fashion MNIST dataset (just plain old standardization)\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n\ntrain_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\ntest_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\nmodel = SimpleCNN()\nmodel.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nnum_epochs = 5\nall_losses = []\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_losses = []\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        data, targets = data.to(device), targets.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(data)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        epoch_losses.append(loss.item())\n    \n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {np.mean(epoch_losses):.4f}\")\n    all_losses.extend(epoch_losses)\n\n# plot train loss curve\nplt.figure(figsize=(10, 5))\nplt.plot(all_losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss Curve')\nplt.show()\n\n# Evaluation\nmodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for data, targets in test_loader:\n        data, targets = data.to(device), targets.to(device)\n        outputs = model(data)\n        _, predicted = torch.max(outputs.data, 1)\n        total += targets.size(0)\n        correct += (predicted == targets).sum().item()\naccuracy = 100 * correct / total\nprint(f'Test Accuracy:{accuracy:.2f} %')\n\ndevice: mps\nEpoch [1/5], Loss: 0.4570\nEpoch [2/5], Loss: 0.2674\nEpoch [3/5], Loss: 0.2233\nEpoch [4/5], Loss: 0.1939\nEpoch [5/5], Loss: 0.1718\n\n\n\n\n\n\n\n\n\nTest Accuracy:91.85 %\n\n\nOver 91% accuracy. Neat!! Now let’s visualize some of the predictions.\n\nimport random\n\nclass_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']   # verify using train_dataset.classes\n\nmodel.eval()\nfig, axes = plt.subplots(2, 5, figsize=(15, 6))\nfig.suptitle('Sample Predictions', fontsize=16)\n\nwith torch.no_grad():\n    for i, idx in enumerate(random.sample(range(len(test_dataset)), 10)):\n        image, label = test_dataset[idx]\n        output = model(image.unsqueeze(0).to(device))\n        predicted = output.argmax(1).item()\n        \n        ax = axes[i // 5, i % 5]\n        ax.imshow(image.squeeze(), cmap='gray')\n        ax.axis('off')\n        ax.set_title(f'Pred: {class_names[predicted]}\\nTrue: {class_names[label]}', \n                     color='green' if predicted == label else 'red')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nNow let us visualize the confisuion matrix on the test set to analyse errors.\n\n\nCode\n# see the confusion matrix\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\n\n# Collect predictions and true labels\ny_true, y_pred = [], []\nmodel.eval()\nwith torch.no_grad():\n    for data, targets in test_loader:\n        data, targets = data.to(device), targets.to(device)\n        outputs = model(data)\n        _, predicted = torch.max(outputs.data, 1)\n        y_true.extend(targets.cpu().numpy())\n        y_pred.extend(predicted.cpu().numpy())\n\n# Create and plot confusion matrix\ncm = confusion_matrix(y_true, y_pred)\nplt.figure(figsize=(10, 8))\nplt.imshow(cm, interpolation='nearest', cmap=\"Pastel1\")\nplt.title('Confusion Matrix')\nplt.colorbar()\ntick_marks = range(len(class_names))\nplt.xticks(tick_marks, class_names, rotation=45)\nplt.yticks(tick_marks, class_names)\nplt.xlabel('Predicted')\nplt.ylabel('True')\n\n# Add text annotations\nfor i in range(cm.shape[0]):\n    for j in range(cm.shape[1]):\n        plt.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can see that the model is particularly confused in the “Tshirt” & “Shirt” class (less than 70% accuracy) and most confident in “Trouser”, “Sneaker”, “Bag” and “Ankle boot” (greater than 95% accuracy)"
  },
  {
    "objectID": "posts/cnns/00_cnn_basics.html#concluding",
    "href": "posts/cnns/00_cnn_basics.html#concluding",
    "title": "Intro to Convolution Neural Networks",
    "section": "Concluding",
    "text": "Concluding\nI hope you clearly understand the core CNN fundamental architecture. This is just the base that’ll act as a foundation for more interesting architectures as we go on. Until then, see you in the next one!\nByeee :)"
  },
  {
    "objectID": "posts/cnns/00_cnn_basics.html#further-reading",
    "href": "posts/cnns/00_cnn_basics.html#further-reading",
    "title": "Intro to Convolution Neural Networks",
    "section": "Further Reading",
    "text": "Further Reading\n\nSebastian Raschka Intorduction to CNNs course here\nCS231N Cnn notes here. Great intuition and more detail about the shapes, local connectivity, spatial arrangement, and loads of other stuff.\nA beginner friendly article on Medium (its a great blog-series for ML) here\nVisualizing and Understanding Convolutional Networks. 2014 paper here\nFind out some of the modifications made on this basic architecture here"
  },
  {
    "objectID": "posts/few-shot-learning/prototype_networks.html",
    "href": "posts/few-shot-learning/prototype_networks.html",
    "title": "Few Shot learning: Classify using few examples!",
    "section": "",
    "text": "If you aren’t already familiar with it, few-shot learning (FSL) is an exciting concept in machine learning that focuses on training models to make accurate predictions or classifications when provided with only a very limited amount of labeled data for each category or class. Unlike traditional machine learning approaches that often require substantial labeled datasets for training, few-shot learning aims to address scenarios where acquiring such extensive labeled data is impractical or costly.\nIn few-shot learning, the model is trained to generalize effectively from a small number of examples (or shots) per category, allowing it to make predictions for new, unseen data points with confidence. This capability is particularly valuable in situations where data is scarce, and manual annotation is labor-intensive or expensive. Refer to [1] for an in-depth survey in this field.\nIn this blog, I’m going to show you how to implement a basic few-shot classification technique for text."
  },
  {
    "objectID": "posts/few-shot-learning/prototype_networks.html#introduction",
    "href": "posts/few-shot-learning/prototype_networks.html#introduction",
    "title": "Few Shot learning: Classify using few examples!",
    "section": "",
    "text": "If you aren’t already familiar with it, few-shot learning (FSL) is an exciting concept in machine learning that focuses on training models to make accurate predictions or classifications when provided with only a very limited amount of labeled data for each category or class. Unlike traditional machine learning approaches that often require substantial labeled datasets for training, few-shot learning aims to address scenarios where acquiring such extensive labeled data is impractical or costly.\nIn few-shot learning, the model is trained to generalize effectively from a small number of examples (or shots) per category, allowing it to make predictions for new, unseen data points with confidence. This capability is particularly valuable in situations where data is scarce, and manual annotation is labor-intensive or expensive. Refer to [1] for an in-depth survey in this field.\nIn this blog, I’m going to show you how to implement a basic few-shot classification technique for text."
  },
  {
    "objectID": "posts/few-shot-learning/prototype_networks.html#terminology",
    "href": "posts/few-shot-learning/prototype_networks.html#terminology",
    "title": "Few Shot learning: Classify using few examples!",
    "section": "Terminology",
    "text": "Terminology\nBefore we begin, let us familiarize ourselves with the correct terminology.\nWhat characterizes FSL is having only a few examples at hand, for unseen classes, during inference. So basically we are showing the model only a few examples of a class which it may or may not have encountered during its pre-training before we make predictions using that model.\nSupport Set, 𝒮: The few annotated examples that we have, make up the support set, with which we may or may not update the model weights to make it generalize to the new classes.\nQuery Set, 𝒬: The query set consists of our test set, i.e. the samples we want to classify using the base model and a support set.\nN-way K-shot learning scheme: This is a common phrase used in the FSL literature, which essentially describes the few-shot problem statement that a model will be dealing with. “N” is the number of classes we have at test time and “K” is the number of samples per class we have in our support set “𝒮”\n1-shot classification: When K=1, i.e. we have only one labeled sample available per class.\n0-shot classification: K=0, i.e. we do not have any labeled samples available during inference.\nLet us have a look at an example.\n\n\nCode\n# sample set is 3-way, 3-shot.\nclasses = ['camera', 'battery', 'display']\n\nsample_set = {\n    'camera': [\n        'absolutely love this quality of my photos!!',\n        'it even gives great quality in dim lighting. fabulous!!',\n        'the camera should be much better for such a high price'\n    ],\n    'battery': [\n        \"The battery life on this device is exceptional! It easily lasts me the entire day with heavy usage.\",\n        \"I'm a bit disappointed with the battery performance. It drains quite quickly, especially when using power-hungry apps.\",\n        \"The battery is decent, not too bad, not too good. It gets me through the day, but I was hoping for better longevity.\"\n    ],\n    'display': [\n        \"The display on this device is stunning! Colors are vivid, and the resolution is top-notch.\",\n        \"I'm not too impressed with the display quality. It seems a bit washed out, and the brightness could be better.\",\n        \"The display is okay, but nothing extraordinary. It gets the job done for everyday tasks.\"\n    ]\n}\n\nquery_set = [\"i hate the batteries\", \"does it give good quality photos in the night?\"]\n\n\nHere we have a 3-way (there are 3 classes), 3-shot (3 examples for each class) setting."
  },
  {
    "objectID": "posts/few-shot-learning/prototype_networks.html#high-level-design",
    "href": "posts/few-shot-learning/prototype_networks.html#high-level-design",
    "title": "Few Shot learning: Classify using few examples!",
    "section": "High level design",
    "text": "High level design\nLet us have a quick look at the architecture of the system.\n\n\n\nA simple few shot classification system\n\n\nThis is the flow of our solution:\nThe first step is to get an embedding module. That can be created using regular supervised learning (Resnets trained on Imagenet) or self-supervised learning (BERT and co). Then, we use the embedding module to get feature representations for our classes in the support set. A simple way to do this is to turn each class’s examples into embeddings and take the mean of those vectors. This then becomes our “prototype” vectors to compare against. Now for each query, we can take the embeddings of the query text and use cosine similarity to find the predicted class. This closely resembles This system basically allows us to leverage transfer learning to use large backbones as our embedding module. And there is also the advantage of not performing any gradient updates. This helps us maintain a much more dynamic and flexible system.\nThe idea of comparing query samples with the support set samples is inspired by metric learning. Refer to [3, 4] for better understanding.\nLet’s implement this using the transformers library. You can find the implementation in this colab notebook."
  },
  {
    "objectID": "posts/few-shot-learning/prototype_networks.html#implementation",
    "href": "posts/few-shot-learning/prototype_networks.html#implementation",
    "title": "Few Shot learning: Classify using few examples!",
    "section": "Implementation",
    "text": "Implementation\nLet’s start with the good old BERT base model.\n\n1. Import libraries and download model\n\n\nCode\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport numpy as np\nfrom typing import Dict\nfrom pprint import pprint\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\nif torch.backends.mps.is_available():\n    device = torch.device('mps')\nelse:\n    device = torch.device('cpu')\n    \n# load the model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\nmodel = AutoModel.from_pretrained(\"bert-base-uncased\")\n\n\n\n\n2. Tokenize and encode a sentence\n\n\nCode\ntext = \"He's such a great guy!!\"\nencoded_input = tokenizer(\n  text, \n  return_tensors='pt', \n  padding='max_length',     # True will pad to max-len in batch\n  max_length=32\n)\nprint(f\"encoded input:\")\npprint(encoded_input)\n\n\nencoded input:\n{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]]),\n 'input_ids': tensor([[ 101, 2002, 1005, 1055, 2107, 1037, 2307, 3124,  999,  999,  102,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0]]),\n 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]])}\n\n\nwhere,\n\ninput_ids: token id of each token\ntoken_type_id: When we pass two sentences for downstream fine-tuning in BERT, this is used to identify which token belongs to which sentence.\nattention_mask: which tokens to ignore. As you’ll see, the padding tokens have been masked.\n\n\n\n3. Generate embeddings using model\nThe output has 2 parts, cls_token_embeddings and last_hidden_states of the tokens. We can either use the cls_embeddings to represent the sentence or pool the vectors in last_hidden_states. The pooling can be max/min/mean.\nThe dimension of the output will be equal to the embedding dimension of the model, i.e. 784 in our case.\n\n\nCode\ndef get_embeddings(model, tokenizer, text, pooling='mean'):\n  \n  encoded_input = tokenizer(\n    text, \n    return_tensors='pt', \n    padding='max_length', \n    max_length=16, \n    truncation=True\n  )\n  encoded_input = encoded_input.to(device)\n\n  model.to(device)\n\n  model.eval()\n  with torch.no_grad():\n    output = model(**encoded_input)\n    last_hidden_state, pooler_output = output[0], output[1]\n    \n    if pooling=='cls':\n      embedding = pooler_output\n    else:\n      # ignore the pad tokens embeddings by multiplying with attention mask\n      last_hidden_state = (last_hidden_state * encoded_input['attention_mask'].unsqueeze(2))\n      embedding = last_hidden_state.mean(dim=-2)\n  return np.array(embedding.cpu())\n\n\nembeddings = get_embeddings(model, tokenizer, 'hey there! how are you?')\nprint(f\"shape of embeddings: {embeddings.shape}\")\n\n\nshape of embeddings: (1, 768)\n\n\n\n\n4. Prepare the prototypes:\nTo prepare the class prototypes we’ll take the mean of the sentences for each class.\n\n\nCode\ndef make_prototypes(model, tokenizer, sample_set: Dict):\n  prototype_vectors = dict()\n  sentence_embeddings = dict()\n  for category, sentences in sample_set.items():\n    sentence_embeds = get_embeddings(model, tokenizer, sentences)\n    sentence_embeddings[category] = sentence_embeds\n    prototype_vectors[category] = np.mean(sentence_embeddings[category], axis=0)\n  return prototype_vectors\n\n\n\n\n5. Classify\nTo classify a query text, we can run cosine similarity against the prototype vectors and return the argmax as the most probable class!\n\n\nCode\ndef classify(model, tokenizer, text, prototype_vectors=None, sample_set=None):\n  if prototype_vectors==None:\n      assert sample_set!=None, \"prototype vectors are not passed, either pass a sample set prototype vectors\"\n      prototype_vectors = make_prototypes(sample_set)\n\n  query_embeddings = get_embeddings(model, tokenizer, text)\n  \n  prototype_matrix = np.stack(list(prototype_vectors.values()))\n  scores = sentence_transformers.util.cos_sim(query_embeddings, prototype_matrix)\n  return scores\n\n\nUsing the above-defined functions and the sample set from before, we have:\n\n\nCode\nprototype_vectors = make_prototypes(model, tokenizer, sample_set)\nquery_text = \"i hate the batteries\"\noutput = classify(model, tokenizer, query_text, prototype_vectors=prototype_vectors)\n\nprint(f\"output: {output}\")\nprint(f\"the predicted class is {classes[output.argmax().item()]}\")\n\n\noutput: tensor([[0.6121, 0.7127, 0.6388]])\nthe predicted class is battery\n\n\nA bit strange! Although the expected class is predicted, scores for other classes are also high. Let’s try a harder query.\n\n\nCode\nquery = ['does it give good quality photos in the night?']\noutput = classify(model, tokenizer, query, prototype_vectors=prototype_vectors)\n\nprint(f\"output: {output}\")\nprint(f\"the predicted class is {classes[output.argmax().item()]}\")\n\n\noutput: tensor([[0.7984, 0.7043, 0.7647]])\nthe predicted class is camera\n\n\nAlthough the highest similarity is for ‘camera’, the similarity should be much higher.\nThe results do not get better even if we try cls-pooling. This only means that the embeddings produced by the model do not give us an accurate representation of the sentence.\nWe would then do good to remember that BERT pre-train was trained by MaskedLM, NextSentencePrediction, hence the original purpose of BERT is not to create a meaningful embedding of the sentence but for some specific downstream task. In fact, as the authors of the sentence-transformer paper [2] point out, out-of-the-box Bert embeddings perform even worse than GLoVE representations!\n\nJacob Devlin’s comment: I’m not sure what these vectors are, since BERT does not generate meaningful sentence vectors. It seems that this is is doing average pooling over the word tokens to get a sentence vector, but we never suggested that this will generate meaningful sentence representations. And even if they are decent representations when fed into a DNN trained for a downstream task, it doesn’t mean that they will be meaningful in terms of cosine distance. (Since cosine distance is a linear space where all dimensions are weighted equally).\n\nThere are a few ways to improve the bert-base for sentence-level tasks and both involve finetuning the model with some data.\n\nadding a linear layer on top and fine-tuning it.\nmaking embeddings better by contrastive learning."
  },
  {
    "objectID": "posts/few-shot-learning/prototype_networks.html#using-sentence-transformers",
    "href": "posts/few-shot-learning/prototype_networks.html#using-sentence-transformers",
    "title": "Few Shot learning: Classify using few examples!",
    "section": "Using sentence transformers",
    "text": "Using sentence transformers\nUltimately, what we need is a better embedding module. Luckily we have such models. As it turns out, contrastive learning is an excellent approach for tuning our models such that different sentences produce semantically different embeddings.\nWe will explore contrastive learning and its inner workings some other day, but for now, let’s pick up open-source models that have been finetuned using contrastive learning. There is an entire library (aka sentence-transformers) and paper[2] dedicated to this.\nWe’ll use the sentence-transformers/stsb-bert-base model for our purposes.\n\n1. Import packages and download model\n\n\nCode\nfrom sentence_transformers import SentenceTransformer\n\n# load a sentence transformer model\nsts_model = SentenceTransformer('sentence-transformers/stsb-bert-base')\nmodel2 = sts_model[0].auto_model.to(device)\ntokenizer2 = sts_model[0].tokenizer\n\n\n\n\n2. Use the above-defined functions to prepare prototype vectors and classify them in a few-shot setting\n\n\nCode\nprototype_vectors = make_prototypes(model2, tokenizer2, sample_set)\nquery_text = \"i hate the batteries\"\noutput = classify(model2, tokenizer2, query_text, prototype_vectors)\n\nprint(f\"output: {output}\")\nprint(f\"the predicted class is {classes[output.argmax().item()]}\")\n\n\noutput: tensor([[0.0910, 0.4780, 0.1606]])\nthe predicted class is battery\n\n\n\n\nCode\n\nquery = ['does it give good quality photos in the night?']\noutput = classify(model2, tokenizer2, query, prototype_vectors=prototype_vectors)\n\nprint(f\"output: {output}\")\nprint(f\"the predicted class is {classes[output.argmax().item()]}\")\n\n\noutput: tensor([[0.4467, 0.1012, 0.2998]])\nthe predicted class is camera\n\n\nAs we can see, the scores seem much more reasonable this time around. There is a much better correlation with the ground truth labels. Using better base models trained in multiple tasks further improves the performance of these models."
  },
  {
    "objectID": "posts/few-shot-learning/prototype_networks.html#conclusion",
    "href": "posts/few-shot-learning/prototype_networks.html#conclusion",
    "title": "Few Shot learning: Classify using few examples!",
    "section": "Conclusion",
    "text": "Conclusion\nThis brings us to the end of this blog. In summary, we explored the realm of few-shot learning, a machine-learning approach tailored for accurate predictions with limited labeled data. Initially, we employed BERT, but its design didn’t align with our objectives. Instead, we leveraged a model fine-tuned for sentence-level tasks, sentence-transformers/stsb-bert-base, which significantly improved our results.\nThese are a few things to note:\nAlthough we directly used pre-trained models here, an interesting undertaking would be to perform the contrastive fine-tuning ourselves. Also, instead of using cosine similarity, we can train lightweight classifiers on top of our embedding module for better performance.\nThat’ll be all from my side. Until next time, Happy Reading!"
  },
  {
    "objectID": "posts/few-shot-learning/prototype_networks.html#references",
    "href": "posts/few-shot-learning/prototype_networks.html#references",
    "title": "Few Shot learning: Classify using few examples!",
    "section": "References",
    "text": "References\n[1] Survey paper on few-shot learning\n[2] Sentence-Bert paper\n[3] Prototypical Networks\n[4] Excellent much more techincal blog by Lilian Weng"
  },
  {
    "objectID": "forest.html",
    "href": "forest.html",
    "title": "forest",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts/floyd_warshall.html",
    "href": "posts/floyd_warshall.html",
    "title": "The Floyd-Warshall Algorithm",
    "section": "",
    "text": "Lets have a quick look at a basic graph algorithm today:\nGiven a weighted graph, finding the shortest path between all pairs of nodes.\n\n# lets define a undirected graph (we'll assume no weights for now)\n\n# Show the edges matrix (assume A-F as indices 0-6 in the matrix)\nlabels = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F'}\n\n# adjacency list (nodes: [connected nodes])\nG = {\n    'A': ['B','C'],\n    'B': ['A','E','D'],\n    'C': ['A','F','D'],\n    'D': ['B','C'],\n    'E': ['F','B'],\n    'F': ['E','C']\n}\n\n# Convert the graph dictionary to an edge matrix\nedges_matrix = [[0 if labels[i] not in G[labels[j]] else 1 for j in range(len(labels))] for i in range(len(labels))]\n\nedges_matrix\n\n[[0, 1, 1, 0, 0, 0],\n [1, 0, 0, 1, 1, 0],\n [1, 0, 0, 1, 0, 1],\n [0, 1, 1, 0, 0, 0],\n [0, 1, 0, 0, 0, 1],\n [0, 0, 1, 0, 1, 0]]\n\n\n\n\nCode\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Convert the graph dictionary to a NetworkX graph to visualize\nnx_graph = nx.Graph()\nfor i in range(len(edges_matrix)):\n    for j in range(len(edges_matrix[i])):\n        if edges_matrix[i][j] == 1:\n            nx_graph.add_edge(i, j)\n\n# Draw the graph with white nodes\nnx.draw(nx_graph, with_labels=True, labels=labels, font_weight='bold', node_color='white')\nplt.show()\n\n\n\n\n\n\n\n\n\nLets code the algorithm.\nTo begin with, we’ll have a distances matrix with same shape as edge matrix. Each value there represents the distance bw its row_num (i) and col_num (j), aka its weights! If no edge, the distance is \\infty.\nWe’ll also print the distance matrix as a dataframe to visualize its node labels.\n\nimport math \nimport pandas as pd\n\ninf = math.inf\nprint_grid = lambda x: print(pd.DataFrame(distances, index=labels.values(), columns=labels.values()))\n\nenum = {1:1, 0:inf}\ndistances = [row[:] for row in edges_matrix]\nsize = len(edges_matrix)\n\nfor i in range(size):\n    for j in range(size):\n        distances[i][j] = enum[distances[i][j]]\n\nprint_grid(distances)\n\n     A    B    C    D    E    F\nA  inf  1.0  1.0  inf  inf  inf\nB  1.0  inf  inf  1.0  1.0  inf\nC  1.0  inf  inf  1.0  inf  1.0\nD  inf  1.0  1.0  inf  inf  inf\nE  inf  1.0  inf  inf  inf  1.0\nF  inf  inf  1.0  inf  1.0  inf\n\n\nNow we have distance between any 2 nodes i and j, which we can iterate over in a nested for-loop. Now lets perform another loop for N times (N = num_nodes). In each pass we’ll add a new node k into the mix and perform the following operation:\ndistances[i][j] = min(distances[i][j], distances[i][k] + distances[k][j])\nNote that (i, j, k) \\mid (i,j,k) \\in \\{0, 1, \\ldots, n-1\\}\nAt any k^{th} step, we have i, j and a total of k vertices. We then have 2 choices: Is the path bw i and j through k vertices shorter than the distance bw i and j through k-1 vertices.\n\nFor any 2 nodes i \\& j does adding the k_{th} node to its path reduce the present minimum distance? The present minimum distance by the way, is the shortest path between nodes i \\& j considering all possible combinations with k-1 previous nodes.\n\nThis way by the time we reach the last node, we have a matrix that shows the shortest distance between any 2 nodes considering all available nodes. Beautiful isn’t it! Without the DP this would just look 1 hop deep into the network for each comparison.\nNow lets watch it in action for the graph we defined.\n\nvisited_nodes = []\nfor k in range(size):\n    for i in range(size):\n        for j in range(size):\n            distances[i][j] = min(distances[i][j], distances[i][k]+distances[k][j])\n            \n    # Print the distances matrix as a grid after every k\n    visited_nodes.append(labels[k])\n    print(f\"Pass {k+1}: with {visited_nodes}:\\n\")\n    print_grid(distances)\n    print(\"\\n=========\\n\")\n\nPass 1: with ['A']:\n\n     A    B    C    D    E    F\nA  inf  1.0  1.0  inf  inf  inf\nB  1.0  2.0  2.0  1.0  1.0  inf\nC  1.0  2.0  2.0  1.0  inf  1.0\nD  inf  1.0  1.0  inf  inf  inf\nE  inf  1.0  inf  inf  inf  1.0\nF  inf  inf  1.0  inf  1.0  inf\n\n=========\n\nPass 2: with ['A', 'B']:\n\n     A    B  C    D  E    F\nA  2.0  1.0  1  2.0  2  inf\nB  1.0  2.0  2  1.0  1  inf\nC  1.0  2.0  2  1.0  3  1.0\nD  2.0  1.0  1  2.0  2  inf\nE  2.0  1.0  3  2.0  2  1.0\nF  inf  inf  1  inf  1  inf\n\n=========\n\nPass 3: with ['A', 'B', 'C']:\n\n   A  B  C  D  E  F\nA  2  1  1  2  2  2\nB  1  2  2  1  1  3\nC  1  2  2  1  3  1\nD  2  1  1  2  2  2\nE  2  1  3  2  2  1\nF  2  3  1  2  1  2\n\n=========\n\nPass 4: with ['A', 'B', 'C', 'D']:\n\n   A  B  C  D  E  F\nA  2  1  1  2  2  2\nB  1  2  2  1  1  3\nC  1  2  2  1  3  1\nD  2  1  1  2  2  2\nE  2  1  3  2  2  1\nF  2  3  1  2  1  2\n\n=========\n\nPass 5: with ['A', 'B', 'C', 'D', 'E']:\n\n   A  B  C  D  E  F\nA  2  1  1  2  2  2\nB  1  2  2  1  1  2\nC  1  2  2  1  3  1\nD  2  1  1  2  2  2\nE  2  1  3  2  2  1\nF  2  2  1  2  1  2\n\n=========\n\nPass 6: with ['A', 'B', 'C', 'D', 'E', 'F']:\n\n   A  B  C  D  E  F\nA  2  1  1  2  2  2\nB  1  2  2  1  1  2\nC  1  2  2  1  2  1\nD  2  1  1  2  2  2\nE  2  1  2  2  2  1\nF  2  2  1  2  1  2\n\n=========\n\n\n\nLets traverse this evolution of the distance value for nodes F and D.\nLet potential_stops be the k-1 sets we have visited at the start of k^{th}.\nWe can see theres no direct path, so obviously the value is \\infty to begin with. Even with potential_stops=\\{A,B\\} we cant get to D. So the value after 2 passes is still \\infty.\nWhen we add C, we should get a path, and sure we do! we arrive at the answer directly which is 2 (ie F-&gt;C-&gt;D).\nWhy does that happen here? Its because for all nodes being added to the potential_stops, we are checking whether adding k^{th} node improves the shortest path among any 2 nodes! Hence at value of k, we know that if the shortest path exists in the given set, we have it already!\nSo the FW algorithm helps us find shortest distance between all node pairs in a weighted graph. The graph can be directed/undirected and its weights can be negative too."
  },
  {
    "objectID": "posts/floyd_warshall.html#time-complexity",
    "href": "posts/floyd_warshall.html#time-complexity",
    "title": "The Floyd-Warshall Algorithm",
    "section": "Time Complexity",
    "text": "Time Complexity\nT: O(N^3), where N is the number of vertices.\nLets assume a baseline of all-pairs-Dijkstra’s Algorithm:\n\nPurpose: Computes the shortest path from a single source to all other vertices in a weighted graph.\nDijkstra: To find the shortest paths between all pairs of vertices, you can run Dijkstra’s algorithm times (once for each vertex as the source). The Dijkstra algo bsaically just uses plain BFS to get the shortest paths.\nTime Complexity: O(E + N\\log N) for a single source, ie O(N(E + N\\log N)) for N nodes\n\nWhen for dense graohs E approaches N^2, T approaches O(N^3) and hence the algo is similar to FW, But when E&lt;&lt;N^2, ie sparse graphs: T-&gt;O(N^2 logN) -&gt; O(N^2)\nDijkstra’s algorithm assumes that all edge weights are non-negative. If your graph has negative weights, Dijkstra’s algorithm can give incorrect results.\nWhile FW can open up use-cases where negative weights are common, such as in routing algorithms, certain scheduling problems, or in algorithms for finding the transitive closure of a graph.\nThats it for this one. Hope you understood it well! Cya!"
  },
  {
    "objectID": "posts/cnns/01_resnet.html",
    "href": "posts/cnns/01_resnet.html",
    "title": "Training a Resnet in Pytorch",
    "section": "",
    "text": "In my last post Intro to CNNs, we discussed the basics of CNNs and how to train a simple CNN from scratch. In this post, we will train a ResNet in Pytorch.\nResnets came around right after VGGs, where the author explored the idea of training very deep networks for better performance, since deeper models could in thoery learn more complex features. But after a certain point, more depth didnt help with wither validation accuracy nor training performance. This was confusing, since we would assume a larger model can atleast replicate the performance of a smaller network and then some!\nMathematically, if we have a shallower network H(x) that performs well, a deeper network F(x) should be able to learn an identity mapping in its additional layers, effectively becoming:\nF(x) = H(x) + (F(x) - H(x))\nwhere (F(x) - H(x)) is the residual mapping. Instead of hoping that F(x) will learn the entire desired mapping, we can explicitly let it learn the residual mapping (F(x) - H(x)). This is the key idea behind ResNets: learning residual functions with reference to the layer inputs, rather than learning unreferenced functions.\nThis approach allows for the creation of very deep networks without the problem of vanishing gradients, as the gradient can flow directly through the skip connections (also known as shortcut connections) during backpropagation. We can imagine this as a graident highway that goes right through the network and then comes back to the input.\n\n\n\nThis is a resnet-architecture"
  },
  {
    "objectID": "posts/cnns/01_resnet.html#convrelu-layer",
    "href": "posts/cnns/01_resnet.html#convrelu-layer",
    "title": "Training a Resnet in Pytorch",
    "section": "ConvReLU Layer",
    "text": "ConvReLU Layer\nEach block contains some conv_layers followed by a max_pool layer. If the input to a block is (H, W, C), the output of the block is (H/2, W/2, C*2). All in all each block still preserves the total volume.\nLet us look inside a conv block, for eg the conv2 block in Figure 1. We can see that the particular block has a max_pool (red) layer and 2 single-conv layers(blue). Each single-conv layer within that block is a:\nconv_layer = [CNN-&gt;ReLU]\nIn torch,\n\nimport torch \nfrom torch import nn\n\nclass ConvReLU(nn.Module):\n    \"\"\"\n    A single conv layer followed by batchnorm and activation.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, activation=nn.ReLU(), stride=1, padding=1):\n        super().__init__()\n\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n        self.activation = activation\n        \n    def forward(self, x):\n        if self.activation is not None:\n            x = self.activation(self.conv(x))\n        else:\n            x = self.conv(x)\n\n        return x\n\nLets create the conv1 layer and visualize the input and output volumes. The input is a rgb image of size 224x224x3\n\ninput_img = torch.rand(224, 224, 3)   \n \n# input images must have shape (C,H,W) this is usually managed by torch's to_tensor transform\ninput_img = torch.permute(input_img, (2, 0, 1))      \n\n# conv layer params -&gt; C_in=3, C_out=64, K=3, S=1, P=1\nconv_layer_1 = ConvReLU(3, 64, kernel_size=3, stride=1, padding=1) \nconv_layer_2 = ConvReLU(64, 64, kernel_size=3, stride=1, padding=1)\n\n# max pool layer params -&gt; K=2, S=2\nmax_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\nprint(\"input volume = {}\\n\".format(input_img.shape))\nprint(\"output volume after conv_layer_1 = {}\".format(conv_layer_1(input_img).shape))\nprint(\"output volume after conv_layer_2 = {}\".format(conv_layer_2(conv_layer_1(input_img)).shape))\n# print(\"output volume after max_pool = {}\".format(max_pool(conv_layer_2(conv_layer_1(input_img))).shape))"
  },
  {
    "objectID": "posts/cnns/01_resnet.html#block-of-convrelu-layers",
    "href": "posts/cnns/01_resnet.html#block-of-convrelu-layers",
    "title": "Training a Resnet in Pytorch",
    "section": "Block of ConvReLU layers",
    "text": "Block of ConvReLU layers\nNow to make a block of VGG, we combine multiple such layers together. If you see the Figure 1, each block’s has a red layer in the beginning. The red layer is a max_pool layer with stride=2 and kernel_size=2. This results in the H,W being halved. Then the subsequent conv_layers preserve the H,W and double the channels. Lets look at a block below:\nBlock = [conv_layer_1-&gt;[conv_layer_i]*(n-1)].\nThe feature extractor will look like this:\n[block_1 -&gt; Max_pool -&gt; block_2 -&gt; Max_pool -&gt; block_3 -&gt; ...]\nLets create a block class:\n\nclass Block(nn.Module):\n\n    def __init__(self, input_channels, num_layers, out_channels, activation=nn.ReLU()):\n        super().__init__()\n        \n        self.out_channels = input_channels*2 if out_channels is None else out_channels\n        \n        # first layer must half the H_in,W_in and double the C_in with S=2, P=1\n        conv_1 = ConvReLU(input_channels, self.out_channels, kernel_size=3, stride=1, padding=1)\n        layers = [conv_1]\n        \n        # rest of the layers preserve the H_in, W_in and C_in with S=1 and P=1\n        for _ in range(num_layers-1):\n            conv_ = ConvReLU(self.out_channels, self.out_channels, kernel_size=3, stride=1, padding=1)\n            layers.append(conv_)\n\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.layers(x)\n\n\nmax_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\nconv1_block = Block(input_channels=3, num_layers=2, out_channels=64)\n\nx = conv1_block(input_img)\n\nprint(\"output volume after conv1_block = {}\".format(x.shape))\n\n# conv_block_2 has half the h,w (due to max pool right before it) and double the input channels. \nconv2_block = Block(input_channels=64, num_layers=2, out_channels=128)\nx = conv2_block(max_pool(x))\n\nprint(\"output volume after conv2_block = {}\".format(x.shape))"
  },
  {
    "objectID": "posts/cnns/01_resnet.html#vgg-model",
    "href": "posts/cnns/01_resnet.html#vgg-model",
    "title": "Training a Resnet in Pytorch",
    "section": "VGG model",
    "text": "VGG model\nWe can go on this way for the rest of the blocks, but lets bring it together in a VGG class itself.\nLets write down the vgg configuration as a list of items. The model can be imagined as a feature extractor with a classifier head on top.\nThe classifier part has fixed configuration, but the feature extractor can be configured by passing a list of numbers and strings to the constructor. Each item is either a number or a string and represents a conv layer or a max_pool layer. The number is the number of conv layers in the block, and the string is ‘M’ which indicates a max pool layer.\nThe classifier part consists of 2 hidden-FC layers (4096 neurons each) with activations and dropout regularization added. The final layer is the classifier head with 1000 outputs. (Imagenet dataset has 1k classes)\nLastly, to enable the model to initialize with variable input sizes, we have used a dynamic input size of the 1st FC layer., and a dynamic hidden_fc_size for the last FC layer. This will allow us to experiment with smaller datasets locally\n\n_vgg_config = {\n    'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n    'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n    'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n    'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M']\n}\nimport logging \nlogging.basicConfig(level=logging.DEBUG)\n\nclass VGG(nn.Module):\n    def __init__(self, config, input_shape=(224,224,3), dropout=0.5, num_classes=1000, hidden_fc_size=4096):\n        super().__init__()\n\n        self.input_shape = input_shape\n        self.output_shape = self.input_shape\n        self.hidden_fc_size = hidden_fc_size\n        prev_channels = self.input_shape[2]\n        \n        self.blocks = []\n        for i in config:\n            if i == 'M':\n                self.blocks.append(nn.MaxPool2d(kernel_size=2, stride=2)) \n                self.output_shape = (self.output_shape[0]//2, self.output_shape[1]//2, self.output_shape[2])\n            else:\n                self.blocks.append(ConvReLU(prev_channels, i, kernel_size=3, stride=1, padding=1))\n                self.output_shape = (self.output_shape[0], self.output_shape[1], i)\n                prev_channels = i\n\n        self.feature_extractor = nn.Sequential(*self.blocks)\n        \n        self.ConvMLP = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(self.output_shape[0]*self.output_shape[1]*self.output_shape[2], self.hidden_fc_size),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(self.hidden_fc_size, self.hidden_fc_size),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n        )\n\n        self.head = nn.Linear(self.hidden_fc_size, num_classes)\n\n    def forward(self, x):\n        # if a single item turn into a batch\n        if x.ndim == 3:\n            x = x.unsqueeze(0)\n\n        x = self.feature_extractor(x)\n        x = x.view(x.size(0), -1)  # this flattens the tensor to (batch_size, -1)\n        x = self.ConvMLP(x)\n        x = self.head(x)\n        return x\n\nNow lets visualize the model, output shapes of various layers and its size, etc using torchinfo.summary(). To install the package run pip install torchinfo on your terminal.\n\nfrom torchinfo import summary\n\nvgg19 = VGG(_vgg_config['vgg19'])\nsummary(vgg19, input_size=(3,224,224))\n\nA pretty huge model (140million parameters) isn’t it?! The original model was trained on the entire Imagenet dataset (150gb). But we’ll use the TinyImagenet dataset(&lt;1gb) for this example. So we’ll use a much smaller model.\n\n\nNUM_CLASSES=200 # Tiny ImageNet has 200 classes\nIMG_SIZE = 64\ndevice = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n\nmodel = VGG(\n    _vgg_config['vgg11'], \n    input_shape=(IMG_SIZE, IMG_SIZE, 3), \n    num_classes=NUM_CLASSES,\n    hidden_fc_size=1026\n).to(device)\n        \nsummary(model, input_size=(3,IMG_SIZE,IMG_SIZE))\n\nThis brings our model size down to a much more manageble 12million parameters from the original ~140 million parameters.\nFor training we’ll use the Adam optimizer and learing rate of 0.001. We’ll also use the CrossEntropyLoss as the loss function as this is a classification task.\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom torch.utils.tensorboard import SummaryWriter\nimport os\nfrom tqdm import tqdm\nimport urllib.request\nimport zipfile\nfrom PIL import Image\n\nif not os.path.exists(\"data/\"):\n    os.mkdir(\"data/\")\n\n# Hyperparameters\nBATCH_SIZE = 4\nLEARNING_RATE = 0.001\nNUM_EPOCHS = 10\nDATASET_PATH = \"./data/tiny-imagenet-200\"\n\n# Download and extract Tiny ImageNet\ndef download_and_extract_tiny_imagenet():\n    url = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n    filename = \"tiny-imagenet-200.zip\"\n    extract_path = DATASET_PATH\n    \n    if not os.path.exists(extract_path):\n        print(\"Downloading Tiny ImageNet...\")\n        urllib.request.urlretrieve(url, filename)\n        \n        print(\"Extracting Tiny ImageNet...\")\n        with zipfile.ZipFile(filename, 'r') as zip_ref:\n            zip_ref.extractall(\".\")\n        \n        os.remove(filename)\n        print(\"Tiny ImageNet downloaded and extracted.\")\n    else:\n        print(\"Tiny ImageNet already exists.\")\n    os.system(\"mv tiny-imagenet-200 data/tiny-imagenet-200\")\n    return extract_path\n\n# Custom Dataset for Tiny ImageNet\nclass TinyImageNetDataset(Dataset):\n    def __init__(self, root, split='train', transform=None, labels=None):\n        self.root = root\n        self.split = split\n        self.transform = transform\n        self.images = []\n        self.labels = []\n        \n\n        if split == 'train':\n            for class_dir in os.listdir(os.path.join(root, 'train')):\n                class_path = os.path.join(root, 'train', class_dir, 'images')\n                for img_name in os.listdir(class_path):\n                    self.images.append(os.path.join(class_path, img_name))\n                    self.labels.append(int(class_dir[1:]))\n        elif split == 'val':\n            val_annotations = os.path.join(root, 'val', 'val_annotations.txt')\n            with open(val_annotations, 'r') as f:\n                for line in f:\n                    img_name, class_id, _, _, _, _ = line.strip().split('\\t')\n                    print()\n                    self.images.append(os.path.join(root, 'val', 'images', img_name))\n                    self.labels.append(int(class_id[1:]))\n        self.labelset = list(set(self.labels)) if labels is None else labels\n        self.labels_to_class = {label:idx for idx,label in enumerate(self.labelset)}\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_path = self.images[idx]\n        label = self.labels_to_class[self.labels[idx]]\n        image = Image.open(img_path).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, label\n\n# Download and extract the dataset\nif os.path.exists(\"./data/tiny-imagenet-200\"):\n    dataset_path = DATASET_PATH\nelse:\n    dataset_path = download_and_extract_tiny_imagenet()\n\n# Data transforms\ntrain_transform = transforms.Compose([\n    # transforms.RandomResizedCrop(64),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nval_transform = transforms.Compose([\n    # transforms.Resize(64),\n    # transforms.CenterCrop(64),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Load datasets\ntrain_dataset = TinyImageNetDataset(dataset_path, split='train', transform=train_transform)\nval_dataset = TinyImageNetDataset(dataset_path, split='val', transform=val_transform, labels=train_dataset.labelset)\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=1)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=1)\n\n# Initialize model, loss function, and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n# TensorBoard writer\nwriter = SummaryWriter('runs/vgg_tiny_imagenet')\n\ndef train_and_validate(epoch):\n    model.train()\n    train_loss = 0\n    train_correct = 0\n    val_loss = 0\n    val_correct = 0\n\n    # Training\n    for batch_idx, (data, target) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} - Training\")):\n        print(batch_idx)\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n        pred = output.argmax(dim=1, keepdim=True)\n        train_correct += pred.eq(target.view_as(pred)).sum().item()\n        print(f\"train loss {torch.mean(train_loss)}\")\n        writer.add_scalar('Loss/train_step', loss.item())\n        # writer.add_scalar('Loss/val', val_loss, epoch)\n    train_loss /= len(train_loader)\n    train_accuracy = 100. * train_correct / len(train_loader.dataset)\n\n    # Validation\n    model.eval()\n    with torch.no_grad():\n        for data, target in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} - Validation\"):\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            loss = criterion(output, target)\n            val_loss += loss.item()\n            pred = output.argmax(dim=1, keepdim=True)\n            writer.add_scalar('Loss/val_step', loss.item())\n            val_correct += pred.eq(target.view_as(pred)).sum().item()\n            print(f\"val loss {torch.mean(val_loss)}\")\n    val_loss /= len(val_loader)\n    val_accuracy = 100. * val_correct / len(val_loader.dataset)\n\n    # Log to TensorBoard\n    writer.add_scalar('Loss/train', train_loss, epoch)\n    writer.add_scalar('Loss/val', val_loss, epoch)\n    writer.add_scalar('Accuracy/train', train_accuracy, epoch)\n    writer.add_scalar('Accuracy/val', val_accuracy, epoch)\n\n    print(f'Epoch {epoch+1}/{NUM_EPOCHS}:')\n    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n    print(f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n\n# Training loop\nfor epoch in range(NUM_EPOCHS):\n    train_and_validate(epoch)\n\nwriter.close()\nprint(\"Training completed!\")\n\n\nRunning vgg11 on colab gives us a score of ~28% accuracy on the validation set in 20 epochs, while training loss and val loss both were kind of still decreasing.\n\n\nimport matplotlib.pyplot as plt\n\ndef plot_params_and_grads(model, n=3):\n    grad_values = [(name, param.grad.abs().cpu().detach().numpy())\n                   for name, param in model.named_parameters()\n                   if param.grad is not None]\n    \n    def plot_grads(grad_values, title_prefix):\n        num_plots = min(n, len(grad_values))\n        fig, axes = plt.subplots(1, num_plots, figsize=(5 * num_plots, 5))\n        if num_plots == 1:\n            axes = [axes]  # Ensure axes is iterable if only one plot\n\n        for idx in range(num_plots):\n            name, grad = grad_values[idx]\n            axes[idx].hist(grad.flatten(), bins=50)\n            axes[idx].set_title(f'{title_prefix} Layer {name}')\n            axes[idx].set_xlabel('Gradient Magnitude')\n            axes[idx].set_ylabel('Frequency')\n        \n        plt.tight_layout()\n        plt.show()\n\n    # Plot first n parameters\n    plot_grads(grad_values[:n], title_prefix='First')\n\n    # Plot last n parameters\n    plot_grads(grad_values[-n:], title_prefix='Last')\n\n\nplot_params_and_grads(model, n=3)\n\n\nScaling to vgg_16:\n  suprisingly model isnt learning at all. loss not going down. upon inspection of gradients we find that while training vgg_199 there are practically no gradients.\n\n  Also note that grads start even when loss isnt decreasing at the first few steps.\nLook at the the below figures:\n### VGG-11 \n\n\n\n\n\n\nFigure 2: “VGG-11 First Layers Gradient Plot”\n\n\n\n### VGG-16 \n\n\n\n\n\n\nFigure 3: “VGG-16 First Layers Gradient Plot”(Vanishing Gradients)\n\n\n\nDoes removing bottlenecked maxpool before fc layers help?: No, it doesnt. :(\nwhat about vgg13? being clser to the 11 variant this doesnt experience vanishing gradients.\n\nThis is esssentialy the central limitations that still remain after going very deep. Depth is not enough, we need better ways to allow gradients to be propagated through these deep networks.\n\ncomputational considerations\nA usual Conv2d layer has a computation complexity of O(H_{in}*W_{in}*k^2*C_{in}*C_{out}). Derivation:\n\nnumber of output cells = H_{out}*W_{out}*C_{out}\ncomputation required to compute each output cell = k^2*C_{in}\nnumber of ops\n-&gt; number of output cells * computation required to compute each output cell\n-&gt; H_{out}*W_{out}*k^2*C_{in}*C_{out}"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deepam Minda",
    "section": "",
    "text": "Hi Y’all. My name is Deepam Minda and this is my website. I’ll be writing a lot more blogs, TILs, etc about various things that interest me, which include but are not limited to AI, tech, music, football. I hope you learn something new here. Feel free to share your thoughts or hit me up.\nCheers!"
  },
  {
    "objectID": "posts/floyd_warshall.html#implementation",
    "href": "posts/floyd_warshall.html#implementation",
    "title": "The Floyd-Warshall Algorithm",
    "section": "",
    "text": "Lets have a quick look at a basic graph algorithm today:\nGiven a weighted graph, finding the shortest path between all pairs of nodes.\n\n# lets define a undirected graph (we'll assume no weights for now)\n\n# Show the edges matrix (assume A-F as indices 0-6 in the matrix)\nlabels = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F'}\n\n# adjacency list (nodes: [connected nodes])\nG = {\n    'A': ['B','C'],\n    'B': ['A','E','D'],\n    'C': ['A','F','D'],\n    'D': ['B','C'],\n    'E': ['F','B'],\n    'F': ['E','C']\n}\n\n# Convert the graph dictionary to an edge matrix\nedges_matrix = [[0 if labels[i] not in G[labels[j]] else 1 for j in range(len(labels))] for i in range(len(labels))]\n\nedges_matrix\n\n[[0, 1, 1, 0, 0, 0],\n [1, 0, 0, 1, 1, 0],\n [1, 0, 0, 1, 0, 1],\n [0, 1, 1, 0, 0, 0],\n [0, 1, 0, 0, 0, 1],\n [0, 0, 1, 0, 1, 0]]\n\n\n\n\nCode\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Convert the graph dictionary to a NetworkX graph to visualize\nnx_graph = nx.Graph()\nfor i in range(len(edges_matrix)):\n    for j in range(len(edges_matrix[i])):\n        if edges_matrix[i][j] == 1:\n            nx_graph.add_edge(i, j)\n\n# Draw the graph with white nodes\nnx.draw(nx_graph, with_labels=True, labels=labels, font_weight='bold', node_color='white')\nplt.show()\n\n\n\n\n\n\n\n\n\nLets code the algorithm.\nTo begin with, we’ll have a distances matrix with same shape as edge matrix. Each value there represents the distance bw its row_num (i) and col_num (j), aka its weights! If no edge, the distance is \\infty.\nWe’ll also print the distance matrix as a dataframe to visualize its node labels.\n\nimport math \nimport pandas as pd\n\ninf = math.inf\nprint_grid = lambda x: print(pd.DataFrame(distances, index=labels.values(), columns=labels.values()))\n\nenum = {1:1, 0:inf}\ndistances = [row[:] for row in edges_matrix]\nsize = len(edges_matrix)\n\nfor i in range(size):\n    for j in range(size):\n        distances[i][j] = enum[distances[i][j]]\n\nprint_grid(distances)\n\n     A    B    C    D    E    F\nA  inf  1.0  1.0  inf  inf  inf\nB  1.0  inf  inf  1.0  1.0  inf\nC  1.0  inf  inf  1.0  inf  1.0\nD  inf  1.0  1.0  inf  inf  inf\nE  inf  1.0  inf  inf  inf  1.0\nF  inf  inf  1.0  inf  1.0  inf\n\n\nNow we have distance between any 2 nodes i and j, which we can iterate over in a nested for-loop. Now lets perform another loop for N times (N = num_nodes). In each pass we’ll add a new node k into the mix and perform the following operation:\ndistances[i][j] = min(distances[i][j], distances[i][k] + distances[k][j])\nNote that (i, j, k) \\mid (i,j,k) \\in \\{0, 1, \\ldots, n-1\\}\nAt any k^{th} step, we have i, j and a total of k vertices. We then have 2 choices: Is the path bw i and j through k vertices shorter than the distance bw i and j through k-1 vertices.\n\nFor any 2 nodes i \\& j does adding the k_{th} node to its path reduce the present minimum distance? The present minimum distance by the way, is the shortest path between nodes i \\& j considering all possible combinations with k-1 previous nodes.\n\nThis way by the time we reach the last node, we have a matrix that shows the shortest distance between any 2 nodes considering all available nodes. Beautiful isn’t it! Without the DP this would just look 1 hop deep into the network for each comparison.\nNow lets watch it in action for the graph we defined.\n\nvisited_nodes = []\nfor k in range(size):\n    for i in range(size):\n        for j in range(size):\n            distances[i][j] = min(distances[i][j], distances[i][k]+distances[k][j])\n            \n    # Print the distances matrix as a grid after every k\n    visited_nodes.append(labels[k])\n    print(f\"Pass {k+1}: with {visited_nodes}:\\n\")\n    print_grid(distances)\n    print(\"\\n=========\\n\")\n\nPass 1: with ['A']:\n\n     A    B    C    D    E    F\nA  inf  1.0  1.0  inf  inf  inf\nB  1.0  2.0  2.0  1.0  1.0  inf\nC  1.0  2.0  2.0  1.0  inf  1.0\nD  inf  1.0  1.0  inf  inf  inf\nE  inf  1.0  inf  inf  inf  1.0\nF  inf  inf  1.0  inf  1.0  inf\n\n=========\n\nPass 2: with ['A', 'B']:\n\n     A    B  C    D  E    F\nA  2.0  1.0  1  2.0  2  inf\nB  1.0  2.0  2  1.0  1  inf\nC  1.0  2.0  2  1.0  3  1.0\nD  2.0  1.0  1  2.0  2  inf\nE  2.0  1.0  3  2.0  2  1.0\nF  inf  inf  1  inf  1  inf\n\n=========\n\nPass 3: with ['A', 'B', 'C']:\n\n   A  B  C  D  E  F\nA  2  1  1  2  2  2\nB  1  2  2  1  1  3\nC  1  2  2  1  3  1\nD  2  1  1  2  2  2\nE  2  1  3  2  2  1\nF  2  3  1  2  1  2\n\n=========\n\nPass 4: with ['A', 'B', 'C', 'D']:\n\n   A  B  C  D  E  F\nA  2  1  1  2  2  2\nB  1  2  2  1  1  3\nC  1  2  2  1  3  1\nD  2  1  1  2  2  2\nE  2  1  3  2  2  1\nF  2  3  1  2  1  2\n\n=========\n\nPass 5: with ['A', 'B', 'C', 'D', 'E']:\n\n   A  B  C  D  E  F\nA  2  1  1  2  2  2\nB  1  2  2  1  1  2\nC  1  2  2  1  3  1\nD  2  1  1  2  2  2\nE  2  1  3  2  2  1\nF  2  2  1  2  1  2\n\n=========\n\nPass 6: with ['A', 'B', 'C', 'D', 'E', 'F']:\n\n   A  B  C  D  E  F\nA  2  1  1  2  2  2\nB  1  2  2  1  1  2\nC  1  2  2  1  2  1\nD  2  1  1  2  2  2\nE  2  1  2  2  2  1\nF  2  2  1  2  1  2\n\n=========\n\n\n\nLets traverse this evolution of the distance value for nodes F and D.\nLet potential_stops be the k-1 sets we have visited at the start of k^{th}.\nWe can see theres no direct path, so obviously the value is \\infty to begin with. Even with potential_stops=\\{A,B\\} we cant get to D. So the value after 2 passes is still \\infty.\nWhen we add C, we should get a path, and sure we do! we arrive at the answer directly which is 2 (ie F-&gt;C-&gt;D).\nWhy does that happen here? Its because for all nodes being added to the potential_stops, we are checking whether adding k^{th} node improves the shortest path among any 2 nodes! Hence at value of k, we know that if the shortest path exists in the given set, we have it already!\nSo the FW algorithm helps us find shortest distance between all node pairs in a weighted graph. The graph can be directed/undirected and its weights can be negative too."
  }
]