[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi Y’all. My name is Deepam Minda and this is my website. I want to be writing a lot more, blogs, TILs, whatever. Im a data scientist myself so naturally the content here would also mostly be around it. I hope you learn something new here. Feel free to share your thoughts or hit me up.\nCheers!"
  },
  {
    "objectID": "posts/rnns/01_rnns_from_scratch.html",
    "href": "posts/rnns/01_rnns_from_scratch.html",
    "title": "RNNS from scratch",
    "section": "",
    "text": "The only reason you would be hearing RNNs right now is probably when xLSTMs were released in May, 2024. Apart from this they have pretty much taken a back seat to watch transformers revolutionalize NLP and the entire field of AI in general.\nBut one would do well to remember how we got here, and RNNs played a massive role in bringing us here. So in this blog post I’m going to build a small RNN model and we’ll try to train it to generate text."
  },
  {
    "objectID": "posts/rnns/01_rnns_from_scratch.html#on-to-modelling",
    "href": "posts/rnns/01_rnns_from_scratch.html#on-to-modelling",
    "title": "RNNS from scratch",
    "section": "On to modelling",
    "text": "On to modelling\nNow let us build a model and train it. For starters we’ll just use a torch.nn.RNN layer to build the model and train it. Once we get the entire training and testing pipeline complete, we can come back and build the model from scratch.\n\nHow exactly can we generate text though?\nWe know we can get hidden states of the entire sequence as outputs from a rnn layer, but each hidden state h_t has ponly seen information till timestep t. What we can do is this:\n\nonly use the last hidden state and feed it to a linear layer with output shape equal to size of vocabulary.\nif softmax is applied on top of the linear layer’s output, it turns raw logits into the probabilities for different tokens in our vocab. This can be done outside the forward function too.\n\nNow each input sequence gives us one output token ie y_{t+1}. then we can take the sequence from 1 to t+1 and generate token y_{t+2}.\n\n\nDuring Training\nTo maximize training we can use all hidden state outputs instead of the last one. Because why wouldnt we want the model to learn from all its outputs!\n\noutput of rnn layer of shape (N, s, h) is fed into a linear layer of shape (h, vocab\\_size) to get (N,s,vocab\\_size) outputs. then we can pass them via softmax and apply cross-entropy loss on all of them and backpropagate through the model.\nsince at token level we now have N*s tokens on which we will calculate loss, its simpler to flatten the targets and generated logits before passing to cross-entropy loss.\n\nNote: Had it been a classification task, we could just pass last hidden_state, h_n to a linear layer with output size equal to number of classes.\n\n\nshow code\nclass Rnn_model(nn.Module):\n\n    def __init__(self, embedding_size, max_seq_len, hidden_size, num_layers=1, vocab_size=None):\n        super(Rnn_model, self).__init__()\n\n        self.max_seq_len = max_seq_len\n        self.vocab_size = len(vocab) if vocab_size is None else vocab_size\n        self.embedding = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=embedding_size)\n        self.rnn = nn.RNN(input_size=embedding_size, hidden_size=hidden_size, batch_first=True, num_layers=num_layers)\n        self.fc = nn.Linear(hidden_size, len(vocab))\n        self.softmax = nn.Softmax(dim=-1)\n        self.criterion = nn.CrossEntropyLoss(ignore_index=0)  # ignore pad token\n\n    def forward(self, x, targets=None):\n        x = x[:, -self.max_seq_len:]\n        x = self.embedding(x)\n        H, h_n = self.rnn(x)\n\n        # y = self.fc(H[:,-1,:])\n        y = self.fc(H)\n        \n        if targets is not None:\n            B, T, V = y.shape\n            loss = self.criterion(y.view(B*T, V), targets.view(-1))\n        else: loss=None\n        \n        return y, loss\n    \n    \n    @torch.no_grad\n    def generate(self, input_text, max_len=32):\n\n        # input_text = [\"hello there\", \"hello there \"]\n        encoded_tokens = torch.tensor(encode_arr([input_text], max_seq_len=32))\n        if encoded_tokens.ndim == 1:\n            encoded_tokens = encoded_tokens.unsqueeze(0)\n\n        for i in range(max_len):\n            # print(encoded_tokens.shape, encoded_tokens[:, -self.max_seq_len:].shape)\n            outputs, _ = self(encoded_tokens[:, -self.max_seq_len:])\n\n            # last output token\n            outputs = outputs[:, -1, :]\n\n            # get pribabilities from logits\n            next_token_probs = nn.functional.softmax(outputs, dim=-1)\n\n            # sample indices from it using a multinomial distribution\n            next_tokens = torch.multinomial(next_token_probs, 1)\n\n            # concat prediction to original text\n            encoded_tokens = torch.concat((encoded_tokens, next_tokens), axis=1)\n\n        decoded_texts = decode_arr(encoded_tokens)\n        if len(decoded_texts)==1:\n            return decoded_texts[0] #.replace(\"&lt;pad&gt;\", \"\") \n        else: \n            # return [text.replace(\"&lt;pad&gt;\", \"\") for text in decoded_texts]\n            return decoded_texts\n\n\n\n\nshow code\n# model = Rnn_model(embedding_size=10, max_seq_len=32, hidden_size=20, num_layers=1)\n# x, y = get_data(train_data, 4, 32)\n# y_hat, loss = model(x)\n# x.shape, y.shape\n\n\n\n\nshow code\n# print(y.shape, y_hat.shape)\n# # y = y.view(-1)\n# B, T, V = y_hat.shape\n# y_hat.view(B*T, V)\n\n# loss = nn.CrossEntropyLoss(ignore_index=0)\n# loss(y_hat.view(B*T, V), y.view(-1))\n# # loss(y_hat, y)\n# # y.shape, y_hat.shape\n\n\n\n\nshow code\n\n# input_text = [\"whats up my G GGGGG\", \"hey there\"]\n# max_seq_len=32\n# encoded_tokens = torch.tensor(encode_arr([input_text], max_seq_len=max_seq_len))\n# print(encoded_tokens)\n# if encoded_tokens.ndim == 1:\n#     encoded_tokens = encoded_tokens.unsqueeze(0)\n\n# for i in range(4):\n#     print(encoded_tokens.shape, encoded_tokens[:, -max_seq_len:].shape)\n#     outputs = model(encoded_tokens[:, -max_seq_len:])\n#     # last output token\n#     next_token_probs = outputs[:, -1, :]\n#     # print(next_token_probs)\n#     # sample indices from it using a multinomial distribution\n#     next_tokens = torch.multinomial(next_token_probs, 1)\n#     # print(next_tokens)\n#     encoded_tokens = torch.concat((encoded_tokens, next_tokens), axis=1)\n\n# print(encoded_tokens)\n\n\n\n\nshow code\n# Now a simple training loop.\n\nNUM_STEPS = 10000\nMAX_SEQ_LEN = 64\nBATCH_SIZE = 16\nEMBEDDING_SIZE = 20\nHIDDEN_SIZE = 20\nNUM_LAYERS = 2\nLR = 0.001 \nlog_every = 50\n\nmodel = Rnn_model(embedding_size=EMBEDDING_SIZE, max_seq_len=MAX_SEQ_LEN, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS)\noptimizer = torch.optim.Adam(model.parameters(), lr=LR)\nlosses = []\nfor i in range(NUM_STEPS):\n    optimizer.zero_grad()\n    x, y = get_data(train_data, BATCH_SIZE, MAX_SEQ_LEN)\n    outputs, loss = model(x, y)\n    loss.backward()\n    optimizer.step()\n    if i%log_every==0:\n        losses.append(loss)\n\n\n\nsqitching from SGD to adam instantly gives boost (2.8 vs under 2)\n\n\n\nshow code\nmodel.generate(\"hey there handsome what\")\n\n\n'&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;hey there handsome what not prrestetion frected mual am'"
  },
  {
    "objectID": "posts/few-shot-learning/prototype_networks.html",
    "href": "posts/few-shot-learning/prototype_networks.html",
    "title": "Few Shot learning: Classify using few examples!",
    "section": "",
    "text": "If you aren’t already familiar with it, few-shot learning (FSL) is an exciting concept in machine learning that focuses on training models to make accurate predictions or classifications when provided with only a very limited amount of labeled data for each category or class. Unlike traditional machine learning approaches that often require substantial labeled datasets for training, few-shot learning aims to address scenarios where acquiring such extensive labeled data is impractical or costly.\nIn few-shot learning, the model is trained to generalize effectively from a small number of examples (or shots) per category, allowing it to make predictions for new, unseen data points with confidence. This capability is particularly valuable in situations where data is scarce, and manual annotation is labor-intensive or expensive. Refer to [1] for an in-depth survey in this field.\nIn this blog, I’m going to show you how to implement a basic few-shot classification technique for text."
  },
  {
    "objectID": "posts/few-shot-learning/prototype_networks.html#introduction",
    "href": "posts/few-shot-learning/prototype_networks.html#introduction",
    "title": "Few Shot learning: Classify using few examples!",
    "section": "",
    "text": "If you aren’t already familiar with it, few-shot learning (FSL) is an exciting concept in machine learning that focuses on training models to make accurate predictions or classifications when provided with only a very limited amount of labeled data for each category or class. Unlike traditional machine learning approaches that often require substantial labeled datasets for training, few-shot learning aims to address scenarios where acquiring such extensive labeled data is impractical or costly.\nIn few-shot learning, the model is trained to generalize effectively from a small number of examples (or shots) per category, allowing it to make predictions for new, unseen data points with confidence. This capability is particularly valuable in situations where data is scarce, and manual annotation is labor-intensive or expensive. Refer to [1] for an in-depth survey in this field.\nIn this blog, I’m going to show you how to implement a basic few-shot classification technique for text."
  },
  {
    "objectID": "posts/few-shot-learning/prototype_networks.html#terminology",
    "href": "posts/few-shot-learning/prototype_networks.html#terminology",
    "title": "Few Shot learning: Classify using few examples!",
    "section": "Terminology",
    "text": "Terminology\nBefore we begin, let us familiarize ourselves with the correct terminology.\nWhat characterizes FSL is having only a few examples at hand, for unseen classes, during inference. So basically we are showing the model only a few examples of a class which it may or may not have encountered during its pre-training before we make predictions using that model.\nSupport Set, 𝒮: The few annotated examples that we have, make up the support set, with which we may or may not update the model weights to make it generalize to the new classes.\nQuery Set, 𝒬: The query set consists of our test set, i.e. the samples we want to classify using the base model and a support set.\nN-way K-shot learning scheme: This is a common phrase used in the FSL literature, which essentially describes the few-shot problem statement that a model will be dealing with. “N” is the number of classes we have at test time and “K” is the number of samples per class we have in our support set “𝒮”\n1-shot classification: When K=1, i.e. we have only one labeled sample available per class.\n0-shot classification: K=0, i.e. we do not have any labeled samples available during inference.\nLet us have a look at an example.\n\n\nCode\n# sample set is 3-way, 3-shot.\nclasses = ['camera', 'battery', 'display']\n\nsample_set = {\n    'camera': [\n        'absolutely love this quality of my photos!!',\n        'it even gives great quality in dim lighting. fabulous!!',\n        'the camera should be much better for such a high price'\n    ],\n    'battery': [\n        \"The battery life on this device is exceptional! It easily lasts me the entire day with heavy usage.\",\n        \"I'm a bit disappointed with the battery performance. It drains quite quickly, especially when using power-hungry apps.\",\n        \"The battery is decent, not too bad, not too good. It gets me through the day, but I was hoping for better longevity.\"\n    ],\n    'display': [\n        \"The display on this device is stunning! Colors are vivid, and the resolution is top-notch.\",\n        \"I'm not too impressed with the display quality. It seems a bit washed out, and the brightness could be better.\",\n        \"The display is okay, but nothing extraordinary. It gets the job done for everyday tasks.\"\n    ]\n}\n\nquery_set = [\"i hate the batteries\", \"does it give good quality photos in the night?\"]\n\n\nHere we have a 3-way (there are 3 classes), 3-shot (3 examples for each class) setting."
  },
  {
    "objectID": "posts/few-shot-learning/prototype_networks.html#high-level-design",
    "href": "posts/few-shot-learning/prototype_networks.html#high-level-design",
    "title": "Few Shot learning: Classify using few examples!",
    "section": "High level design",
    "text": "High level design\nLet us have a quick look at the architecture of the system.\n\n\n\nA simple few shot classification system\n\n\nThis is the flow of our solution:\nThe first step is to get an embedding module. That can be created using regular supervised learning (Resnets trained on Imagenet) or self-supervised learning (BERT and co). Then, we use the embedding module to get feature representations for our classes in the support set. A simple way to do this is to turn each class’s examples into embeddings and take the mean of those vectors. This then becomes our “prototype” vectors to compare against. Now for each query, we can take the embeddings of the query text and use cosine similarity to find the predicted class. This closely resembles This system basically allows us to leverage transfer learning to use large backbones as our embedding module. And there is also the advantage of not performing any gradient updates. This helps us maintain a much more dynamic and flexible system.\nThe idea of comparing query samples with the support set samples is inspired by metric learning. Refer to [3, 4] for better understanding.\nLet’s implement this using the transformers library. You can find the implementation in this colab notebook."
  },
  {
    "objectID": "posts/few-shot-learning/prototype_networks.html#implementation",
    "href": "posts/few-shot-learning/prototype_networks.html#implementation",
    "title": "Few Shot learning: Classify using few examples!",
    "section": "Implementation",
    "text": "Implementation\nLet’s start with the good old BERT base model.\n\n1. Import libraries and download model\n\n\nCode\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport numpy as np\nfrom typing import Dict\nfrom pprint import pprint\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\nif torch.backends.mps.is_available():\n    device = torch.device('mps')\nelse:\n    device = torch.device('cpu')\n    \n# load the model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\nmodel = AutoModel.from_pretrained(\"bert-base-uncased\")\n\n\n\n\n2. Tokenize and encode a sentence\n\n\nCode\ntext = \"He's such a great guy!!\"\nencoded_input = tokenizer(\n  text, \n  return_tensors='pt', \n  padding='max_length',     # True will pad to max-len in batch\n  max_length=32\n)\nprint(f\"encoded input:\")\npprint(encoded_input)\n\n\nencoded input:\n{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]]),\n 'input_ids': tensor([[ 101, 2002, 1005, 1055, 2107, 1037, 2307, 3124,  999,  999,  102,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0]]),\n 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]])}\n\n\nwhere,\n\ninput_ids: token id of each token\ntoken_type_id: When we pass two sentences for downstream fine-tuning in BERT, this is used to identify which token belongs to which sentence.\nattention_mask: which tokens to ignore. As you’ll see, the padding tokens have been masked.\n\n\n\n3. Generate embeddings using model\nThe output has 2 parts, cls_token_embeddings and last_hidden_states of the tokens. We can either use the cls_embeddings to represent the sentence or pool the vectors in last_hidden_states. The pooling can be max/min/mean.\nThe dimension of the output will be equal to the embedding dimension of the model, i.e. 784 in our case.\n\n\nCode\ndef get_embeddings(model, tokenizer, text, pooling='mean'):\n  \n  encoded_input = tokenizer(\n    text, \n    return_tensors='pt', \n    padding='max_length', \n    max_length=16, \n    truncation=True\n  )\n  encoded_input = encoded_input.to(device)\n\n  model.to(device)\n\n  model.eval()\n  with torch.no_grad():\n    output = model(**encoded_input)\n    last_hidden_state, pooler_output = output[0], output[1]\n    \n    if pooling=='cls':\n      embedding = pooler_output\n    else:\n      # ignore the pad tokens embeddings by multiplying with attention mask\n      last_hidden_state = (last_hidden_state * encoded_input['attention_mask'].unsqueeze(2))\n      embedding = last_hidden_state.mean(dim=-2)\n  return np.array(embedding.cpu())\n\n\nembeddings = get_embeddings(model, tokenizer, 'hey there! how are you?')\nprint(f\"shape of embeddings: {embeddings.shape}\")\n\n\nshape of embeddings: (1, 768)\n\n\n\n\n4. Prepare the prototypes:\nTo prepare the class prototypes we’ll take the mean of the sentences for each class.\n\n\nCode\ndef make_prototypes(model, tokenizer, sample_set: Dict):\n  prototype_vectors = dict()\n  sentence_embeddings = dict()\n  for category, sentences in sample_set.items():\n    sentence_embeds = get_embeddings(model, tokenizer, sentences)\n    sentence_embeddings[category] = sentence_embeds\n    prototype_vectors[category] = np.mean(sentence_embeddings[category], axis=0)\n  return prototype_vectors\n\n\n\n\n5. Classify\nTo classify a query text, we can run cosine similarity against the prototype vectors and return the argmax as the most probable class!\n\n\nCode\ndef classify(model, tokenizer, text, prototype_vectors=None, sample_set=None):\n  if prototype_vectors==None:\n      assert sample_set!=None, \"prototype vectors are not passed, either pass a sample set prototype vectors\"\n      prototype_vectors = make_prototypes(sample_set)\n\n  query_embeddings = get_embeddings(model, tokenizer, text)\n  \n  prototype_matrix = np.stack(list(prototype_vectors.values()))\n  scores = sentence_transformers.util.cos_sim(query_embeddings, prototype_matrix)\n  return scores\n\n\nUsing the above-defined functions and the sample set from before, we have:\n\n\nCode\nprototype_vectors = make_prototypes(model, tokenizer, sample_set)\nquery_text = \"i hate the batteries\"\noutput = classify(model, tokenizer, query_text, prototype_vectors=prototype_vectors)\n\nprint(f\"output: {output}\")\nprint(f\"the predicted class is {classes[output.argmax().item()]}\")\n\n\noutput: tensor([[0.6121, 0.7127, 0.6388]])\nthe predicted class is battery\n\n\nA bit strange! Although the expected class is predicted, scores for other classes are also high. Let’s try a harder query.\n\n\nCode\nquery = ['does it give good quality photos in the night?']\noutput = classify(model, tokenizer, query, prototype_vectors=prototype_vectors)\n\nprint(f\"output: {output}\")\nprint(f\"the predicted class is {classes[output.argmax().item()]}\")\n\n\noutput: tensor([[0.7984, 0.7043, 0.7647]])\nthe predicted class is camera\n\n\nAlthough the highest similarity is for ‘camera’, the similarity should be much higher.\nThe results do not get better even if we try cls-pooling. This only means that the embeddings produced by the model do not give us an accurate representation of the sentence.\nWe would then do good to remember that BERT pre-train was trained by MaskedLM, NextSentencePrediction, hence the original purpose of BERT is not to create a meaningful embedding of the sentence but for some specific downstream task. In fact, as the authors of the sentence-transformer paper [2] point out, out-of-the-box Bert embeddings perform even worse than GLoVE representations!\n\nJacob Devlin’s comment: I’m not sure what these vectors are, since BERT does not generate meaningful sentence vectors. It seems that this is is doing average pooling over the word tokens to get a sentence vector, but we never suggested that this will generate meaningful sentence representations. And even if they are decent representations when fed into a DNN trained for a downstream task, it doesn’t mean that they will be meaningful in terms of cosine distance. (Since cosine distance is a linear space where all dimensions are weighted equally).\n\nThere are a few ways to improve the bert-base for sentence-level tasks and both involve finetuning the model with some data.\n\nadding a linear layer on top and fine-tuning it.\nmaking embeddings better by contrastive learning."
  },
  {
    "objectID": "posts/few-shot-learning/prototype_networks.html#using-sentence-transformers",
    "href": "posts/few-shot-learning/prototype_networks.html#using-sentence-transformers",
    "title": "Few Shot learning: Classify using few examples!",
    "section": "Using sentence transformers",
    "text": "Using sentence transformers\nUltimately, what we need is a better embedding module. Luckily we have such models. As it turns out, contrastive learning is an excellent approach for tuning our models such that different sentences produce semantically different embeddings.\nWe will explore contrastive learning and its inner workings some other day, but for now, let’s pick up open-source models that have been finetuned using contrastive learning. There is an entire library (aka sentence-transformers) and paper[2] dedicated to this.\nWe’ll use the sentence-transformers/stsb-bert-base model for our purposes.\n\n1. Import packages and download model\n\n\nCode\nfrom sentence_transformers import SentenceTransformer\n\n# load a sentence transformer model\nsts_model = SentenceTransformer('sentence-transformers/stsb-bert-base')\nmodel2 = sts_model[0].auto_model.to(device)\ntokenizer2 = sts_model[0].tokenizer\n\n\n\n\n2. Use the above-defined functions to prepare prototype vectors and classify them in a few-shot setting\n\n\nCode\nprototype_vectors = make_prototypes(model2, tokenizer2, sample_set)\nquery_text = \"i hate the batteries\"\noutput = classify(model2, tokenizer2, query_text, prototype_vectors)\n\nprint(f\"output: {output}\")\nprint(f\"the predicted class is {classes[output.argmax().item()]}\")\n\n\noutput: tensor([[0.0910, 0.4780, 0.1606]])\nthe predicted class is battery\n\n\n\n\nCode\n\nquery = ['does it give good quality photos in the night?']\noutput = classify(model2, tokenizer2, query, prototype_vectors=prototype_vectors)\n\nprint(f\"output: {output}\")\nprint(f\"the predicted class is {classes[output.argmax().item()]}\")\n\n\noutput: tensor([[0.4467, 0.1012, 0.2998]])\nthe predicted class is camera\n\n\nAs we can see, the scores seem much more reasonable this time around. There is a much better correlation with the ground truth labels. Using better base models trained in multiple tasks further improves the performance of these models."
  },
  {
    "objectID": "posts/few-shot-learning/prototype_networks.html#conclusion",
    "href": "posts/few-shot-learning/prototype_networks.html#conclusion",
    "title": "Few Shot learning: Classify using few examples!",
    "section": "Conclusion",
    "text": "Conclusion\nThis brings us to the end of this blog. In summary, we explored the realm of few-shot learning, a machine-learning approach tailored for accurate predictions with limited labeled data. Initially, we employed BERT, but its design didn’t align with our objectives. Instead, we leveraged a model fine-tuned for sentence-level tasks, sentence-transformers/stsb-bert-base, which significantly improved our results.\nThese are a few things to note:\nAlthough we directly used pre-trained models here, an interesting undertaking would be to perform the contrastive fine-tuning ourselves. Also, instead of using cosine similarity, we can train lightweight classifiers on top of our embedding module for better performance.\nThat’ll be all from my side. Until next time, Happy Reading!"
  },
  {
    "objectID": "posts/few-shot-learning/prototype_networks.html#references",
    "href": "posts/few-shot-learning/prototype_networks.html#references",
    "title": "Few Shot learning: Classify using few examples!",
    "section": "References",
    "text": "References\n[1] Survey paper on few-shot learning\n[2] Sentence-Bert paper\n[3] Prototypical Networks\n[4] Excellent much more techincal blog by Lilian Weng"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blogs by a noob techie",
    "section": "",
    "text": "RNNS from scratch\n\n\n\n\n\n\nclassification\n\n\nrnns\n\n\nnlp\n\n\nseq-to-seq\n\n\n\n\n\n\n\n\n\nJul 20, 2024\n\n\nDeepam Minda\n\n\n\n\n\n\n\n\n\n\n\n\nFew Shot learning: Classify using few examples!\n\n\n\n\n\n\nclassification\n\n\nfew-shot learning\n\n\nnlp\n\n\n\n\n\n\n\n\n\nSep 12, 2023\n\n\nDeepam Minda\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/rnns/01_rnns_from_scratch.html#import-data-and-process-it",
    "href": "posts/rnns/01_rnns_from_scratch.html#import-data-and-process-it",
    "title": "RNNS from scratch",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport random\nimport numpy as np \n\nLet us load a dataset. I downloaded a few mystery books and concatenated their raw text to make the dataset. Follow [this] link to get the dataset.\n\ndataset_path = \"./corpus.txt\"\n\nwith open(dataset_path, 'r') as f:\n    data = f.read()\n\nLets have a look at some of the text we have.\n\n\nshow code\nprint(f\"Sample data chunk:\\n'{data[1000:1500]}'\")\n\n\nSample data chunk:\n'rld has seen, but as a lover he would have placed himself in a\nfalse position. He never spoke of the softer passions, save with a gibe\nand a sneer. They were admirable things for the observer—excellent for\ndrawing the veil from men’s motives and actions. But for the trained\nreasoner to admit such intrusions into his own delicate and finely\nadjusted temperament was to introduce a distracting factor which might\nthrow a doubt upon all his mental results. Grit in a sensitive\ninstrument, or a crack i'\n\n\nLets split the data into train and test (80-20)\n\n\nshow code\ntrain_size = int(len(data) * 0.8)\ntrain_data, test_data = data[:train_size], data[train_size:]\n\nprint(f\"train_size: {len(train_data)}, test_size: {len(test_data)}  (in number of tokens)\")\n\n\ntrain_size: 2449724, test_size: 612431  (in number of tokens)\n\n\nNow let us encode the text to numerical data that our model can understand. Encoding the data generally means tokenization and then encoding. To keep it super simple, we’ll just use individual characters as tokens.\nNowadays however, subword tokenization algorithms like Byte-Pair Encoding are the norm. But let us not get caught up in those for now.\n\nvocab = list(set(train_data+\"12345678910qwertyuioplkjhgfdsamnbvcxz~!@#$%^&*()_+`-=[];'./,{}\\\":?&gt;&lt;\\|\"))\n\nPAD_TOKEN = '&lt;pad&gt;'\nspecial_tokens = [PAD_TOKEN]\nMAX_SEQ_LEN = 32\n\nvocab = [*special_tokens, *vocab]\n\ndecode_mapping = dict(enumerate(vocab))\nencode_mapping = {v:k for k,v in decode_mapping.items()}\n\nencode = lambda text: [encode_mapping[char] for char in text]\ndecode = lambda text: [decode_mapping[char] for char in text]\n\nNow we can do a simple sanity check by encoding a text and decoding it. we should get the original string back.\n\n\nshow code\ntext = \"hey there\"\nencoded_text = encode(text)\n\nprint(f\"original text: {text}\")\nprint(f\"tokenized text: {list(text)}\")\nprint(f'encoded text: {encoded_text}')\nprint(f\"decoded back:{decode(encoded_text)}\")\n\n\noriginal text: hey there\ntokenized text: ['h', 'e', 'y', ' ', 't', 'h', 'e', 'r', 'e']\nencoded text: [95, 66, 120, 79, 46, 95, 66, 71, 66]\ndecoded back:['h', 'e', 'y', ' ', 't', 'h', 'e', 'r', 'e']\n\n\nNow we need functions that will do this for batches of texts rather than single text. When dealing with batches, there are a few extra considerations: - You would typically want your batches to contain texts which are of same length, so that matrix/tensor operations can be performed. Hence we need to truncate longer sentences and pad shorter sentences to a fixed length.\nFor simplicity let us also define a function that will fetch us a random batch of data from our training set.\n\ndef get_data(data, batch_size=16, seq_len=256):\n\n    buffer = 1000\n    l = len(data)\n    start = np.random.randint(0,l-buffer, size=batch_size)\n    end = start + seq_len\n    texts = [data[s:e] for s,e in zip(start,end)]\n    \n    encoded_texts = encode_arr(texts, max_seq_len=seq_len+1)\n    targets = encoded_texts[:, 1:]\n    inputs = encoded_texts[:, :-1]\n    \n    targets = torch.tensor(targets, dtype=torch.long)\n    inputs = torch.tensor(inputs, dtype=torch.long)\n    \n    return inputs, targets \n\n\n\nshow code\nx, y = get_data(train_data, 4, 23)\nprint(\"target is just x shifted one token to the left!\")\nprint(f\"x[0]: {x[0]},\\ny[0] {y[0]}\")\n\n\ntarget is just x shifted one token to the left!\nx[0]: tensor([  0,   7,   7, 118,  95,  66,  79,  73, 110,  76,  75,  66,  71,  60,\n         86,  79, 103,  71,  66,  86,  86,  69,  76]),\ny[0] tensor([  7,   7, 118,  95,  66,  79,  73, 110,  76,  75,  66,  71,  60,  86,\n         79, 103,  71,  66,  86,  86,  69,  76, 117])"
  },
  {
    "objectID": "posts/rnns/01_rnns_from_scratch.html#understanding-rnns",
    "href": "posts/rnns/01_rnns_from_scratch.html#understanding-rnns",
    "title": "RNNS from scratch",
    "section": "Understanding RNNs",
    "text": "Understanding RNNs\nRNNs have 2 matrices, one (W_{xh}) that maps input tokens to hidden_vector size and another (W_{hh}) that maps from hidden_vector to hidden_vector. You’ll see how these are used in a minute.\nLet us first look at input-output shapes for an RNN layer. We initially had a batch of text-tokens. Lets assume batch size of 4 and max_seq_len of 32. Hence the shape of input is (4,32).\nNow for each token, we encode it to a number and then map it to a vector (which we generally call an embedding). Hence each token is now represented by a vector of fixed-shape, and lets call this embedding_dimension and set it to 10.\nThe shape of our input batch is now (batch_size, max_seq_len, emb_dim), ie (4,32,10).\nNow let us peek into the matrix multiplications inside a RNN layer. Firstly, lets us recall that for a linear layer, this is the matrix equation:\nz (N, n_{out}) = \\sigma(x (N, n_{in}) * W_x^T (n_{in}, n_{out}) + b (N))\nwhere ,\n\ninput-features = n_{in}\noutput-features = n_{out}\nbatch-size = N\n\nIn a linear layer, each token/feature is attended to by a different weight in the weight matrix and no information is shared among the sequence tokens. But when processing “sequences” we obviously want the model to remember stuff from previous tokens for the current token, right?\nHence RNNs maintain a hidden_vector for each token, that takes as input the current token and the hidden_vector from the previous token’s output.\nSo for the t’th token,\nh_t (N, h)= x_t (N, n_{in}) * W_{xh}^T (n_{in}, h) + h_{t-1} (N, h) * W_{hh}^T (h, h) + biases\nwhere\n\ninput-features = n_{in}\nhidden-size = h\nbatch-size = N\nsequence-length = s\n\nAs you’ll notice since each token depends on previous tokens output, we cannot process this parallelly and have to iteratively calculate the output for each token. Also note we generally refer to the different tokens in a sequence as different timesteps, ie token at timestep t is x_t.\nHence for a complete batch, inputs are:\n\nX of shape (N, s, n_{in})\nh_0 of shape (N, h) (this is optional, if not given most libraries will initiate a h_0 of all zeros or random numbers)\n\nAnd outputs are:\n\nhidden states of all timesteps, ie H of shape (N, s, h)\nlast_hidden_state ie h_n of shape (N, h)\n\nNote: sometimes you will see outputs of rnn fed into a linear layer like so,\noutputs, h_n = self.rnn(x)\ny = self.fc(outputs[:,-1,:])\nHere h_n and outputs[:,-1,:] are the same thing. They both represent the last hidden state for the entire batch. (to make shapes equal use h_n.squeeze())\nLets verify the above by passing inputs to an rnn layer.\n\n\nshow code\nemb_dim = 50\nhidden_size = 100\nbatch_size = 4\nmax_seq_len = 256\n\nprint(f\"batch_size: {batch_size}, hidden_size: {hidden_size}, max_seq_len: {max_seq_len}, emb_dim: {emb_dim}\")\nX,y = get_data(train_data, seq_len=max_seq_len, batch_size=batch_size)\nprint(f\"shape of initial input -&gt; {X.shape}\")\n\nemb_layer = nn.Embedding(num_embeddings=len(vocab), embedding_dim=emb_dim)\nrnn_layer = nn.RNN(input_size=emb_dim, hidden_size=hidden_size, batch_first=True, bidirectional=False, num_layers=1)\n\nX = emb_layer(X)\nprint(f\"post embedding; shape of input to RNN layer -&gt; {X.shape}\")\nh_0 = torch.randn(1, batch_size, hidden_size)\noutputs = rnn_layer(X, h_0)\n\nprint(f\"RNN output shapes -&gt; {outputs[0].shape}, {[outputs[1][i].shape for i in range(len(outputs[1]))]}\")\n\n\nbatch_size: 4, hidden_size: 100, max_seq_len: 256, emb_dim: 50\nshape of initial input -&gt; torch.Size([4, 256])\npost embedding; shape of input to RNN layer -&gt; torch.Size([4, 256, 50])\nRNN output shapes -&gt; torch.Size([4, 256, 100]), [torch.Size([4, 100])]"
  },
  {
    "objectID": "posts/rnns/01_rnns_from_scratch.html#language-modelling",
    "href": "posts/rnns/01_rnns_from_scratch.html#language-modelling",
    "title": "RNNS from scratch",
    "section": "Language Modelling",
    "text": "Language Modelling\nNow let us build a model and train it. For starters we’ll just use a torch.nn.RNN layer to build the model and train it. Once we get the entire training and testing pipeline complete, we can come back and build the model from scratch.\n\nHow exactly can we generate text though?\nWe know we can get hidden states of the entire sequence as outputs from a rnn layer, but each hidden state h_t has ponly seen information till timestep t. What we can do is this:\n\nonly use the last hidden state and feed it to a linear layer with output shape equal to size of vocabulary.\nif softmax is applied on top of the linear layer’s output, it turns raw logits into the probabilities for different tokens in our vocab. This can be done outside the forward function too.\n\nNow each input sequence gives us one output token ie y_{t+1}. then we can take the sequence from 1 to t+1 and generate token y_{t+2}.\n\n\nDuring Training\nTo maximize training we can use all hidden state outputs instead of the last one. Because why wouldnt we want the model to learn from all its outputs!\n\noutput of rnn layer of shape (N, s, h) is fed into a linear layer of shape (h, vocab\\_size) to get (N,s,vocab\\_size) outputs. then we can pass them via softmax and apply cross-entropy loss on all of them and backpropagate through the model.\nsince at token level we now have N*s tokens on which we will calculate loss, its simpler to flatten the targets and generated logits before passing to cross-entropy loss.\n\nNote: Had it been a classification task, we could just pass last hidden_state, h_n to a linear layer with output size equal to number of classes.\n\n\nshow code\nclass Rnn_model(nn.Module):\n\n    def __init__(self, embedding_size, max_seq_len, hidden_size, num_layers=1, vocab_size=None):\n        super(Rnn_model, self).__init__()\n\n        self.max_seq_len = max_seq_len\n        self.vocab_size = len(vocab) if vocab_size is None else vocab_size\n        self.embedding = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=embedding_size)\n        self.rnn = nn.RNN(input_size=embedding_size, hidden_size=hidden_size, batch_first=True, num_layers=num_layers)\n        self.fc = nn.Linear(hidden_size, len(vocab))\n        self.softmax = nn.Softmax(dim=-1)\n        self.criterion = nn.CrossEntropyLoss(ignore_index=0)  # ignore pad token\n\n    def forward(self, x, targets=None):\n        x = x[:, -self.max_seq_len:]\n        x = self.embedding(x)\n        H, h_n = self.rnn(x)\n\n        # y = self.fc(H[:,-1,:])\n        y = self.fc(H)\n        \n        if targets is not None:\n            B, T, V = y.shape\n            loss = self.criterion(y.view(B*T, V), targets.view(-1))\n        else: loss=None\n        \n        return y, loss\n    \n    \n    @torch.no_grad\n    def generate(self, input_text, max_len=32):\n\n        # input_text = [\"hello there\", \"hello there \"]\n        encoded_tokens = torch.tensor(encode_arr([input_text], max_seq_len=32))\n        if encoded_tokens.ndim == 1:\n            encoded_tokens = encoded_tokens.unsqueeze(0)\n\n        for i in range(max_len):\n            # print(encoded_tokens.shape, encoded_tokens[:, -self.max_seq_len:].shape)\n            outputs, _ = self(encoded_tokens[:, -self.max_seq_len:])\n\n            # last output token\n            outputs = outputs[:, -1, :]\n\n            # get pribabilities from logits\n            next_token_probs = nn.functional.softmax(outputs, dim=-1)\n\n            # sample indices from it using a multinomial distribution\n            next_tokens = torch.multinomial(next_token_probs, 1)\n\n            # concat prediction to original text\n            encoded_tokens = torch.concat((encoded_tokens, next_tokens), axis=1)\n\n        decoded_texts = decode_arr(encoded_tokens)\n        if len(decoded_texts)==1:\n            return decoded_texts[0] #.replace(\"&lt;pad&gt;\", \"\") \n        else: \n            # return [text.replace(\"&lt;pad&gt;\", \"\") for text in decoded_texts]\n            return decoded_texts\n\n\n\n\nshow code\n# model = Rnn_model(embedding_size=10, max_seq_len=32, hidden_size=20, num_layers=1)\n# x, y = get_data(train_data, 4, 32)\n# y_hat, loss = model(x)\n# x.shape, y.shape\n\n\n\n\nshow code\n# print(y.shape, y_hat.shape)\n# # y = y.view(-1)\n# B, T, V = y_hat.shape\n# y_hat.view(B*T, V)\n\n# loss = nn.CrossEntropyLoss(ignore_index=0)\n# loss(y_hat.view(B*T, V), y.view(-1))\n# # loss(y_hat, y)\n# # y.shape, y_hat.shape\n\n\n\n\nshow code\n\n# input_text = [\"whats up my G GGGGG\", \"hey there\"]\n# max_seq_len=32\n# encoded_tokens = torch.tensor(encode_arr([input_text], max_seq_len=max_seq_len))\n# print(encoded_tokens)\n# if encoded_tokens.ndim == 1:\n#     encoded_tokens = encoded_tokens.unsqueeze(0)\n\n# for i in range(4):\n#     print(encoded_tokens.shape, encoded_tokens[:, -max_seq_len:].shape)\n#     outputs = model(encoded_tokens[:, -max_seq_len:])\n#     # last output token\n#     next_token_probs = outputs[:, -1, :]\n#     # print(next_token_probs)\n#     # sample indices from it using a multinomial distribution\n#     next_tokens = torch.multinomial(next_token_probs, 1)\n#     # print(next_tokens)\n#     encoded_tokens = torch.concat((encoded_tokens, next_tokens), axis=1)\n\n# print(encoded_tokens)\n\n\n\n\nshow code\n# Now a simple training loop.\n\nNUM_STEPS = 10000\nMAX_SEQ_LEN = 64\nBATCH_SIZE = 16\nEMBEDDING_SIZE = 20\nHIDDEN_SIZE = 20\nNUM_LAYERS = 2\nLR = 0.001 \nlog_every = 50\n\nmodel = Rnn_model(embedding_size=EMBEDDING_SIZE, max_seq_len=MAX_SEQ_LEN, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS)\noptimizer = torch.optim.Adam(model.parameters(), lr=LR)\nlosses = []\nfor i in range(NUM_STEPS):\n    optimizer.zero_grad()\n    x, y = get_data(train_data, BATCH_SIZE, MAX_SEQ_LEN)\n    outputs, loss = model(x, y)\n    loss.backward()\n    optimizer.step()\n    if i%log_every==0:\n        losses.append(loss)\n\n\n\nsqitching from SGD to adam instantly gives boost (2.8 vs under 2)\n\n\n\nshow code\nmodel.generate(\"hey there handsome what\")\n\n\n'&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;hey there handsome what not prrestetion frected mual am'"
  },
  {
    "objectID": "posts/rnns/01_rnns_from_scratch.html#the-model",
    "href": "posts/rnns/01_rnns_from_scratch.html#the-model",
    "title": "RNNS from scratch",
    "section": "The model",
    "text": "The model\n\nclass Rnn_model(nn.Module):\n\n    def __init__(self, embedding_size, max_seq_len, hidden_size, num_layers=1, vocab_size=None):\n        \"\"\"\n        Initializes the Rnn_model class.\n\n        Args:\n            embedding_size (int): The size of the embedding dimension.\n            max_seq_len (int): The maximum sequence length.\n            hidden_size (int): The size of the hidden state dimension.\n            num_layers (int, optional): The number of recurrent layers. Defaults to 1.\n            vocab_size (int, optional): The size of the vocabulary. Defaults to None.\n\n        \"\"\"\n        super(Rnn_model, self).__init__()\n\n        self.max_seq_len = max_seq_len\n        self.vocab_size = len(vocab) if vocab_size is None else vocab_size\n        self.embedding = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=embedding_size)\n        self.rnn = nn.RNN(input_size=embedding_size, hidden_size=hidden_size, batch_first=True, num_layers=num_layers)\n        self.fc = nn.Linear(hidden_size, len(vocab))\n        self.softmax = nn.Softmax(dim=-1)\n        self.criterion = nn.CrossEntropyLoss(ignore_index=0)  # ignore pad token\n\n    def forward(self, x, targets=None):\n        \"\"\" \n        a forward pas thorugh the model.\n        x: input torch tensor (B,T,S)\n        targets: input targets (B,T,S)\n\n        Returns\n        (model output logits, loss)\n        \"\"\"\n        x = x[:, -self.max_seq_len:]\n        x = self.embedding(x)\n        H, h_n = self.rnn(x)\n\n        # y = self.fc(H[:,-1,:])\n        y = self.fc(H)\n        \n        if targets is not None:\n            B, T, V = y.shape\n            loss = self.criterion(y.view(B*T, V), targets.view(-1))\n        else: loss=None\n        \n        return y, loss\n    \n    \n    @torch.no_grad\n    def generate(self, input_text, max_len=32):\n        \"\"\" \n        input_text: a string or list of strings to generate text using the model.\n        max_len: model will generate maximum of max_len tokens.\n        \"\"\"\n        encoded_tokens = torch.tensor(encode_arr([input_text], max_seq_len=32))\n        if encoded_tokens.ndim == 1:\n            encoded_tokens = encoded_tokens.unsqueeze(0)\n\n        for i in range(max_len):\n            outputs, _ = self(encoded_tokens[:, -self.max_seq_len:])\n\n            # last output token\n            outputs = outputs[:, -1, :]\n\n            # get pribabilities from logits\n            next_token_probs = nn.functional.softmax(outputs, dim=-1)\n\n            # sample indices from it using a multinomial distribution\n            next_tokens = torch.multinomial(next_token_probs, 1)\n\n            # concat prediction to original text\n            encoded_tokens = torch.concat((encoded_tokens, next_tokens), axis=1)\n\n        decoded_texts = decode_arr(encoded_tokens)\n        if len(decoded_texts)==1:\n            return decoded_texts[0] #.replace(\"&lt;pad&gt;\", \"\") \n        else: \n            # return [text.replace(\"&lt;pad&gt;\", \"\") for text in decoded_texts]\n            return decoded_texts\n\n\n\nshow code\n# model = Rnn_model(embedding_size=10, max_seq_len=32, hidden_size=20, num_layers=1)\n# x, y = get_data(train_data, 4, 32)\n# y_hat, loss = model(x)\n# x.shape, y.shape\n\n# print(y.shape, y_hat.shape)\n# # y = y.view(-1)\n# B, T, V = y_hat.shape\n# y_hat.view(B*T, V)\n\n# loss = nn.CrossEntropyLoss(ignore_index=0)\n# loss(y_hat.view(B*T, V), y.view(-1))\n# # loss(y_hat, y)\n# # y.shape, y_hat.shape\n\n\n# input_text = [\"whats up my G GGGGG\", \"hey there\"]\n# max_seq_len=32\n# encoded_tokens = torch.tensor(encode_arr([input_text], max_seq_len=max_seq_len))\n# print(encoded_tokens)\n# if encoded_tokens.ndim == 1:\n#     encoded_tokens = encoded_tokens.unsqueeze(0)\n\n# for i in range(4):\n#     print(encoded_tokens.shape, encoded_tokens[:, -max_seq_len:].shape)\n#     outputs = model(encoded_tokens[:, -max_seq_len:])\n#     # last output token\n#     next_token_probs = outputs[:, -1, :]\n#     # print(next_token_probs)\n#     # sample indices from it using a multinomial distribution\n#     next_tokens = torch.multinomial(next_token_probs, 1)\n#     # print(next_tokens)\n#     encoded_tokens = torch.concat((encoded_tokens, next_tokens), axis=1)\n\n# print(encoded_tokens)"
  },
  {
    "objectID": "posts/rnns/01_rnns_from_scratch.html#training-loop",
    "href": "posts/rnns/01_rnns_from_scratch.html#training-loop",
    "title": "RNNS from scratch",
    "section": "Training loop",
    "text": "Training loop\n\nNUM_STEPS = 1000\nMAX_SEQ_LEN = 64\nBATCH_SIZE = 32\nEMBEDDING_SIZE = 20\nHIDDEN_SIZE = 50\nNUM_LAYERS = 2\nLR = 0.001 \nlog_every = 50\n\nmodel = Rnn_model(embedding_size=EMBEDDING_SIZE, max_seq_len=MAX_SEQ_LEN, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS)\noptimizer = torch.optim.Adam(model.parameters(), lr=LR)\nlosses = []\nfor i in range(NUM_STEPS):\n    optimizer.zero_grad()\n    x, y = get_data(train_data, BATCH_SIZE, MAX_SEQ_LEN)\n    outputs, loss = model(x, y)\n    loss.backward()\n    optimizer.step()\n    if i%log_every==0:\n        losses.append(loss)\n\nlosses = [loss.detach().numpy().item() for loss in losses] \nprint(losses[::3])\n\n[4.754984378814697, 2.9505343437194824, 2.4850621223449707, 2.314824104309082, 2.132694721221924, 2.0362727642059326, 2.0536997318267822]\n\n\n\n\nshow code\n# plot losses\nimport matplotlib.pyplot as plt\nplt.plot(losses)\nplt.title(\"Training Loss\")\nplt.xlabel(\"Steps\")\nplt.ylabel(\"Loss\")\n\n\nText(0, 0.5, 'Loss')\n\n\n\n\n\n\n\n\n\n\nsqitching from SGD to adam instantly gives boost (2.8 vs under 2)\ntripling the data gave no gains on 50k basic rnn model!\n\nLets look at some outputs from the model.\n\nfor line in model.generate([\"hey there handsome what\", \"I recall \"]):\n    print(line.replace(\"&lt;pad&gt;\", \"\"))\n\nhey there handsome what you reoth the that the beive do\nI recall the sroad tood oulf the inlow hi\n\n\n\n\nshow code\n# calculate size of model parameters\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"paramter count of the model: {count_parameters(model)}\")\n\n\nparamter count of the model: 43320\n\n\nThe results dont make much sense, but the model is learning to generate text. By scaling the model and training for longer, we can get better results.\nThank you for reading this and I hope you found it useful. The next time I update this blog post, I’ll be adding the following:\n\nEvaluation on test set (metrics like perplexity).\nImplementing RNN, LSTM and GRU from scratch.\n\nByee :)"
  }
]