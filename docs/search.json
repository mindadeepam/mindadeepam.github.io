[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi Y‚Äôall. My name is Deepam Minda and this is my website. I want to be writing a lot more, blogs, TILs, whatever. Im a data scientist myself so naturally the content here would also mostly be around it. I hope you learn something new here. Feel free to share your thoughts or hit me up.\nCheers!"
  },
  {
    "objectID": "posts/few-shot-learning/prototype_networks.html",
    "href": "posts/few-shot-learning/prototype_networks.html",
    "title": "Few Shot learning: Classify using few examples!",
    "section": "",
    "text": "If you aren‚Äôt already familiar with it, few-shot learning (FSL) is an exciting concept in machine learning that focuses on training models to make accurate predictions or classifications when provided with only a very limited amount of labeled data for each category or class. Unlike traditional machine learning approaches that often require substantial labeled datasets for training, few-shot learning aims to address scenarios where acquiring such extensive labeled data is impractical or costly.\nIn few-shot learning, the model is trained to generalize effectively from a small number of examples (or shots) per category, allowing it to make predictions for new, unseen data points with confidence. This capability is particularly valuable in situations where data is scarce, and manual annotation is labor-intensive or expensive. Refer to [1] for an in-depth survey in this field.\nIn this blog, I‚Äôm going to show you how to implement a basic few-shot classification technique for text."
  },
  {
    "objectID": "posts/few-shot-learning/prototype_networks.html#introduction",
    "href": "posts/few-shot-learning/prototype_networks.html#introduction",
    "title": "Few Shot learning: Classify using few examples!",
    "section": "",
    "text": "If you aren‚Äôt already familiar with it, few-shot learning (FSL) is an exciting concept in machine learning that focuses on training models to make accurate predictions or classifications when provided with only a very limited amount of labeled data for each category or class. Unlike traditional machine learning approaches that often require substantial labeled datasets for training, few-shot learning aims to address scenarios where acquiring such extensive labeled data is impractical or costly.\nIn few-shot learning, the model is trained to generalize effectively from a small number of examples (or shots) per category, allowing it to make predictions for new, unseen data points with confidence. This capability is particularly valuable in situations where data is scarce, and manual annotation is labor-intensive or expensive. Refer to [1] for an in-depth survey in this field.\nIn this blog, I‚Äôm going to show you how to implement a basic few-shot classification technique for text."
  },
  {
    "objectID": "posts/few-shot-learning/prototype_networks.html#terminology",
    "href": "posts/few-shot-learning/prototype_networks.html#terminology",
    "title": "Few Shot learning: Classify using few examples!",
    "section": "Terminology",
    "text": "Terminology\nBefore we begin, let us familiarize ourselves with the correct terminology.\nWhat characterizes FSL is having only a few examples at hand, for unseen classes, during inference. So basically we are showing the model only a few examples of a class which it may or may not have encountered during its pre-training before we make predictions using that model.\nSupport Set, ùíÆ: The few annotated examples that we have, make up the support set, with which we may or may not update the model weights to make it generalize to the new classes.\nQuery Set, ùí¨: The query set consists of our test set, i.e.¬†the samples we want to classify using the base model and a support set.\nN-way K-shot learning scheme: This is a common phrase used in the FSL literature, which essentially describes the few-shot problem statement that a model will be dealing with. ‚ÄúN‚Äù is the number of classes we have at test time and ‚ÄúK‚Äù is the number of samples per class we have in our support set ‚ÄúùíÆ‚Äù\n1-shot classification: When K=1, i.e.¬†we have only one labeled sample available per class.\n0-shot classification: K=0, i.e.¬†we do not have any labeled samples available during inference.\nLet us have a look at an example.\n\n\nCode\n# sample set is 3-way, 3-shot.\nclasses = ['camera', 'battery', 'display']\n\nsample_set = {\n    'camera': [\n        'absolutely love this quality of my photos!!',\n        'it even gives great quality in dim lighting. fabulous!!',\n        'the camera should be much better for such a high price'\n    ],\n    'battery': [\n        \"The battery life on this device is exceptional! It easily lasts me the entire day with heavy usage.\",\n        \"I'm a bit disappointed with the battery performance. It drains quite quickly, especially when using power-hungry apps.\",\n        \"The battery is decent, not too bad, not too good. It gets me through the day, but I was hoping for better longevity.\"\n    ],\n    'display': [\n        \"The display on this device is stunning! Colors are vivid, and the resolution is top-notch.\",\n        \"I'm not too impressed with the display quality. It seems a bit washed out, and the brightness could be better.\",\n        \"The display is okay, but nothing extraordinary. It gets the job done for everyday tasks.\"\n    ]\n}\n\nquery_set = [\"i hate the batteries\", \"does it give good quality photos in the night?\"]\n\n\nHere we have a 3-way (there are 3 classes), 3-shot (3 examples for each class) setting."
  },
  {
    "objectID": "posts/few-shot-learning/prototype_networks.html#high-level-design",
    "href": "posts/few-shot-learning/prototype_networks.html#high-level-design",
    "title": "Few Shot learning: Classify using few examples!",
    "section": "High level design",
    "text": "High level design\nLet us have a quick look at the architecture of the system.\n\n\n\n\n\n\nFigure¬†1: A simple few shot classification system\n\n\n\nThis is the flow of our solution:\nThe first step is to get an embedding module. That can be created using regular supervised learning (Resnets trained on Imagenet) or self-supervised learning (BERT and co). Then, we use the embedding module to get feature representations for our classes in the support set. A simple way to do this is to turn each class‚Äôs examples into embeddings and take the mean of those vectors. This then becomes our ‚Äúprototype‚Äù vectors to compare against. Now for each query, we can take the embeddings of the query text and use cosine similarity to find the predicted class. This closely resembles This system basically allows us to leverage transfer learning to use large backbones as our embedding module. And there is also the advantage of not performing any gradient updates. This helps us maintain a much more dynamic and flexible system.\nThe idea of comparing query samples with the support set samples is inspired by metric learning. Refer to [3, 4] for better understanding.\nLet‚Äôs implement this using the transformers library. You can find the implementation in this colab notebook."
  },
  {
    "objectID": "posts/few-shot-learning/prototype_networks.html#implementation",
    "href": "posts/few-shot-learning/prototype_networks.html#implementation",
    "title": "Few Shot learning: Classify using few examples!",
    "section": "Implementation",
    "text": "Implementation\nLet‚Äôs start with the good old BERT base model.\n\n1. Import libraries and download model\n\n\nCode\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport numpy as np\nfrom typing import Dict\nfrom pprint import pprint\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\nif torch.backends.mps.is_available():\n    device = torch.device('mps')\nelse:\n    device = torch.device('cpu')\n    \n# load the model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\nmodel = AutoModel.from_pretrained(\"bert-base-uncased\")\n\n\n\n\n2. Tokenize and encode a sentence\n\n\nCode\ntext = \"He's such a great guy!!\"\nencoded_input = tokenizer(\n  text, \n  return_tensors='pt', \n  padding='max_length',     # True will pad to max-len in batch\n  max_length=32\n)\nprint(f\"encoded input:\")\npprint(encoded_input)\n\n\nencoded input:\n{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]]),\n 'input_ids': tensor([[ 101, 2002, 1005, 1055, 2107, 1037, 2307, 3124,  999,  999,  102,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0]]),\n 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]])}\n\n\nwhere,\n\ninput_ids: token id of each token\ntoken_type_id: When we pass two sentences for downstream fine-tuning in BERT, this is used to identify which token belongs to which sentence.\nattention_mask: which tokens to ignore. As you‚Äôll see, the padding tokens have been masked.\n\n\n\n3. Generate embeddings using model\nThe output has 2 parts, cls_token_embeddings and last_hidden_states of the tokens. We can either use the cls_embeddings to represent the sentence or pool the vectors in last_hidden_states. The pooling can be max/min/mean.\nThe dimension of the output will be equal to the embedding dimension of the model, i.e.¬†784 in our case.\n\n\nCode\ndef get_embeddings(model, tokenizer, text, pooling='mean'):\n  \n  encoded_input = tokenizer(\n    text, \n    return_tensors='pt', \n    padding='max_length', \n    max_length=16, \n    truncation=True\n  )\n  encoded_input = encoded_input.to(device)\n\n  model.to(device)\n\n  model.eval()\n  with torch.no_grad():\n    output = model(**encoded_input)\n    last_hidden_state, pooler_output = output[0], output[1]\n    \n    if pooling=='cls':\n      embedding = pooler_output\n    else:\n      # ignore the pad tokens embeddings by multiplying with attention mask\n      last_hidden_state = (last_hidden_state * encoded_input['attention_mask'].unsqueeze(2))\n      embedding = last_hidden_state.mean(dim=-2)\n  return np.array(embedding.cpu())\n\n\nembeddings = get_embeddings(model, tokenizer, 'hey there! how are you?')\nprint(f\"shape of embeddings: {embeddings.shape}\")\n\n\nshape of embeddings: (1, 768)\n\n\n\n\n4. Prepare the prototypes:\nTo prepare the class prototypes we‚Äôll take the mean of the sentences for each class.\n\n\nCode\ndef make_prototypes(model, tokenizer, sample_set: Dict):\n  prototype_vectors = dict()\n  sentence_embeddings = dict()\n  for category, sentences in sample_set.items():\n    sentence_embeds = get_embeddings(model, tokenizer, sentences)\n    sentence_embeddings[category] = sentence_embeds\n    prototype_vectors[category] = np.mean(sentence_embeddings[category], axis=0)\n  return prototype_vectors\n\n\n\n\n5. Classify\nTo classify a query text, we can run cosine similarity against the prototype vectors and return the argmax as the most probable class!\n\n\nCode\ndef classify(model, tokenizer, text, prototype_vectors=None, sample_set=None):\n  if prototype_vectors==None:\n      assert sample_set!=None, \"prototype vectors are not passed, either pass a sample set prototype vectors\"\n      prototype_vectors = make_prototypes(sample_set)\n\n  query_embeddings = get_embeddings(model, tokenizer, text)\n  \n  prototype_matrix = np.stack(list(prototype_vectors.values()))\n  scores = sentence_transformers.util.cos_sim(query_embeddings, prototype_matrix)\n  return scores\n\n\nUsing the above-defined functions and the sample set from before, we have:\n\n\nCode\nprototype_vectors = make_prototypes(model, tokenizer, sample_set)\nquery_text = \"i hate the batteries\"\noutput = classify(model, tokenizer, query_text, prototype_vectors=prototype_vectors)\n\nprint(f\"output: {output}\")\nprint(f\"the predicted class is {classes[output.argmax().item()]}\")\n\n\noutput: tensor([[0.6121, 0.7127, 0.6388]])\nthe predicted class is battery\n\n\nA bit strange! Although the expected class is predicted, scores for other classes are also high. Let‚Äôs try a harder query.\n\n\nCode\nquery = ['does it give good quality photos in the night?']\noutput = classify(model, tokenizer, query, prototype_vectors=prototype_vectors)\n\nprint(f\"output: {output}\")\nprint(f\"the predicted class is {classes[output.argmax().item()]}\")\n\n\noutput: tensor([[0.7984, 0.7043, 0.7647]])\nthe predicted class is camera\n\n\nAlthough the highest similarity is for ‚Äòcamera‚Äô, the similarity should be much higher.\nThe results do not get better even if we try cls-pooling. This only means that the embeddings produced by the model do not give us an accurate representation of the sentence.\nWe would then do good to remember that BERT pre-train was trained by MaskedLM, NextSentencePrediction, hence the original purpose of BERT is not to create a meaningful embedding of the sentence but for some specific downstream task. In fact, as the authors of the sentence-transformer paper [2] point out, out-of-the-box Bert embeddings perform even worse than GLoVE representations!\n\nJacob Devlin‚Äôs comment: I‚Äôm not sure what these vectors are, since BERT does not generate meaningful sentence vectors. It seems that this is is doing average pooling over the word tokens to get a sentence vector, but we never suggested that this will generate meaningful sentence representations. And even if they are decent representations when fed into a DNN trained for a downstream task, it doesn‚Äôt mean that they will be meaningful in terms of cosine distance. (Since cosine distance is a linear space where all dimensions are weighted equally).\n\nThere are a few ways to improve the bert-base for sentence-level tasks and both involve finetuning the model with some data.\n\nadding a linear layer on top and fine-tuning it.\nmaking embeddings better by contrastive learning."
  },
  {
    "objectID": "posts/few-shot-learning/prototype_networks.html#using-sentence-transformers",
    "href": "posts/few-shot-learning/prototype_networks.html#using-sentence-transformers",
    "title": "Few Shot learning: Classify using few examples!",
    "section": "Using sentence transformers",
    "text": "Using sentence transformers\nUltimately, what we need is a better embedding module. Luckily we have such models. As it turns out, contrastive learning is an excellent approach for tuning our models such that different sentences produce semantically different embeddings.\nWe will explore contrastive learning and its inner workings some other day, but for now, let‚Äôs pick up open-source models that have been finetuned using contrastive learning. There is an entire library (aka sentence-transformers) and paper[2] dedicated to this.\nWe‚Äôll use the sentence-transformers/stsb-bert-base model for our purposes.\n\n1. Import packages and download model\n\n\nCode\nfrom sentence_transformers import SentenceTransformer\n\n# load a sentence transformer model\nsts_model = SentenceTransformer('sentence-transformers/stsb-bert-base')\nmodel2 = sts_model[0].auto_model.to(device)\ntokenizer2 = sts_model[0].tokenizer\n\n\n\n\n2. Use the above-defined functions to prepare prototype vectors and classify them in a few-shot setting\n\n\nCode\nprototype_vectors = make_prototypes(model2, tokenizer2, sample_set)\nquery_text = \"i hate the batteries\"\noutput = classify(model2, tokenizer2, query_text, prototype_vectors)\n\nprint(f\"output: {output}\")\nprint(f\"the predicted class is {classes[output.argmax().item()]}\")\n\n\noutput: tensor([[0.0910, 0.4780, 0.1606]])\nthe predicted class is battery\n\n\n\n\nCode\n\nquery = ['does it give good quality photos in the night?']\noutput = classify(model2, tokenizer2, query, prototype_vectors=prototype_vectors)\n\nprint(f\"output: {output}\")\nprint(f\"the predicted class is {classes[output.argmax().item()]}\")\n\n\noutput: tensor([[0.4467, 0.1012, 0.2998]])\nthe predicted class is camera\n\n\nAs we can see, the scores seem much more reasonable this time around. There is a much better correlation with the ground truth labels. Using better base models trained in multiple tasks further improves the performance of these models."
  },
  {
    "objectID": "posts/few-shot-learning/prototype_networks.html#conclusion",
    "href": "posts/few-shot-learning/prototype_networks.html#conclusion",
    "title": "Few Shot learning: Classify using few examples!",
    "section": "Conclusion",
    "text": "Conclusion\nThis brings us to the end of this blog. In summary, we explored the realm of few-shot learning, a machine-learning approach tailored for accurate predictions with limited labeled data. Initially, we employed BERT, but its design didn‚Äôt align with our objectives. Instead, we leveraged a model fine-tuned for sentence-level tasks, sentence-transformers/stsb-bert-base, which significantly improved our results.\nThese are a few things to note:\nAlthough we directly used pre-trained models here, an interesting undertaking would be to perform the contrastive fine-tuning ourselves. Also, instead of using cosine similarity, we can train lightweight classifiers on top of our embedding module for better performance.\nThat‚Äôll be all from my side. Until next time, Happy Reading!"
  },
  {
    "objectID": "posts/few-shot-learning/prototype_networks.html#references",
    "href": "posts/few-shot-learning/prototype_networks.html#references",
    "title": "Few Shot learning: Classify using few examples!",
    "section": "References",
    "text": "References\n[1] Survey paper on few-shot learning\n[2] Sentence-Bert paper\n[3] Prototypical Networks\n[4] Excellent much more techincal blog by Lilian Weng"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deepam Minda blogs",
    "section": "",
    "text": "Few Shot learning: Classify using few examples!\n\n\n\n\n\n\nclassification\n\n\nfew-shot learning\n\n\nnlp\n\n\n\n\n\n\n\n\n\nSep 12, 2023\n\n\nDeepam Minda\n\n\n\n\n\n\nNo matching items"
  }
]