[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi Y’all. My name is Deepam Minda and this is my website. I want to be writing a lot more, blogs, TILs, whatever. Im a data scientist myself so naturally the content here would also mostly be around it. I hope you learn something new here. Feel free to share your thoughts or hit me up.\nCheers!"
  },
  {
    "objectID": "posts/rnns/01_rnns_from_scratch.html",
    "href": "posts/rnns/01_rnns_from_scratch.html",
    "title": "RNNS from scratch",
    "section": "",
    "text": "The only reason you would be hearing RNNs right now is probably when xLSTMs were released in May, 2024. Apart from this they have pretty much taken a back seat to watch transformers revolutionalize NLP and the entire field of AI in general.\nBut one would do well to remember how we got here, and RNNs played a massive role in bringing us here. So in this blog post I’m going to build a small RNN model and we’ll try to train it to generate text."
  },
  {
    "objectID": "posts/rnns/01_rnns_from_scratch.html#understanding-rnns",
    "href": "posts/rnns/01_rnns_from_scratch.html#understanding-rnns",
    "title": "RNNS from scratch",
    "section": "Understanding RNNs",
    "text": "Understanding RNNs\nRNNs have 2 matrices, one (W_{xh}) that maps input tokens to hidden_vector size and another (W_{hh}) that maps from hidden_vector to hidden_vector. You’ll see how these are used in a minute.\nLet us first look at input-output shapes for an RNN layer. We initially had a batch of text-tokens. Lets assume batch size of 4 and max_seq_len of 32. Hence the shape of input is (4,32).\nNow for each token, we encode it to a number and then map it to a vector (which we generally call an embedding). Hence each token is now represented by a vector of fixed-shape, and lets call this embedding_dimension and set it to 10.\nThe shape of our input batch is now (batch_size, max_seq_len, emb_dim), ie (4,32,10).\nNow let us peek into the matrix multiplications inside a RNN layer. Firstly, lets us recall that for a linear layer, this is the matrix equation:\nz (N, n_{out}) = \\sigma(x (N, n_{in}) * W_x^T (n_{in}, n_{out}) + b (N))\nwhere ,\n\ninput-features = n_{in}\noutput-features = n_{out}\nbatch-size = N\n\nIn a linear layer, each token/feature is attended to by a different weight in the weight matrix and no information is shared among the sequence tokens. But when processing “sequences” we obviously want the model to remember stuff from previous tokens for the current token, right?\nHence RNNs maintain a hidden_vector for each token, that takes as input the current token and the hidden_vector from the previous token’s output.\nSo for the t’th token,\nh_t (N, h)= x_t (N, n_{in}) * W_{xh}^T (n_{in}, h) + h_{t-1} (N, h) * W_{hh}^T (h, h) + biases\nwhere\n\ninput-features = n_{in}\nhidden-size = h\nbatch-size = N\nsequence-length = s\n\nAs you’ll notice since each token depends on previous tokens output, we cannot process this parallelly and have to iteratively calculate the output for each token. Also note we generally refer to the different tokens in a sequence as different timesteps, ie token at timestep t is x_t.\nHence for a complete batch, inputs are:\n\nX of shape (N, s, n_{in})\nh_0 of shape (N, h) (this is optional, if not given most libraries will initiate a h_0 of all zeros or random numbers)\n\nAnd outputs are:\n\nhidden states of all timesteps, ie H of shape (N, s, h)\nlast_hidden_state ie h_n of shape (N, h)\n\nNote: sometimes you will see outputs of rnn fed into a linear layer like so,\noutputs, h_n = self.rnn(x)\ny = self.fc(outputs[:,-1,:])\nHere h_n and outputs[:,-1,:] are the same thing. They both represent the last hidden state for the entire batch. (to make shapes equal use h_n.squeeze())\nLets verify the above by passing inputs to an rnn layer.\n\n\nshow code\nemb_dim = 50\nhidden_size = 100\nbatch_size = 4\nmax_seq_len = 256\n\nprint(f\"batch_size: {batch_size}, hidden_size: {hidden_size}, max_seq_len: {max_seq_len}, emb_dim: {emb_dim}\")\nX,y = get_data(train_data, seq_len=max_seq_len, batch_size=batch_size)\nprint(f\"shape of initial input -&gt; {X.shape}\")\n\nemb_layer = nn.Embedding(num_embeddings=len(vocab), embedding_dim=emb_dim)\nrnn_layer = nn.RNN(input_size=emb_dim, hidden_size=hidden_size, batch_first=True, bidirectional=False, num_layers=1)\n\nX = emb_layer(X)\nprint(f\"post embedding; shape of input to RNN layer -&gt; {X.shape}\")\nh_0 = torch.randn(1, batch_size, hidden_size)\noutputs = rnn_layer(X, h_0)\n\nprint(f\"RNN output shapes -&gt; {outputs[0].shape}, {[outputs[1][i].shape for i in range(len(outputs[1]))]}\")\n\n\nbatch_size: 4, hidden_size: 100, max_seq_len: 256, emb_dim: 50\nshape of initial input -&gt; torch.Size([4, 256])\npost embedding; shape of input to RNN layer -&gt; torch.Size([4, 256, 50])\nRNN output shapes -&gt; torch.Size([4, 256, 100]), [torch.Size([4, 100])]"
  },
  {
    "objectID": "posts/rnns/01_rnns_from_scratch.html#the-model",
    "href": "posts/rnns/01_rnns_from_scratch.html#the-model",
    "title": "RNNS from scratch",
    "section": "The model",
    "text": "The model\n\nclass Rnn_model(nn.Module):\n\n    def __init__(self, embedding_size, max_seq_len, hidden_size, num_layers=1, vocab_size=None):\n        \"\"\"\n        Initializes the Rnn_model class.\n\n        Args:\n            embedding_size (int): The size of the embedding dimension.\n            max_seq_len (int): The maximum sequence length.\n            hidden_size (int): The size of the hidden state dimension.\n            num_layers (int, optional): The number of recurrent layers. Defaults to 1.\n            vocab_size (int, optional): The size of the vocabulary. Defaults to None.\n\n        \"\"\"\n        super(Rnn_model, self).__init__()\n\n        self.max_seq_len = max_seq_len\n        self.vocab_size = len(vocab) if vocab_size is None else vocab_size\n        self.embedding = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=embedding_size)\n        self.rnn = nn.RNN(input_size=embedding_size, hidden_size=hidden_size, batch_first=True, num_layers=num_layers)\n        self.fc = nn.Linear(hidden_size, len(vocab))\n        self.softmax = nn.Softmax(dim=-1)\n        self.criterion = nn.CrossEntropyLoss(ignore_index=0)  # ignore pad token\n\n    def forward(self, x, targets=None):\n        \"\"\" \n        a forward pas thorugh the model.\n        x: input torch tensor (B,T,S)\n        targets: input targets (B,T,S)\n\n        Returns\n        (model output logits, loss)\n        \"\"\"\n        x = x[:, -self.max_seq_len:]\n        x = self.embedding(x)\n        H, h_n = self.rnn(x)\n\n        # y = self.fc(H[:,-1,:])\n        y = self.fc(H)\n        \n        if targets is not None:\n            B, T, V = y.shape\n            loss = self.criterion(y.view(B*T, V), targets.view(-1))\n        else: loss=None\n        \n        return y, loss\n    \n    \n    @torch.no_grad\n    def generate(self, input_text, max_len=32):\n        \"\"\" \n        input_text: a string or list of strings to generate text using the model.\n        max_len: model will generate maximum of max_len tokens.\n        \"\"\"\n        encoded_tokens = torch.tensor(encode_arr([input_text], max_seq_len=32))\n        if encoded_tokens.ndim == 1:\n            encoded_tokens = encoded_tokens.unsqueeze(0)\n\n        for i in range(max_len):\n            outputs, _ = self(encoded_tokens[:, -self.max_seq_len:])\n\n            # last output token\n            outputs = outputs[:, -1, :]\n\n            # get pribabilities from logits\n            next_token_probs = nn.functional.softmax(outputs, dim=-1)\n\n            # sample indices from it using a multinomial distribution\n            next_tokens = torch.multinomial(next_token_probs, 1)\n\n            # concat prediction to original text\n            encoded_tokens = torch.concat((encoded_tokens, next_tokens), axis=1)\n\n        decoded_texts = decode_arr(encoded_tokens)\n        if len(decoded_texts)==1:\n            return decoded_texts[0] #.replace(\"&lt;pad&gt;\", \"\") \n        else: \n            # return [text.replace(\"&lt;pad&gt;\", \"\") for text in decoded_texts]\n            return decoded_texts\n\n\n\nshow code\n# model = Rnn_model(embedding_size=10, max_seq_len=32, hidden_size=20, num_layers=1)\n# x, y = get_data(train_data, 4, 32)\n# y_hat, loss = model(x)\n# x.shape, y.shape\n\n# print(y.shape, y_hat.shape)\n# # y = y.view(-1)\n# B, T, V = y_hat.shape\n# y_hat.view(B*T, V)\n\n# loss = nn.CrossEntropyLoss(ignore_index=0)\n# loss(y_hat.view(B*T, V), y.view(-1))\n# # loss(y_hat, y)\n# # y.shape, y_hat.shape\n\n\n# input_text = [\"whats up my G GGGGG\", \"hey there\"]\n# max_seq_len=32\n# encoded_tokens = torch.tensor(encode_arr([input_text], max_seq_len=max_seq_len))\n# print(encoded_tokens)\n# if encoded_tokens.ndim == 1:\n#     encoded_tokens = encoded_tokens.unsqueeze(0)\n\n# for i in range(4):\n#     print(encoded_tokens.shape, encoded_tokens[:, -max_seq_len:].shape)\n#     outputs = model(encoded_tokens[:, -max_seq_len:])\n#     # last output token\n#     next_token_probs = outputs[:, -1, :]\n#     # print(next_token_probs)\n#     # sample indices from it using a multinomial distribution\n#     next_tokens = torch.multinomial(next_token_probs, 1)\n#     # print(next_tokens)\n#     encoded_tokens = torch.concat((encoded_tokens, next_tokens), axis=1)\n\n# print(encoded_tokens)"
  },
  {
    "objectID": "posts/rnns/01_rnns_from_scratch.html#training-loop",
    "href": "posts/rnns/01_rnns_from_scratch.html#training-loop",
    "title": "RNNS from scratch",
    "section": "Training loop",
    "text": "Training loop\n\nNUM_STEPS = 1000\nMAX_SEQ_LEN = 64\nBATCH_SIZE = 32\nEMBEDDING_SIZE = 20\nHIDDEN_SIZE = 50\nNUM_LAYERS = 2\nLR = 0.001 \nlog_every = 50\n\nmodel = Rnn_model(embedding_size=EMBEDDING_SIZE, max_seq_len=MAX_SEQ_LEN, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS)\noptimizer = torch.optim.Adam(model.parameters(), lr=LR)\nlosses = []\nfor i in range(NUM_STEPS):\n    optimizer.zero_grad()\n    x, y = get_data(train_data, BATCH_SIZE, MAX_SEQ_LEN)\n    outputs, loss = model(x, y)\n    loss.backward()\n    optimizer.step()\n    if i%log_every==0:\n        losses.append(loss)\n\nlosses = [loss.detach().numpy().item() for loss in losses] \nprint(losses[::3])\n\n[4.754984378814697, 2.9505343437194824, 2.4850621223449707, 2.314824104309082, 2.132694721221924, 2.0362727642059326, 2.0536997318267822]\n\n\n\n\nshow code\n# plot losses\nimport matplotlib.pyplot as plt\nplt.plot(losses)\nplt.title(\"Training Loss\")\nplt.xlabel(\"Steps\")\nplt.ylabel(\"Loss\")\n\n\nText(0, 0.5, 'Loss')\n\n\n\n\n\n\n\n\n\n\nsqitching from SGD to adam instantly gives boost (2.8 vs under 2)\ntripling the data gave no gains on 50k basic rnn model!\n\nLets look at some outputs from the model.\n\nfor line in model.generate([\"hey there handsome what\", \"I recall \"]):\n    print(line.replace(\"&lt;pad&gt;\", \"\"))\n\nhey there handsome what you reoth the that the beive do\nI recall the sroad tood oulf the inlow hi\n\n\n\n\nshow code\n# calculate size of model parameters\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"paramter count of the model: {count_parameters(model)}\")\n\n\nparamter count of the model: 43320\n\n\nThe results dont make much sense, but the model is learning to generate text. By scaling the model and training for longer, we can get better results.\nThank you for reading this and I hope you found it useful. The next time I update this blog post, I’ll be adding the following:\n\nEvaluation on test set (metrics like perplexity).\nImplementing RNN, LSTM and GRU from scratch.\n\nByee :)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blogs by a noob techie",
    "section": "",
    "text": "RNNS from scratch\n\n\n\n\n\n\nclassification\n\n\nrnns\n\n\nnlp\n\n\nseq-to-seq\n\n\n\n\n\n\n\n\n\nJul 20, 2024\n\n\nDeepam Minda\n\n\n\n\n\n\nNo matching items"
  }
]